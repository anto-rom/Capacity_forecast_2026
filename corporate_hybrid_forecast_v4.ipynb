{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0aa95d0",
   "metadata": {},
   "source": [
    "# Corporate Hybrid Forecast Notebook ((Prophet vs ARIMA vs TBATS/ETS)) – v4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15154c",
   "metadata": {},
   "source": [
    "## 01 - Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "303433e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Capacity Forecast 2026 – Hybrid (Prophet / ARIMA / TBATS-ETS) – v5\n",
    "\n",
    "What's included vs. your v4.2:\n",
    "- Model on monthly rate per workday: forecast rate in log1p scale, then multiply by future workdays (reduces variance & MAPE)\n",
    "- Blending by wMAPE (weights ~ 1/wMAPE) while still reporting sMAPE in the CV sheet\n",
    "- Robust month-to-month smoothing using rolling Median ± K·MAD (prevents spikes like 43k)\n",
    "- More conservative ARIMA grid (less overreactive)\n",
    "- ETS with damped trend added as a candidate for blending (stable option)\n",
    "- Keep: log-scale modelling + expm1_safe + dynamic caps, winsorization, adaptive/sanitized CV, top-down daily reconciliation, sheet names\n",
    "\n",
    "All comments are in English per your preference.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import Optional, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    Prophet = None\n",
    "try:\n",
    "    from tbats import TBATS\n",
    "except Exception:\n",
    "    TBATS = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "\n",
    "# Inputs (adjust if your file locations change)\n",
    "INCOMING_SOURCE_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\Incoming_new.xlsx\"  # Sheet 'Main'\n",
    "INCOMING_SHEET = \"Main\"\n",
    "DEPT_MAP_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\department.xlsx\"\n",
    "DEPT_MAP_SHEET = \"map\"\n",
    "PRODUCTIVITY_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\productivity_agents.xlsx\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_XLSX = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\"\n",
    "\n",
    "# Horizons and switches\n",
    "H_MONTHS = 12             # monthly forecast horizon\n",
    "DAILY_HORIZON_DAYS = 90   # daily plan horizon\n",
    "REPORT_START_MONTH = \"2025-01\"  # show historical Actuals from this month in capacity_error\n",
    "\n",
    "# Top-down reconciliation for daily forecasts\n",
    "USE_DAILY_FROM_MONTHLY = True\n",
    "\n",
    "# Strong \"hard clipping\" at output is disabled (we now use dynamic caps inside expm1_safe)\n",
    "ENABLE_FORECAST_CLIP = False\n",
    "FORECAST_CLIP_MULTIPLIER = 2.5  # kept for reference if you ever want to re-enable\n",
    "\n",
    "# Optional final growth guard (disabled by default; enable if needed)\n",
    "APPLY_LOCAL_GROWTH_GUARD = False\n",
    "MAX_GROWTH = 1.8   # +80% vs local ref\n",
    "MIN_GROWTH = 0.5   # -50% vs local ref\n",
    "\n",
    "# Organization-specific\n",
    "WEEKLY_START_THU = True\n",
    "\n",
    "# Fixed language shares\n",
    "LANGUAGE_SHARES = {\n",
    "    'English': 0.6435,\n",
    "    'French': 0.0741,\n",
    "    'German': 0.0860,\n",
    "    'Italian': 0.0667,\n",
    "    'Portuguese': 0.0162,\n",
    "    'Spanish': 0.1135\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5140c58e",
   "metadata": {},
   "source": [
    "## 02. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605480eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_incoming(path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load daily incoming volumes. Build ticket_total if needed.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Incoming file not found:\\n{path}\\n\")\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xlsm\", \".xls\"]:\n",
    "        if not sheet_name:\n",
    "            raise ValueError(\"Excel file detected but no sheet_name provided (e.g., 'Main').\")\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension for incoming data: {ext}\")\n",
    "\n",
    "    base_required = {'Date', 'department_id'}\n",
    "    missing = base_required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Incoming file must contain {sorted(list(base_required))}. Found: {list(df.columns)}\")\n",
    "\n",
    "    if 'ticket_total' not in df.columns:\n",
    "        if 'total_incoming' in df.columns:\n",
    "            df['ticket_total'] = pd.to_numeric(df['total_incoming'], errors='coerce').fillna(0)\n",
    "        elif {'incoming_from_customers', 'incoming_from_transfers'}.issubset(df.columns):\n",
    "            df['ticket_total'] = (\n",
    "                pd.to_numeric(df['incoming_from_customers'], errors='coerce').fillna(0) +\n",
    "                pd.to_numeric(df['incoming_from_transfers'], errors='coerce').fillna(0)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Missing 'ticket_total' or components to create it.\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if df['Date'].isna().any():\n",
    "        bad = df.loc[df['Date'].isna()]\n",
    "        raise ValueError(f\"Some Date values could not be parsed. Example rows:\\n{bad.head(5)}\")\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['ticket_total'] = pd.to_numeric(df['ticket_total'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    if 'department_name' in df.columns:\n",
    "        df['department_name'] = df['department_name'].astype(str).str.strip()\n",
    "    else:\n",
    "        df['department_name'] = None\n",
    "    if 'vertical' in df.columns:\n",
    "        df['vertical'] = df['vertical'].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dept_map(path: str, sheet: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load dept mapping -> department_name, vertical.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['department_id', 'department_name', 'vertical'])\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xls\"):\n",
    "        if sheet:\n",
    "            mp = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        else:\n",
    "            xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "            mp = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "    else:\n",
    "        mp = pd.read_csv(path)\n",
    "\n",
    "    rename_map = {\n",
    "        'dept_id': 'department_id',\n",
    "        'dept_name': 'department_name',\n",
    "        'name': 'department_name',\n",
    "        'segment': 'vertical',\n",
    "        'vertical_name': 'vertical'\n",
    "    }\n",
    "    mp = mp.rename(columns={k: v for k, v in rename_map.items() if k in mp.columns})\n",
    "    if 'department_id' not in mp.columns:\n",
    "        raise ValueError(f\"Department map must contain 'department_id'. Found: {list(mp.columns)}\")\n",
    "\n",
    "    mp['department_id'] = mp['department_id'].astype(str).str.strip()\n",
    "    mp['department_name'] = (mp['department_name'].astype(str).str.strip()\n",
    "                             if 'department_name' in mp.columns else None)\n",
    "    mp['vertical'] = (mp['vertical'].astype(str).str.strip()\n",
    "                      if 'vertical' in mp.columns else None)\n",
    "\n",
    "    return mp[['department_id', 'department_name', 'vertical']].drop_duplicates('department_id')\n",
    "\n",
    "\n",
    "def load_productivity(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load agent productivity and compute dept-level mean tickets/agent-day.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Productivity file not found: {path}\")\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    req = {'Date', 'agent_id', 'department_id', 'prod_total_model'}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"productivity_agents.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['prod_total_model'] = pd.to_numeric(df['prod_total_model'], errors='coerce')\n",
    "\n",
    "    prod_dept = (df.groupby('department_id', as_index=False)['prod_total_model']\n",
    "                 .mean()\n",
    "                 .rename(columns={'prod_total_model': 'avg_tickets_per_agent_day'}))\n",
    "    return prod_dept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f65faf",
   "metadata": {},
   "source": [
    "## 03. Helpers & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f12d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_days_in_month(year: int, month: int) -> int:\n",
    "    \"\"\"Approximate Mon-Fri working days in a month.\"\"\"\n",
    "    rng = pd.date_range(start=pd.Timestamp(year=year, month=month, day=1),\n",
    "                        end=pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(0),\n",
    "                        freq='D')\n",
    "    return int(np.sum(rng.weekday < 5))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred) -> float:\n",
    "    \"\"\"sMAPE robust for intermittent series.\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom[denom == 0] = 1.0\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "\n",
    "def wmape(y_true, y_pred) -> float:\n",
    "    \"\"\"Weighted MAPE: sum(|e|)/sum(|y|).\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    if denom <= 0:\n",
    "        return 200.0\n",
    "    return float(100.0 * (np.sum(np.abs(y_true - y_pred)) / denom))\n",
    "\n",
    "\n",
    "def apply_mapping(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge department_name / vertical using department_id.\"\"\"\n",
    "    merged = incoming.merge(mapping, on='department_id', how='left', suffixes=('', '_map'))\n",
    "    if 'department_name' not in merged.columns:\n",
    "        merged['department_name'] = None\n",
    "    if 'department_name_map' not in merged.columns:\n",
    "        merged['department_name_map'] = None\n",
    "    merged['department_name'] = merged['department_name'].fillna(merged['department_name_map']).fillna(\"Unknown\")\n",
    "\n",
    "    if 'vertical' not in merged.columns:\n",
    "        merged['vertical'] = None\n",
    "    if 'vertical_map' not in merged.columns:\n",
    "        merged['vertical_map'] = None\n",
    "    merged['vertical'] = merged['vertical'].fillna(merged['vertical_map']).fillna(\"Unmapped\")\n",
    "\n",
    "    drop_cols = [c for c in merged.columns if c.endswith('_map')]\n",
    "    merged.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    return merged\n",
    "\n",
    "\n",
    "def winsorize_monthly(ts_m: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Winsorize monthly series to reduce the influence of extreme outliers.\"\"\"\n",
    "    if ts_m.empty:\n",
    "        return ts_m\n",
    "    lo = ts_m.quantile(lower_q)\n",
    "    hi = ts_m.quantile(upper_q)\n",
    "    return ts_m.clip(lower=lo, upper=hi)\n",
    "\n",
    "# ---------- v4.2 safe inverse & dynamic cap ----------\n",
    "\n",
    "def expm1_safe(log_vals: np.ndarray, cap_original: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe inverse of log1p:\n",
    "    - replace non-finite logs by a very negative number (-> ~0)\n",
    "    - lower-bound logs to avoid underflow\n",
    "    - optional cap on original scale applied in log-domain and after expm1\n",
    "    \"\"\"\n",
    "    x = np.array(log_vals, dtype=float)\n",
    "    x[~np.isfinite(x)] = -50.0\n",
    "    x = np.maximum(x, -50.0)\n",
    "\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        log_cap = np.log1p(cap_original)\n",
    "        x = np.minimum(x, log_cap)\n",
    "\n",
    "    y = np.expm1(x)\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        y = np.minimum(y, cap_original)\n",
    "    return np.clip(y, 0, None)\n",
    "\n",
    "\n",
    "def compute_dynamic_cap(ts_m: pd.Series) -> float:\n",
    "    \"\"\"Generous per-department cap on the original scale to prevent explosions.\"\"\"\n",
    "    if ts_m.empty or (ts_m.max() <= 0):\n",
    "        return np.inf\n",
    "    m12 = float(ts_m.tail(12).mean()) if len(ts_m) >= 3 else float(ts_m.mean())\n",
    "    med = float(ts_m.median())\n",
    "    mx = float(ts_m.max())\n",
    "    base = max(1.0, m12, med, 1.1 * mx)\n",
    "    cap = base * 6.0  # adjust 4.0–8.0 as needed\n",
    "    return cap\n",
    "\n",
    "# ---------- v5 additions: rate modelling & robust smoothing ----------\n",
    "\n",
    "def monthly_rate_series(ts_m: pd.Series) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Return (rate_per_workday, workdays series aligned to ts_m).\"\"\"\n",
    "    w = ts_m.index.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month))\n",
    "    w = w.astype(float).replace(0, np.nan)\n",
    "    rate = ts_m / w\n",
    "    return rate, w\n",
    "\n",
    "\n",
    "def robust_roll_cap(series: pd.Series, window: int = 12, K: float = 6.0) -> pd.Series:\n",
    "    \"\"\"Apply rolling Median ± K*MAD cap to stabilize spikes without flattening the series.\"\"\"\n",
    "    s = series.copy().astype(float)\n",
    "    vals = s.values\n",
    "    for i in range(len(s)):\n",
    "        lo = max(0, i - window)\n",
    "        ref = vals[lo:i] if i > 0 else []\n",
    "        if len(ref) >= 4:\n",
    "            med = np.median(ref)\n",
    "            mad = np.median(np.abs(ref - med)) + 1e-9\n",
    "            upper = med + K * mad\n",
    "            lower = max(0.0, med - K * mad)\n",
    "            vals[i] = min(max(vals[i], lower), upper)\n",
    "        else:\n",
    "            vals[i] = max(vals[i], 0.0)\n",
    "    return pd.Series(vals, index=s.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e44024",
   "metadata": {},
   "source": [
    "## 04. Month modelling (log-scale, rate-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a32e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Monthly modelling (log-scale, rate-aware) ====================\n",
    "\n",
    "def fit_prophet_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"Fit Prophet on log1p(ts_m). If is_rate=True, ts_m is a rate series (per workday).\"\"\"\n",
    "    if Prophet is None:\n",
    "        return None, None\n",
    "    y = np.log1p(ts_m.values)\n",
    "    dfp = pd.DataFrame({'ds': ts_m.index.to_timestamp(), 'y': y})\n",
    "    m = Prophet(weekly_seasonality=False, yearly_seasonality=True, daily_seasonality=False)\n",
    "    m.fit(dfp)\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "        future = m.make_future_dataframe(periods=h_months, freq='MS')\n",
    "        pred = m.predict(future)\n",
    "        pred = pred.set_index(pd.PeriodIndex(pred['ds'], freq='M'))['yhat']\n",
    "        pred = pred.iloc[-h_months:]\n",
    "        # for rates: we do not cap in rate-space; just ensure numeric safety\n",
    "        vals = expm1_safe(pred.values, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=pred.index)\n",
    "\n",
    "    return m, fcast\n",
    "\n",
    "\n",
    "def fit_arima_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"\n",
    "    SARIMAX on log1p(ts_m) with a conservative grid. If is_rate=True, ts_m is rate series.\n",
    "    Grid: p,q in {0,1}; d in {0,1} (prefer d=1 for short histories); P,Q in {0,1} if seasonal.\n",
    "    \"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    best_aic, best_model = np.inf, None\n",
    "    pqs = [0, 1]  # conservative\n",
    "    seasonal = len(ts_m) >= 12\n",
    "    PsQs = [0, 1] if seasonal else [0]\n",
    "\n",
    "    for p in pqs:\n",
    "        for d in ([1] if len(ts_m) < 36 else [0, 1]):\n",
    "            for q in pqs:\n",
    "                for P in PsQs:\n",
    "                    for D in ([0, 1] if seasonal else [0]):\n",
    "                        for Q in PsQs:\n",
    "                            try:\n",
    "                                model = SARIMAX(\n",
    "                                    y, order=(p, d, q),\n",
    "                                    seasonal_order=(P, D, Q, 12 if seasonal else 0),\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False\n",
    "                                ).fit(disp=False)\n",
    "                                if model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_model = model\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "        fc_log = best_model.get_forecast(h_months).predicted_mean\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        vals = expm1_safe(fc_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=idx)\n",
    "\n",
    "    return best_model, fcast\n",
    "\n",
    "\n",
    "def fit_tbats_or_ets_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"\n",
    "    TBATS on log1p(ts_m) if available; else ETS(log1p). If is_rate=True, ts_m is rate.\n",
    "    \"\"\"\n",
    "    y_log = np.log1p(ts_m)\n",
    "\n",
    "    if TBATS is not None and len(ts_m) >= 12:\n",
    "        y_log_ts = pd.Series(y_log.values, index=ts_m.index.to_timestamp())\n",
    "        estimator = TBATS(use_arma_errors=False, seasonal_periods=[12])\n",
    "        model = estimator.fit(y_log_ts)\n",
    "\n",
    "        def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "            vals_log = model.forecast(steps=h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "            if is_rate and future_workdays is not None:\n",
    "                vals = vals * future_workdays.values\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "    else:\n",
    "        seasonal = 12 if len(ts_m) >= 24 else None\n",
    "        model = ExponentialSmoothing(y_log, trend='add',\n",
    "                                     seasonal=('add' if seasonal else None),\n",
    "                                     seasonal_periods=seasonal).fit()\n",
    "\n",
    "        def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "            vals_log = model.forecast(h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "            if is_rate and future_workdays is not None:\n",
    "                vals = vals * future_workdays.values\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "\n",
    "def fit_ets_damped_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"ETS with damped trend on log1p(ts_m); stable candidate for blending.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    seasonal = 12 if len(ts_m) >= 24 else None\n",
    "    model = ExponentialSmoothing(y, trend='add', damped_trend=True,\n",
    "                                 seasonal=('add' if seasonal else None),\n",
    "                                 seasonal_periods=seasonal).fit()\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "        vals_log = model.forecast(h_months)\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=idx)\n",
    "\n",
    "    return model, fcast\n",
    "\n",
    "# ==================== Adaptive CV (rate-aware) ====================\n",
    "\n",
    "def rolling_cv_monthly_adaptive_rate(ts_vol: pd.Series) -> Tuple[Optional[Dict[str, float]], Optional[Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Adaptive rolling-origin CV using rate modelling internally:\n",
    "    - If n >= 15 -> h=3; If 9 <= n < 15 -> h=1\n",
    "    Returns two dicts: (mean sMAPE per model, mean wMAPE per model).\n",
    "    Non-finite predictions in a fold are penalized with 200 (worst-case).\n",
    "    \"\"\"\n",
    "    n = len(ts_vol)\n",
    "    if n < 9:\n",
    "        return None, None\n",
    "    h = 3 if n >= 15 else 1\n",
    "    min_train = max(12, n - (h + 2))\n",
    "\n",
    "    s_out, w_out = [], []\n",
    "    for start in range(min_train, n - h + 1):\n",
    "        train_vol = ts_vol.iloc[:start]\n",
    "        test_vol = ts_vol.iloc[start:start + h]\n",
    "\n",
    "        # Build rate for training and workdays for future h months (relative to the training end)\n",
    "        train_rate, _ = monthly_rate_series(train_vol)\n",
    "        future_idx = pd.period_range(train_vol.index[-1] + 1, periods=h, freq='M')\n",
    "        future_w = future_idx.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month)).astype(float)\n",
    "\n",
    "        s_metrics, w_metrics = {}, {}\n",
    "\n",
    "        # Prophet\n",
    "        mp, fp = fit_prophet_monthly_log(train_rate, is_rate=True)\n",
    "        if fp is not None:\n",
    "            try:\n",
    "                pred_vol = fp(h_months=h, future_workdays=future_w)\n",
    "                pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "                pv[~np.isfinite(pv)] = np.nan\n",
    "                s_metrics['Prophet'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "                w_metrics['Prophet'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            except Exception:\n",
    "                s_metrics['Prophet'] = 200.0; w_metrics['Prophet'] = 200.0\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            ma, fa = fit_arima_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = fa(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['ARIMA'] = 200.0; w_metrics['ARIMA'] = 200.0\n",
    "\n",
    "        # TBATS/ETS\n",
    "        try:\n",
    "            mt, ft = fit_tbats_or_ets_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = ft(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['TBATS/ETS'] = 200.0; w_metrics['TBATS/ETS'] = 200.0\n",
    "\n",
    "        # ETS Damped (for blending only; not displayed in CV sheet)\n",
    "        try:\n",
    "            me, fe = fit_ets_damped_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = fe(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['ETS_Damped'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['ETS_Damped'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['ETS_Damped'] = 200.0; w_metrics['ETS_Damped'] = 200.0\n",
    "\n",
    "        s_out.append(s_metrics); w_out.append(w_metrics)\n",
    "\n",
    "    sm = pd.DataFrame(s_out).mean().to_dict()\n",
    "    wm = pd.DataFrame(w_out).mean().to_dict()\n",
    "    return sm, wm\n",
    "\n",
    "# ==================== Blending ====================\n",
    "\n",
    "def select_or_blend_forecasts(fc_dict: Dict[str, pd.Series],\n",
    "                              cv_scores_wmape: Dict[str, float],\n",
    "                              blend: bool = True):\n",
    "    \"\"\"\n",
    "    Blend using 1/wMAPE as weights (lower better). If blending not possible, select min wMAPE.\n",
    "    \"\"\"\n",
    "    scores = {k: (v if v is not None and np.isfinite(v) else 1e6) for k, v in cv_scores_wmape.items()}\n",
    "    # Filter fc_dict to models that have a score\n",
    "    models = [m for m in fc_dict.keys() if m in scores]\n",
    "    if not models:\n",
    "        # fallback to any available key\n",
    "        k0 = list(fc_dict.keys())[0]\n",
    "        return fc_dict[k0], {'winner': k0, 'weights': {k0: 1.0}}\n",
    "\n",
    "    if not blend:\n",
    "        best = min(models, key=lambda m: scores[m])\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "\n",
    "    inv = {m: (1.0 / scores[m] if scores[m] > 0 else 0.0) for m in models}\n",
    "    total = sum(inv.values())\n",
    "    if total == 0:\n",
    "        best = min(models, key=lambda m: scores[m])\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "    w = {m: inv[m] / total for m in models}\n",
    "\n",
    "    # Align indices and blend\n",
    "    idx = None\n",
    "    for s in fc_dict.values():\n",
    "        idx = s.index if idx is None else idx.union(s.index)\n",
    "    blended = sum(w[m] * fc_dict[m].reindex(idx).fillna(0) for m in models)\n",
    "    return blended, {'winner': min(models, key=lambda m: scores[m]), 'weights': w}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d817804c",
   "metadata": {},
   "source": [
    "## 05. Monthly pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5909fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_monthly_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate daily incoming to monthly by department.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['month'] = df['Date'].dt.to_period('M')\n",
    "    monthly = (df.groupby(['department_id', 'month'], as_index=False)['ticket_total']\n",
    "               .sum()\n",
    "               .rename(columns={'ticket_total': 'incoming_monthly'}))\n",
    "    return monthly\n",
    "\n",
    "\n",
    "def forecast_per_department_monthly(monthly: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hybrid modelling per department (rate-aware) + adaptive CV + robust sanitation.\n",
    "    Returns columns for capacity_error and mape_table_cv.\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    dept_ids = monthly['department_id'].unique().tolist()\n",
    "\n",
    "    for dept in dept_ids:\n",
    "        ts_vol = (monthly.loc[monthly['department_id'] == dept, ['month', 'incoming_monthly']]\n",
    "                  .sort_values('month')\n",
    "                  .set_index('month')['incoming_monthly'])\n",
    "        if not pd.api.types.is_period_dtype(ts_vol.index):\n",
    "            ts_vol.index = pd.PeriodIndex(ts_vol.index, freq='M')\n",
    "        if len(ts_vol) == 0:\n",
    "            continue\n",
    "\n",
    "        # Winsorize volumes and build rate series\n",
    "        ts_vol = winsorize_monthly(ts_vol, 0.01, 0.99)\n",
    "        ts_rate, _ = monthly_rate_series(ts_vol)\n",
    "        ts_rate = ts_rate.fillna(ts_rate.median()).clip(lower=0)\n",
    "\n",
    "        # Future workdays index\n",
    "        future_idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "        future_w = future_idx.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month)).astype(float)\n",
    "\n",
    "        cv_smape, cv_wmape = {}, {}\n",
    "\n",
    "        # Collect forecasts (on volume scale returned by each fcast with is_rate=True)\n",
    "        fc_dict: Dict[str, pd.Series] = {}\n",
    "\n",
    "        # Prophet\n",
    "        if Prophet is not None and len(ts_rate) >= 12:\n",
    "            try:\n",
    "                _, fp = fit_prophet_monthly_log(ts_rate, is_rate=True)\n",
    "                if fp is not None:\n",
    "                    fc_dict['Prophet'] = fp(H_MONTHS, future_workdays=future_w)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            _, fa = fit_arima_monthly_log(ts_rate, is_rate=True)\n",
    "            fc_dict['ARIMA'] = fa(H_MONTHS, future_workdays=future_w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # TBATS/ETS\n",
    "        try:\n",
    "            _, ft = fit_tbats_or_ets_monthly_log(ts_rate, is_rate=True)\n",
    "            fc_dict['TBATS/ETS'] = ft(H_MONTHS, future_workdays=future_w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # ETS Damped (candidate for blending & CV; not shown in CV sheet columns)\n",
    "        try:\n",
    "            _, fe = fit_ets_damped_monthly_log(ts_rate, is_rate=True)\n",
    "            fc_dict['ETS_Damped'] = fe(H_MONTHS, future_workdays=future_w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not fc_dict:\n",
    "            idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "            val = max(0.0, float(ts_vol.mean()))\n",
    "            fc_dict['NaiveMean'] = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # CV metrics (mean) using rate-aware CV\n",
    "        try:\n",
    "            cv_smape, cv_wmape = rolling_cv_monthly_adaptive_rate(ts_vol)\n",
    "            cv_smape = cv_smape or {}\n",
    "            cv_wmape = cv_wmape or {}\n",
    "        except Exception:\n",
    "            cv_smape, cv_wmape = {}, {}\n",
    "\n",
    "        # Blend/select using wMAPE\n",
    "        blended, meta = select_or_blend_forecasts(fc_dict, cv_scores_wmape=cv_wmape, blend=True)\n",
    "\n",
    "        # Enforce finiteness\n",
    "        if not np.isfinite(blended.values).all():\n",
    "            finite_mask = np.isfinite(blended.values)\n",
    "            if finite_mask.any():\n",
    "                finite_mean = float(np.nanmean(blended.values[finite_mask]))\n",
    "                vals = np.where(finite_mask, blended.values, finite_mean)\n",
    "                blended = pd.Series(vals, index=blended.index)\n",
    "            else:\n",
    "                idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "                val = max(0.0, float(ts_vol.mean()))\n",
    "                blended = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # Robust smoothing (Median ± K·MAD) to kill residual spikes\n",
    "        blended = robust_roll_cap(blended, window=12, K=6.0)\n",
    "\n",
    "        # Optional growth guard vs local ref\n",
    "        if APPLY_LOCAL_GROWTH_GUARD:\n",
    "            ref = max(1.0, float(ts_vol.tail(12).mean())) if len(ts_vol) else 1.0\n",
    "            blended = blended.clip(lower=ref * MIN_GROWTH, upper=ref * MAX_GROWTH)\n",
    "\n",
    "        # Prepare output rows (store sMAPE CV columns; weights from wMAPE-based blend)\n",
    "        # Extract weights for sheet columns (Prophet/ARIMA/TBATS/ETS only)\n",
    "        w_prophet = meta['weights'].get('Prophet', np.nan)\n",
    "        w_arima = meta['weights'].get('ARIMA', np.nan)\n",
    "        w_tbats = meta['weights'].get('TBATS/ETS', np.nan)\n",
    "\n",
    "        for per, val in blended.items():\n",
    "            out_rows.append({\n",
    "                'department_id': dept,\n",
    "                'month': per,\n",
    "                'forecast_monthly': max(0.0, float(val)),\n",
    "                'cv_prophet_smape': cv_smape.get('Prophet', np.nan),\n",
    "                'cv_arima_smape': cv_smape.get('ARIMA', np.nan),\n",
    "                'cv_tbats_ets_smape': cv_smape.get('TBATS/ETS', np.nan),\n",
    "                'winner_model': meta['winner'],\n",
    "                'blend_prophet_w': w_prophet,\n",
    "                'blend_arima_w': w_arima,\n",
    "                'blend_tbats_ets_w': w_tbats,\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows)\n",
    "    if not df_out.empty:\n",
    "        df_out['department_id'] = df_out['department_id'].astype(str)\n",
    "        if not pd.api.types.is_period_dtype(df_out['month']):\n",
    "            df_out['month'] = pd.PeriodIndex(df_out['month'], freq='M')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def compute_monthly_accuracy_with_history(monthly: pd.DataFrame,\n",
    "                                          fc_monthly: pd.DataFrame,\n",
    "                                          report_start: str) -> pd.DataFrame:\n",
    "    \"\"\"Build capacity_error-like table with historical Actuals and future Forecasts.\"\"\"\n",
    "    monthly = monthly.copy()\n",
    "    monthly['department_id'] = monthly['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(monthly['month']):\n",
    "        monthly['month'] = pd.PeriodIndex(monthly['month'], freq='M')\n",
    "\n",
    "    fc = fc_monthly.copy()\n",
    "    fc['department_id'] = fc['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(fc['month']):\n",
    "        fc['month'] = pd.PeriodIndex(fc['month'], freq='M')\n",
    "\n",
    "    start_per = pd.Period(report_start, freq='M')\n",
    "    last_actual = monthly['month'].max()\n",
    "\n",
    "    hist = (monthly.loc[monthly['month'] >= start_per, ['department_id', 'month', 'incoming_monthly']]\n",
    "            .rename(columns={'incoming_monthly': 'Actual_Volume'}))\n",
    "    hist['Forecast'] = np.nan\n",
    "\n",
    "    fut = fc[['department_id', 'month', 'forecast_monthly',\n",
    "              'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "              'winner_model', 'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']].copy()\n",
    "    fut = fut.loc[fut['month'] > last_actual]\n",
    "    fut = fut.rename(columns={'forecast_monthly': 'Forecast'})\n",
    "    fut['Actual_Volume'] = np.nan\n",
    "\n",
    "    base = pd.concat([hist, fut], ignore_index=True, sort=False)\n",
    "\n",
    "    base['Forecast_Accuracy'] = np.where(\n",
    "        (base['Actual_Volume'].notna()) & (base['Forecast'].notna()) & (base['Actual_Volume'] > 0),\n",
    "        (1 - (np.abs(base['Forecast'] - base['Actual_Volume']) / base['Actual_Volume'])) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    return base\n",
    "\n",
    "\n",
    "def compute_capacity_monthly(cap_df: pd.DataFrame, prod_dept: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute FTE/day needed per month = Forecast / (avg_tickets_per_agent_day * workdays_in_month).\"\"\"\n",
    "    out = cap_df.merge(prod_dept, on='department_id', how='left')\n",
    "    out['avg_tickets_per_agent_day'] = pd.to_numeric(out['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    out['avg_tickets_per_agent_day'] = out['avg_tickets_per_agent_day'].replace(0, np.nan)\n",
    "    out['workdays_in_month'] = [business_days_in_month(m.start_time.year, m.start_time.month) for m in out['month']]\n",
    "    out['Capacity_FTE_per_day'] = np.where(\n",
    "        (out['avg_tickets_per_agent_day'] > 0) & (out['workdays_in_month'] > 0) & (out['Forecast'].notna()),\n",
    "        out['Forecast'] / (out['avg_tickets_per_agent_day'] * out['workdays_in_month']),\n",
    "        np.nan\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_cv_table(fc_monthly: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build mape_table_cv with sMAPE, best model and weights (from wMAPE-based blend).\"\"\"\n",
    "    if fc_monthly is None or fc_monthly.empty:\n",
    "        raise ValueError(\"fc_monthly is empty; cannot build CV table.\")\n",
    "    cols_keep = [\n",
    "        'department_id',\n",
    "        'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "        'winner_model',\n",
    "        'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w'\n",
    "    ]\n",
    "    df = (fc_monthly[cols_keep]\n",
    "          .drop_duplicates(subset=['department_id'])\n",
    "          .copy())\n",
    "    df = df.rename(columns={\n",
    "        'cv_prophet_smape': 'sMAPE_Prophet_CV',\n",
    "        'cv_arima_smape': 'sMAPE_ARIMA_CV',\n",
    "        'cv_tbats_ets_smape': 'sMAPE_TBATS_ETS_CV',\n",
    "        'winner_model': 'Best_Model',\n",
    "        'blend_prophet_w': 'Weight_Prophet',\n",
    "        'blend_arima_w': 'Weight_ARIMA',\n",
    "        'blend_tbats_ets_w': 'Weight_TBATS_ETS',\n",
    "    })\n",
    "    df['department_id'] = df['department_id'].astype(str)\n",
    "    df = apply_mapping(df, mapping)\n",
    "    ordered_cols = [\n",
    "        'department_id', 'department_name', 'vertical',\n",
    "        'sMAPE_Prophet_CV', 'sMAPE_ARIMA_CV', 'sMAPE_TBATS_ETS_CV',\n",
    "        'Best_Model',\n",
    "        'Weight_Prophet', 'Weight_ARIMA', 'Weight_TBATS_ETS'\n",
    "    ]\n",
    "    df = df[ordered_cols]\n",
    "    return df.sort_values(['vertical', 'department_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2c96d",
   "metadata": {},
   "source": [
    "## 06. Daily pipeline (reconciled with monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "befb3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dow_profile(g: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Build normalized day-of-week profile for a department, fallback to uniform.\"\"\"\n",
    "    prof = (g.assign(dow=g['Date'].dt.dayofweek)\n",
    "              .groupby('dow')['ticket_total']\n",
    "              .mean())\n",
    "    if prof.notna().sum() >= 3:\n",
    "        prof = prof / prof.mean()\n",
    "    else:\n",
    "        prof = pd.Series(1.0, index=range(7))\n",
    "    return prof\n",
    "\n",
    "\n",
    "def disaggregate_month_to_days(dept_df: pd.DataFrame,\n",
    "                               month_period: pd.Period,\n",
    "                               target_sum: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Allocate monthly forecast to each day in that month using recent DOW profile,\n",
    "    guaranteeing that daily sum equals the monthly target (within rounding).\n",
    "    \"\"\"\n",
    "    start = month_period.start_time\n",
    "    end = month_period.end_time\n",
    "    days = pd.date_range(start=start, end=end, freq='D')\n",
    "\n",
    "    hist = dept_df.sort_values('Date').tail(90)\n",
    "    profile = dow_profile(hist)\n",
    "\n",
    "    weights = np.array([profile.get(d.dayofweek, 1.0) for d in days], dtype=float)\n",
    "    weights = np.maximum(weights, 1e-6)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    alloc = target_sum * weights\n",
    "    return pd.DataFrame({'Date': days, 'forecast_daily': alloc})\n",
    "\n",
    "\n",
    "def build_daily_from_monthly(incoming: pd.DataFrame,\n",
    "                             fc_monthly: pd.DataFrame,\n",
    "                             horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Top-down daily plan: disaggregate monthly forecast using DOW profile for days within horizon.\n",
    "    \"\"\"\n",
    "    last_date = incoming['Date'].max()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    end = start + pd.Timedelta(days=horizon_days - 1)\n",
    "    future_months = pd.period_range(start=start.to_period('M'),\n",
    "                                    end=end.to_period('M'), freq='M')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in incoming.groupby('department_id'):\n",
    "        for m in future_months:\n",
    "            fcm = fc_monthly[(fc_monthly['department_id'] == dept) & (fc_monthly['month'] == m)]\n",
    "            if fcm.empty:\n",
    "                continue\n",
    "            target = float(fcm['forecast_monthly'].iloc[0])\n",
    "            if target <= 0:\n",
    "                continue\n",
    "            alloc_df = disaggregate_month_to_days(g, m, target)\n",
    "            alloc_df = alloc_df[(alloc_df['Date'] >= start) & (alloc_df['Date'] <= end)]\n",
    "            alloc_df.insert(0, 'department_id', dept)\n",
    "            rows.append(alloc_df)\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_daily_by_language(df_daily_fc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Split daily forecast by fixed language shares.\"\"\"\n",
    "    parts = []\n",
    "    for lang, w in LANGUAGE_SHARES.items():\n",
    "        tmp = df_daily_fc.copy()\n",
    "        tmp['language'] = lang\n",
    "        tmp['forecast_daily_language'] = tmp['forecast_daily'] * w\n",
    "        parts.append(tmp)\n",
    "    out = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    return out\n",
    "\n",
    "\n",
    "def forecast_daily_baseline(df_daily: pd.DataFrame, horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Independent daily baseline (kept as optional).\"\"\"\n",
    "    df = df_daily.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df = df.sort_values(['department_id', 'Date'])\n",
    "    last_date = df['Date'].max()\n",
    "    if pd.isna(last_date):\n",
    "        raise ValueError(\"forecast_daily_baseline: No valid dates in incoming.\")\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    idx_future = pd.date_range(start=start, periods=horizon_days, freq='D')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in df.groupby('department_id'):\n",
    "        g = g.sort_values('Date')\n",
    "        if len(g) >= 28:\n",
    "            roll_mean = (g.set_index('Date')['ticket_total']\n",
    "                         .rolling(window=28, min_periods=1)\n",
    "                         .mean()\n",
    "                         .iloc[-1])\n",
    "            base = float(roll_mean) if np.isfinite(roll_mean) else float(g['ticket_total'].mean())\n",
    "        else:\n",
    "            base = float(g['ticket_total'].mean())\n",
    "\n",
    "        prof = dow_profile(g)\n",
    "        vals = []\n",
    "        for d in idx_future:\n",
    "            w = prof[d.dayofweek] if d.dayofweek in prof.index else 1.0\n",
    "            vals.append(max(0.0, base * float(w)))\n",
    "        rows.append(pd.DataFrame({'department_id': dept, 'Date': idx_future, 'forecast_daily': vals}))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "\n",
    "\n",
    "def build_daily_capacity_plan(incoming: pd.DataFrame,\n",
    "                              mapping: pd.DataFrame,\n",
    "                              prod_dept: pd.DataFrame,\n",
    "                              fc_monthly: pd.DataFrame,\n",
    "                              horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    End-to-end daily plan:\n",
    "    - If USE_DAILY_FROM_MONTHLY: disaggregate monthly forecast (reconciled)\n",
    "    - Else: robust daily baseline (28D moving average)\n",
    "    - Split by languages\n",
    "    - Attach department_name / vertical\n",
    "    - Compute FTE per day per department/language\n",
    "    \"\"\"\n",
    "    if USE_DAILY_FROM_MONTHLY:\n",
    "        daily_fc = build_daily_from_monthly(incoming, fc_monthly, horizon_days)\n",
    "    else:\n",
    "        daily_fc = forecast_daily_baseline(incoming, horizon_days)\n",
    "\n",
    "    daily_fc_lang = split_daily_by_language(daily_fc)\n",
    "\n",
    "    # Attach names/vertical\n",
    "    daily_fc_lang = apply_mapping(daily_fc_lang, mapping)\n",
    "\n",
    "    # Merge productivity\n",
    "    daily_fc_lang = daily_fc_lang.merge(prod_dept, on='department_id', how='left')\n",
    "\n",
    "    # Compute FTE requirement per day\n",
    "    daily_fc_lang['avg_tickets_per_agent_day'] = pd.to_numeric(daily_fc_lang['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    daily_fc_lang['FTE_per_day'] = np.where(\n",
    "        daily_fc_lang['avg_tickets_per_agent_day'] > 0,\n",
    "        daily_fc_lang['forecast_daily_language'] / daily_fc_lang['avg_tickets_per_agent_day'],\n",
    "        np.nan\n",
    "    )\n",
    "    cols = ['Date', 'department_id', 'department_name', 'vertical', 'language',\n",
    "            'forecast_daily_language', 'FTE_per_day']\n",
    "    daily_plan = daily_fc_lang[cols].sort_values(['Date', 'vertical', 'department_id', 'language'])\n",
    "    return daily_plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107327c",
   "metadata": {},
   "source": [
    "## 07. Main & Excel writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa103d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:33:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:33:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:33:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:33:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:34:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:34:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:34:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:34:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:34:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:34:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:35:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:35:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:35:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:35:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:36:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:36:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:37:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:37:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:38:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:38:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:38:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:38:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:39:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:39:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:39:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:39:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:40:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:40:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:40:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:41:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:41:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:41:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:42:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:42:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:43:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:43:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:43:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:43:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:44:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:44:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:45:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:45:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:45:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:45:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:45:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:45:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:47:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:47:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:47:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:47:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:47:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:47:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:48:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:48:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:48:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:48:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:49:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:49:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:49:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:49:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:51:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:51:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:51:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:51:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:51:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:51:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:52:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:52:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:52:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:52:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:53:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:53:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:53:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:53:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:54:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:54:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:54:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:54:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:54:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:54:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:56:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:56:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:56:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:56:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:57:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:57:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:57:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:57:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:58:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:58:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:58:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:58:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:59:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:59:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:03:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:04:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:04:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:04:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:07:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:07:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:07:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:07:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:08:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:08:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:08:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:08:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) Load inputs\n",
    "    incoming = load_incoming(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET)\n",
    "    mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "    prod = load_productivity(PRODUCTIVITY_PATH)\n",
    "\n",
    "    # 2) Monthly forecast (rate-aware)\n",
    "    monthly = build_monthly_series(incoming)\n",
    "    fc_monthly = forecast_per_department_monthly(monthly)\n",
    "\n",
    "    # 3) capacity_error (historicals + future forecast)\n",
    "    cap_err = compute_monthly_accuracy_with_history(monthly, fc_monthly, REPORT_START_MONTH)\n",
    "    cap_err = compute_capacity_monthly(cap_err, prod)\n",
    "    cap_err = apply_mapping(cap_err, mapping)\n",
    "\n",
    "    # 4) Daily plan (reconciled with monthly by default)\n",
    "    daily_capacity_plan = build_daily_capacity_plan(incoming, mapping, prod, fc_monthly, DAILY_HORIZON_DAYS)\n",
    "\n",
    "    # 5) CV table\n",
    "    cv_table = build_cv_table(fc_monthly, mapping)\n",
    "\n",
    "    # 6) Ensure no inf/-inf propagate to Excel\n",
    "    for df_out in [cap_err, daily_capacity_plan, cv_table]:\n",
    "        df_out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 7) Write Excel with required sheet names\n",
    "    with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        (cap_err[['vertical', 'department_id', 'department_name', 'month',\n",
    "                  'Actual_Volume', 'Forecast', 'Forecast_Accuracy',\n",
    "                  'Capacity_FTE_per_day',\n",
    "                  'winner_model', 'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "                  'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']]\n",
    "         .sort_values(['vertical', 'department_id', 'month'])\n",
    "         .to_excel(w, \"capacity_error\", index=False))\n",
    "\n",
    "        daily_capacity_plan.to_excel(w, \"daily_capacity_plan\", index=False)\n",
    "        cv_table.to_excel(w, \"mape_table_cv\", index=False)\n",
    "\n",
    "    print(\"Excel written:\", OUTPUT_XLSX)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
