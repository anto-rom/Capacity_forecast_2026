{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a28eb07",
   "metadata": {},
   "source": [
    "# Corporate Hybrid Forecast Notebook — v6_4 (Hybrid + Global ML + Exogenous)\n",
    "\n",
    "**Generated:** 2026-02-09 09:13 UTC  \n",
    "This notebook extends the existing hybrid pipeline (Prophet / ARIMA / TBATS-ETS) with:\n",
    "\n",
    "- Weekly aggregation candidate for noisy series (Thu→Wed weeks)\n",
    "- Robust outlier detection/correction **before** modeling\n",
    "- **Global LightGBM per vertical** with temporal features (multi-series)\n",
    "- Naive and seasonal-naive baselines\n",
    "- Dynamic language splitting (use actual language shares when available; else fixed shares)\n",
    "- **Exogenous features**: EU core holidays & events (auto generator or optional CSV files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a32a10",
   "metadata": {},
   "source": [
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75978c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.13.3' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Forecasting libraries\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Prophet\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    try:\n",
    "        from fbprophet import Prophet  # legacy\n",
    "    except Exception:\n",
    "        Prophet = None\n",
    "\n",
    "# TBATS\n",
    "try:\n",
    "    from tbats import TBATS\n",
    "except Exception:\n",
    "    TBATS = None\n",
    "\n",
    "# Machine Learning\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "except Exception:\n",
    "    lgb = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\"\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"input_model\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "\n",
    "INCOMING_SOURCE_PATH = os.path.join(INPUT_DIR, \"Incoming_new.xlsx\")  # Sheet 'Main'\n",
    "INCOMING_SHEET = \"Main\"\n",
    "\n",
    "DEPT_MAP_PATH = os.path.join(INPUT_DIR, \"department.xlsx\")\n",
    "DEPT_MAP_SHEET = \"map\"\n",
    "\n",
    "PRODUCTIVITY_PATH = os.path.join(INPUT_DIR, \"productivity_agents.xlsx\")\n",
    "\n",
    "OUTPUT_XLSX = os.path.join(OUTPUT_DIR, \"capacity_forecast_hybrid.xlsx\")\n",
    "\n",
    "# Horizons\n",
    "H_MONTHS = 12           # monthly horizon\n",
    "DAILY_HORIZON_DAYS = 90 # daily plan horizon\n",
    "REPORT_START_MONTH = \"2025-01\"\n",
    "\n",
    "# Reconciliation daily (top-down from monthly)\n",
    "USE_DAILY_FROM_MONTHLY = True\n",
    "\n",
    "# Organization-specific week: Thursday→Wednesday (end on Wed)\n",
    "WEEKLY_FREQ = \"W-WED\"\n",
    "\n",
    "# Language handling: \"from_column\" (keep Incoming language) or \"fixed_shares\"\n",
    "LANGUAGE_STRATEGY = \"from_column\"\n",
    "\n",
    "# Fixed language shares (fallback)\n",
    "LANGUAGE_SHARES = {\n",
    "    \"English\": 0.6435, \"French\": 0.0741, \"German\": 0.0860,\n",
    "    \"Italian\": 0.0667, \"Portuguese\": 0.0162, \"Spanish\": 0.1135\n",
    "}\n",
    "\n",
    "# Outliers & noisy series\n",
    "OUTLIER_METHOD = \"IQR\"  # \"IQR\" or \"STL\"\n",
    "IQR_LO = 0.01\n",
    "IQR_HI = 0.99\n",
    "NOISE_SCORE_THRESH = 1.00  # robust CV>1 => consider \"high-noise\"\n",
    "\n",
    "# Weekly modeling for noisy series (candidate for the ensemble)\n",
    "ENABLE_WEEKLY_CANDIDATE = True\n",
    "\n",
    "# Global model per vertical (LightGBM)\n",
    "ENABLE_GLOBAL_LGB = True\n",
    "LGB_LAGS = [7, 14, 28]\n",
    "LGB_ROLLS = [7, 28]\n",
    "LGB_N_ESTIMATORS = 400\n",
    "LGB_LEARNING_RATE = 0.04\n",
    "\n",
    "# ==================== Exogenous (Holidays / Events) ====================\n",
    "EXOG_ENABLE = True\n",
    "EXOG_SOURCE = \"auto_core\"  # \"auto_core\" | \"files\"\n",
    "EXOG_FILES = {\n",
    "    \"holidays\": os.path.join(INPUT_DIR, \"holidays_eu.csv\"),  # optional\n",
    "    \"events\": os.path.join(INPUT_DIR, \"events_eu.csv\"),      # optional\n",
    "}\n",
    "# Countries for auto_core generator (EU core set) - not used country-specific in minimal rules\n",
    "EXOG_COUNTRIES = [\"ES\", \"FR\", \"DE\", \"IT\", \"PT\", \"GB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda5749",
   "metadata": {},
   "source": [
    "## 2. Helpers, metrics and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29794023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Utilities ====================\n",
    "\n",
    "def smape(y_true, y_pred) -> float:\n",
    "    \"\"\"Symmetric MAPE in percentage.\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom[denom == 0] = 1.0\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "def smape_df(df: pd.DataFrame, y_col: str, yhat_col: str) -> float:\n",
    "    if df.empty:\n",
    "        return np.nan\n",
    "    return smape(df[y_col].values, df[yhat_col].values)\n",
    "\n",
    "def business_days_in_month(year: int, month: int) -> int:\n",
    "    \"\"\"Count business days (Mon-Fri) in a given month.\"\"\"\n",
    "    rng = pd.date_range(\n",
    "        start=pd.Timestamp(year=year, month=month, day=1),\n",
    "        end=pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(0),\n",
    "        freq=\"D\"\n",
    "    )\n",
    "    return int(np.sum(rng.weekday < 5))\n",
    "\n",
    "def _easter_sunday(year: int) -> date:\n",
    "    \"\"\"Western Easter (Computus). Return date.\"\"\"\n",
    "    a = year % 19\n",
    "    b = year // 100\n",
    "    c = year % 100\n",
    "    d = b // 4\n",
    "    e = b % 4\n",
    "    f = (b + 8) // 25\n",
    "    g = (b - f + 1) // 3\n",
    "    h = (19 * a + b - d - g + 15) % 30\n",
    "    i = c // 4\n",
    "    k = c % 4\n",
    "    L = (32 + 2 * e + 2 * i - h - k) % 7\n",
    "    m = (a + 11 * h + 22 * L) // 451\n",
    "    month = (h + L - 7 * m + 114) // 31\n",
    "    day = ((h + L - 7 * m + 114) % 31) + 1\n",
    "    return date(year, month, day)\n",
    "\n",
    "def _last_friday_of_november(year: int) -> date:\n",
    "    d = date(year, 11, 30)\n",
    "    while d.weekday() != 4:  # 4=Friday\n",
    "        d = d.replace(day=d.day - 1)\n",
    "    return d\n",
    "\n",
    "def expm1_safe(x, cap_original: Optional[float] = None):\n",
    "    \"\"\"Stable expm1 with optional capping on original scale.\"\"\"\n",
    "    a = np.array(x, dtype=float)\n",
    "    a[~np.isfinite(a)] = -50.0\n",
    "    a = np.maximum(a, -50.0)\n",
    "    if cap_original and np.isfinite(cap_original) and cap_original > 0:\n",
    "        log_cap = np.log1p(cap_original)\n",
    "        a = np.minimum(a, log_cap)\n",
    "    y = np.expm1(a)\n",
    "    if cap_original and np.isfinite(cap_original) and cap_original > 0:\n",
    "        y = np.minimum(y, cap_original)\n",
    "    return np.clip(y, 0, None)\n",
    "\n",
    "def compute_dynamic_cap(ts_m: pd.Series) -> float:\n",
    "    \"\"\"Soft cap to prevent explosions; 6x over robust baseline.\"\"\"\n",
    "    if ts_m.empty or (ts_m.max() <= 0):\n",
    "        return np.inf\n",
    "    m12 = float(ts_m.tail(12).mean()) if len(ts_m) >= 3 else float(ts_m.mean())\n",
    "    med, mx = float(ts_m.median()), float(ts_m.max())\n",
    "    base = max(1.0, m12, med, 1.1 * mx)\n",
    "    return base * 6.0\n",
    "\n",
    "def coalesce_language(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Use actual language column when available; else split using fixed shares.\"\"\"\n",
    "    if 'language' in df.columns and df['language'].notna().any() and LANGUAGE_STRATEGY == \"from_column\":\n",
    "        df['language'] = (\n",
    "            df['language']\n",
    "            .astype(str).str.strip()\n",
    "            .replace({'nan': None, 'None': None})\n",
    "            .fillna('English')\n",
    "        )\n",
    "        return df\n",
    "    parts = []\n",
    "    for lang, w in LANGUAGE_SHARES.items():\n",
    "        tmp = df.copy()\n",
    "        tmp['language'] = lang\n",
    "        tmp['ticket_total'] = tmp['ticket_total'] * float(w)\n",
    "        parts.append(tmp)\n",
    "    return pd.concat(parts, ignore_index=True) if parts else df\n",
    "\n",
    "def noise_score_daily(g: pd.DataFrame) -> float:\n",
    "    \"\"\"Robust daily noise score combining robust-CV and spike ratio.\"\"\"\n",
    "    s = g.sort_values('Date')['ticket_total'].astype(float)\n",
    "    if len(s) < 30:\n",
    "        return 0.0\n",
    "    med = float(np.median(s))\n",
    "    if med <= 0:\n",
    "        return 0.0\n",
    "    mad = float(np.median(np.abs(s - med)))\n",
    "    robust_cv = (1.4826 * mad) / med\n",
    "    p95 = float(np.percentile(s, 95))\n",
    "    spike_ratio = (p95 / med) if med > 0 else 0.0\n",
    "    return float(0.7 * robust_cv + 0.3 * (spike_ratio - 1.0))\n",
    "\n",
    "def clean_outliers_daily(g: pd.DataFrame, method=\"IQR\") -> pd.DataFrame:\n",
    "    \"\"\"Daily outlier correction: STL-residual clamp or IQR winsorization.\"\"\"\n",
    "    g = g.copy()\n",
    "    s = g.sort_values('Date')['ticket_total'].astype(float)\n",
    "    if method.upper() == \"STL\" and len(s) >= 60:\n",
    "        ts = pd.Series(s.values, index=g.sort_values('Date')['Date'])\n",
    "        try:\n",
    "            stl = STL(ts, period=7, robust=True).fit()\n",
    "            resid = stl.resid\n",
    "            rstd = float(np.std(resid))\n",
    "            y = np.where(np.abs(resid) > 3 * rstd, ts - np.sign(resid) * 3 * rstd, ts)\n",
    "            g.loc[ts.index, 'ticket_total'] = np.clip(y, 0, None)\n",
    "            return g\n",
    "        except Exception:\n",
    "            pass\n",
    "    # IQR/winsorize-like clamp at 1%/99%\n",
    "    ql = s.quantile(0.01)\n",
    "    qh = s.quantile(0.99)\n",
    "    g['ticket_total'] = s.clip(ql, qh).values\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539ba02",
   "metadata": {},
   "source": [
    "## 3. Exogenous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Exogenous features ====================\n",
    "\n",
    "def build_eu_core_holidays(start_date: pd.Timestamp, end_date: pd.Timestamp,\n",
    "                           countries: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimal EU-core holiday/event calendar for [start, end].\n",
    "    - Holidays: NewYear, GoodFriday, EasterMonday, LabourDay (May 1), Christmas, BoxingDay.\n",
    "    - Events: BlackFriday, CyberMonday.\n",
    "    We don't differentiate by country here to keep it robust.\n",
    "    \"\"\"\n",
    "    years = list(range(start_date.year, end_date.year + 1))\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        # Holidays\n",
    "        rows.append({\"ds\": date(y, 1, 1), \"name\": \"NewYear\", \"type\": \"holiday\", \"weight\": 1.0})\n",
    "        easter = _easter_sunday(y)\n",
    "        rows.append({\"ds\": easter - timedelta(days=2), \"name\": \"GoodFriday\", \"type\": \"holiday\", \"weight\": 1.0})\n",
    "        rows.append({\"ds\": easter + timedelta(days=1), \"name\": \"EasterMonday\", \"type\": \"holiday\", \"weight\": 1.0})\n",
    "        rows.append({\"ds\": date(y, 5, 1), \"name\": \"LabourDay\", \"type\": \"holiday\", \"weight\": 1.0})\n",
    "        rows.append({\"ds\": date(y, 12, 25), \"name\": \"Christmas\", \"type\": \"holiday\", \"weight\": 1.0})\n",
    "        rows.append({\"ds\": date(y, 12, 26), \"name\": \"BoxingDay\", \"type\": \"holiday\", \"weight\": 1.0})\n",
    "        # Commercial events\n",
    "        bf = _last_friday_of_november(y)\n",
    "        rows.append({\"ds\": bf, \"name\": \"BlackFriday\", \"type\": \"event\", \"weight\": 0.6})\n",
    "        rows.append({\"ds\": bf + timedelta(days=4), \"name\": \"CyberMonday\", \"type\": \"event\", \"weight\": 0.6})\n",
    "\n",
    "    exo = pd.DataFrame(rows)\n",
    "    exo['ds'] = pd.to_datetime(exo['ds'])\n",
    "    exo = exo[(exo['ds'] >= pd.to_datetime(start_date.date())) & (exo['ds'] <= pd.to_datetime(end_date.date()))]\n",
    "    return exo.reset_index(drop=True)\n",
    "\n",
    "def load_exogenous_from_files(files: Dict[str, str],\n",
    "                              start_date: pd.Timestamp,\n",
    "                              end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for key in [\"holidays\", \"events\"]:\n",
    "        path = files.get(key, None)\n",
    "        if path and os.path.exists(path):\n",
    "            df = pd.read_csv(path) if path.lower().endswith('.csv') else pd.read_excel(path)\n",
    "            # expected columns: ds, name, weight (weight optional -> default 1.0)\n",
    "            if 'ds' not in df.columns:\n",
    "                continue\n",
    "            df['ds'] = pd.to_datetime(df['ds'])\n",
    "            if 'name' not in df.columns:\n",
    "                df['name'] = key\n",
    "            if 'weight' not in df.columns:\n",
    "                df['weight'] = 1.0\n",
    "            df['type'] = 'holiday' if key == 'holidays' else 'event'\n",
    "            df = df[(df['ds'] >= start_date) & (df['ds'] <= end_date)]\n",
    "            frames.append(df[['ds', 'name', 'type', 'weight']])\n",
    "    if frames:\n",
    "        return pd.concat(frames, ignore_index=True).drop_duplicates()\n",
    "    return pd.DataFrame(columns=['ds', 'name', 'type', 'weight'])\n",
    "\n",
    "def build_exogenous_calendar(incoming: pd.DataFrame,\n",
    "                             horizon_days: int = DAILY_HORIZON_DAYS\n",
    "                             ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - exo_daily: [ds, is_holiday, is_event, weight_h, weight_e]\n",
    "    - exo_monthly: [month, hol_count, evt_count, hol_weight_sum, evt_weight_sum]\n",
    "    \"\"\"\n",
    "    start = incoming['Date'].min() - pd.Timedelta(days=365)\n",
    "    end = incoming['Date'].max() + pd.Timedelta(days=horizon_days)\n",
    "\n",
    "    if not EXOG_ENABLE:\n",
    "        exo_daily = pd.DataFrame({'ds': pd.date_range(start, end, freq='D')})\n",
    "        exo_daily['is_holiday'] = 0\n",
    "        exo_daily['is_event'] = 0\n",
    "        exo_daily['weight_h'] = 0.0\n",
    "        exo_daily['weight_e'] = 0.0\n",
    "    else:\n",
    "        exo = (load_exogenous_from_files(EXOG_FILES, start, end)\n",
    "               if EXOG_SOURCE == \"files\"\n",
    "               else build_eu_core_holidays(start, end, EXOG_COUNTRIES))\n",
    "        cal = pd.DataFrame({'ds': pd.date_range(start, end, freq='D')})\n",
    "        exo_daily = cal.merge(exo, on='ds', how='left')\n",
    "        exo_daily['is_holiday'] = (exo_daily['type'] == 'holiday').astype(int)\n",
    "        exo_daily['is_event'] = (exo_daily['type'] == 'event').astype(int)\n",
    "        exo_daily['weight_h'] = np.where(exo_daily['is_holiday'] == 1, exo_daily['weight'].fillna(1.0), 0.0)\n",
    "        exo_daily['weight_e'] = np.where(exo_daily['is_event'] == 1, exo_daily['weight'].fillna(1.0), 0.0)\n",
    "        exo_daily = exo_daily[['ds', 'is_holiday', 'is_event', 'weight_h', 'weight_e']]\n",
    "\n",
    "    exo_daily['month'] = exo_daily['ds'].dt.to_period('M')\n",
    "    exo_monthly = (\n",
    "        exo_daily.groupby('month', as_index=False)\n",
    "        .agg(hol_count=('is_holiday', 'sum'),\n",
    "             evt_count=('is_event', 'sum'),\n",
    "             hol_weight_sum=('weight_h', 'sum'),\n",
    "             evt_weight_sum=('weight_e', 'sum'))\n",
    "    )\n",
    "    return exo_daily, exo_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac39e7",
   "metadata": {},
   "source": [
    "## 4. Loaders and mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Loaders & mapping ====================\n",
    "\n",
    "def load_incoming(path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load daily incoming volumes from Excel/CSV.\n",
    "    Expected columns (min): Date, department_id, ticket_total (or derivable).\n",
    "    Coalesces language according to LANGUAGE_STRATEGY.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        msg = (\n",
    "            \"Incoming file not found:\\n\"\n",
    "            f\"{path}\\n\"\n",
    "            \"Please update INCOMING_SOURCE_PATH to the correct location.\"\n",
    "        )\n",
    "        raise FileNotFoundError(msg)\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xlsm\", \".xls\"]:\n",
    "        if not sheet_name:\n",
    "            raise ValueError(\"Excel file detected but no sheet_name provided (e.g., 'Main').\")\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension for incoming data: {ext}\")\n",
    "\n",
    "    # Required columns\n",
    "    required = {\"Date\", \"department_id\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        msg = (\n",
    "            \"Incoming must contain columns: \"\n",
    "            f\"{sorted(list(required))}. \"\n",
    "            f\"Found: {list(df.columns)}. \"\n",
    "            f\"Missing: {sorted(list(missing))}\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # ticket_total creation if missing\n",
    "    if \"ticket_total\" not in df.columns:\n",
    "        if \"total_incoming\" in df.columns:\n",
    "            df[\"ticket_total\"] = pd.to_numeric(df[\"total_incoming\"], errors=\"coerce\").fillna(0)\n",
    "        elif {\"incoming_from_customers\", \"incoming_from_transfers\"}.issubset(df.columns):\n",
    "            df[\"ticket_total\"] = (\n",
    "                pd.to_numeric(df[\"incoming_from_customers\"], errors=\"coerce\").fillna(0)\n",
    "                + pd.to_numeric(df[\"incoming_from_transfers\"], errors=\"coerce\").fillna(0)\n",
    "            )\n",
    "        else:\n",
    "            msg = (\n",
    "                \"Incoming must contain 'ticket_total' or alternatives \"\n",
    "                \"('total_incoming' or both 'incoming_from_customers' and 'incoming_from_transfers'). \"\n",
    "                f\"Found columns: {list(df.columns)}\"\n",
    "            )\n",
    "            raise ValueError(msg)\n",
    "\n",
    "    # Dtypes & clean\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    if df[\"Date\"].isna().any():\n",
    "        bad = df.loc[df[\"Date\"].isna()].head(5)\n",
    "        raise ValueError(f\"Some Date values could not be parsed. Example rows:\\n{bad}\")\n",
    "\n",
    "    df[\"department_id\"] = df[\"department_id\"].astype(str).str.strip()\n",
    "    df[\"ticket_total\"] = pd.to_numeric(df[\"ticket_total\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "\n",
    "    # Optional columns that downstream expects\n",
    "    if \"department_name\" not in df.columns:\n",
    "        df[\"department_name\"] = None\n",
    "    if \"vertical\" not in df.columns:\n",
    "        df[\"vertical\"] = None\n",
    "    if \"language\" not in df.columns:\n",
    "        df[\"language\"] = None\n",
    "\n",
    "    # Language handling\n",
    "    df = coalesce_language(df)\n",
    "    return df\n",
    "\n",
    "def load_dept_map(path: str, sheet: Optional[str] = None) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['department_id', 'department_name', 'vertical'])\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xls\"):\n",
    "        mp = pd.read_excel(path, sheet_name=sheet if sheet else 0, engine=\"openpyxl\")\n",
    "    else:\n",
    "        mp = pd.read_csv(path)\n",
    "\n",
    "    rename_map = {\n",
    "        'dept_id': 'department_id', 'dept_name': 'department_name', 'name': 'department_name',\n",
    "        'segment': 'vertical', 'vertical_name': 'vertical'\n",
    "    }\n",
    "    mp = mp.rename(columns={k: v for k, v in rename_map.items() if k in mp.columns})\n",
    "    if 'department_id' not in mp.columns:\n",
    "        raise ValueError(f\"Department map must contain 'department_id'. Found: {list(mp.columns)}\")\n",
    "\n",
    "    mp['department_id'] = mp['department_id'].astype(str).str.strip()\n",
    "    if 'department_name' in mp.columns:\n",
    "        mp['department_name'] = mp['department_name'].astype(str).str.strip()\n",
    "    if 'vertical' in mp.columns:\n",
    "        mp['vertical'] = mp['vertical'].astype(str).str.strip()\n",
    "\n",
    "    return mp[['department_id', 'department_name', 'vertical']].drop_duplicates('department_id')\n",
    "\n",
    "def apply_mapping(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    merged = incoming.merge(mapping, on='department_id', how='left', suffixes=('', '_map'))\n",
    "\n",
    "    if 'department_name' not in merged.columns:\n",
    "        merged['department_name'] = None\n",
    "    if 'department_name_map' not in merged.columns:\n",
    "        merged['department_name_map'] = None\n",
    "    merged['department_name'] = merged['department_name'].fillna(merged['department_name_map']).fillna(\"Unknown\")\n",
    "\n",
    "    if 'vertical' not in merged.columns:\n",
    "        merged['vertical'] = None\n",
    "    if 'vertical_map' not in merged.columns:\n",
    "        merged['vertical_map'] = None\n",
    "    merged['vertical'] = merged['vertical'].fillna(merged['vertical_map']).fillna(\"Unmapped\")\n",
    "\n",
    "    drop_cols = [c for c in merged.columns if c.endswith('_map')]\n",
    "    merged.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    return merged\n",
    "\n",
    "def load_productivity(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Productivity file not found: {path}\")\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    req = {'Date', 'agent_id', 'department_id', 'prod_total_model'}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"productivity_agents.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['prod_total_model'] = pd.to_numeric(df['prod_total_model'], errors='coerce')\n",
    "\n",
    "    prod_dept = (\n",
    "        df.groupby('department_id', as_index=False)['prod_total_model']\n",
    "        .mean().rename(columns={'prod_total_model': 'avg_tickets_per_agent_day'})\n",
    "    )\n",
    "    return prod_dept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b6b59",
   "metadata": {},
   "source": [
    "## 5. Aggregations (montly/weekly) and profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Aggregations ====================\n",
    "\n",
    "def build_monthly_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d['month'] = d['Date'].dt.to_period('M')\n",
    "    monthly = (\n",
    "        d.groupby(['department_id', 'language', 'month'], as_index=False)['ticket_total']\n",
    "        .sum().rename(columns={'ticket_total': 'incoming_monthly'})\n",
    "    )\n",
    "    return monthly\n",
    "\n",
    "def build_weekly_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy().set_index('Date').sort_index()\n",
    "    rows = []\n",
    "    for (dept, lang), g in d.groupby([d['department_id'], d['language']]):\n",
    "        s = g['ticket_total'].resample(WEEKLY_FREQ).sum().dropna()\n",
    "        if not s.empty:\n",
    "            tmp = pd.DataFrame({\n",
    "                'department_id': dept,\n",
    "                'language': lang,\n",
    "                'week': s.index.to_period('W-WED'),\n",
    "                'incoming_weekly': s.values\n",
    "            })\n",
    "            rows.append(tmp)\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "        columns=['department_id', 'language', 'week', 'incoming_weekly']\n",
    "    )\n",
    "\n",
    "def dow_profile(g: pd.DataFrame) -> pd.Series:\n",
    "    prof = (g.assign(dow=g['Date'].dt.dayofweek)\n",
    "              .groupby('dow')['ticket_total'].mean())\n",
    "    if prof.notna().sum() >= 3:\n",
    "        return prof / prof.mean()\n",
    "    return pd.Series(1.0, index=range(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982163e",
   "metadata": {},
   "source": [
    "## 6. Monthly modeling (Prophet/ARIMA/TBATS-ETS) + blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eca100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Modeling (monthly) ====================\n",
    "\n",
    "def winsorize_monthly(ts_m: pd.Series, lower_q: float = IQR_LO, upper_q: float = IQR_HI) -> pd.Series:\n",
    "    if ts_m.empty:\n",
    "        return ts_m\n",
    "    lo = ts_m.quantile(lower_q)\n",
    "    hi = ts_m.quantile(upper_q)\n",
    "    return ts_m.clip(lower=lo, upper=hi)\n",
    "\n",
    "def prepare_monthly_exog(exo_monthly: pd.DataFrame, ts_index: pd.PeriodIndex) -> pd.DataFrame:\n",
    "    ex = exo_monthly.copy()\n",
    "    if not pd.api.types.is_period_dtype(ex['month']):\n",
    "        ex['month'] = pd.PeriodIndex(ex['month'], freq='M')\n",
    "    ex = ex.set_index('month').reindex(ts_index).fillna(0.0)\n",
    "    return ex[['hol_count', 'evt_count', 'hol_weight_sum', 'evt_weight_sum']]\n",
    "\n",
    "def fit_prophet_monthly_log(ts_m: pd.Series, exo_m: Optional[pd.DataFrame] = None):\n",
    "    if Prophet is None or len(ts_m) < 6:\n",
    "        return None, None\n",
    "    y = np.log1p(ts_m.values)\n",
    "    dfp = pd.DataFrame({'ds': ts_m.index.to_timestamp(), 'y': y})\n",
    "    if exo_m is not None:\n",
    "        dfp = dfp.join(exo_m, on=ts_m.index).reset_index(drop=True)\n",
    "\n",
    "    m = Prophet(weekly_seasonality=False, yearly_seasonality=True, daily_seasonality=False)\n",
    "    if exo_m is not None:\n",
    "        for col in exo_m.columns:\n",
    "            m.add_regressor(col)\n",
    "    m.fit(dfp)\n",
    "\n",
    "    def fcast(h_months=H_MONTHS):\n",
    "        future = m.make_future_dataframe(periods=h_months, freq='MS')\n",
    "        if exo_m is not None:\n",
    "            # forward-fill exo for horizon with last row\n",
    "            idx_future = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            pad = exo_m.iloc[[-1]].repeat(h_months)\n",
    "            pad.index = idx_future\n",
    "            ex_full = pd.concat([exo_m, pad], axis=0)\n",
    "            for col in exo_m.columns:\n",
    "                future[col] = ex_full[col].values\n",
    "\n",
    "        pred_df = m.predict(future)\n",
    "        pred_df.index = pd.PeriodIndex(pred_df['ds'], freq='M')\n",
    "        pred = pred_df['yhat'].iloc[-h_months:]\n",
    "        cap = compute_dynamic_cap(ts_m)\n",
    "        vals = expm1_safe(pred.values, cap_original=cap)\n",
    "        return pd.Series(vals, index=pred.index)\n",
    "\n",
    "    return m, fcast\n",
    "\n",
    "def fit_arima_monthly_log(ts_m: pd.Series, exo_m: Optional[pd.DataFrame] = None):\n",
    "    y = np.log1p(ts_m)\n",
    "    best_aic, best_model, best_exog = np.inf, None, None\n",
    "    pqs = [0, 1]\n",
    "    seasonal = len(ts_m) >= 12\n",
    "    PsQs = [0] if seasonal else [0]\n",
    "    for p in pqs:\n",
    "        for d in ([1] if len(ts_m) < 24 else [0, 1]):\n",
    "            for q in pqs:\n",
    "                for P in PsQs:\n",
    "                    for D in ([0, 1] if seasonal else [0]):\n",
    "                        for Q in PsQs:\n",
    "                            try:\n",
    "                                model = SARIMAX(\n",
    "                                    y, order=(p, d, q),\n",
    "                                    seasonal_order=(P, D, Q, 12 if seasonal else 0),\n",
    "                                    exog=exo_m.values if exo_m is not None else None,\n",
    "                                    enforce_stationarity=False, enforce_invertibility=False\n",
    "                                ).fit(disp=False)\n",
    "                                if model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_model = model\n",
    "                                    best_exog = exo_m\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "    def fcast(h_months=H_MONTHS):\n",
    "        if best_model is None:\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            return pd.Series([float(np.exp(y).mean())] * h_months, index=idx)\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        exog_future = None\n",
    "        if best_exog is not None:\n",
    "            pad = best_exog.iloc[[-1]].repeat(h_months)\n",
    "            pad.index = idx\n",
    "            exog_future = pad.values\n",
    "        fc_log = best_model.get_forecast(h_months, exog=exog_future).predicted_mean\n",
    "        cap = compute_dynamic_cap(ts_m)\n",
    "        fc = expm1_safe(fc_log, cap_original=cap)\n",
    "        return pd.Series(fc, index=idx)\n",
    "\n",
    "    return best_model, fcast\n",
    "\n",
    "def fit_tbats_or_ets_monthly_log(ts_m: pd.Series):\n",
    "    y_log = np.log1p(ts_m)\n",
    "\n",
    "    if TBATS is not None and len(ts_m) >= 12:\n",
    "        y_log_ts = pd.Series(y_log.values, index=ts_m.index.to_timestamp())\n",
    "        estimator = TBATS(use_arma_errors=False, seasonal_periods=[12])\n",
    "        model = estimator.fit(y_log_ts)\n",
    "\n",
    "        def fcast(h_months=H_MONTHS):\n",
    "            vals_log = model.forecast(steps=h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            cap = compute_dynamic_cap(ts_m)\n",
    "            vals = expm1_safe(vals_log, cap_original=cap)\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "    seasonal = 12 if len(ts_m) >= 24 else None\n",
    "    model = ExponentialSmoothing(y_log, trend='add',\n",
    "                                 seasonal=('add' if seasonal else None),\n",
    "                                 seasonal_periods=seasonal).fit()\n",
    "\n",
    "    def fcast(h_months=H_MONTHS):\n",
    "        vals_log = model.forecast(h_months)\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        cap = compute_dynamic_cap(ts_m)\n",
    "        vals = expm1_safe(vals_log, cap_original=cap)\n",
    "        return pd.Series(vals, index=idx)\n",
    "\n",
    "    return model, fcast\n",
    "\n",
    "def rolling_cv_monthly_adaptive(ts_m: pd.Series,\n",
    "                                exo_monthly: Optional[pd.DataFrame] = None\n",
    "                                ) -> Optional[Dict[str, float]]:\n",
    "    \"\"\"Small rolling CV for sMAPE across candidates; short series are skipped.\"\"\"\n",
    "    n = len(ts_m)\n",
    "    if n < 9:\n",
    "        return None\n",
    "    h = 3 if n >= 15 else 1\n",
    "    min_train = max(12, n - (h + 2))\n",
    "    splits = []\n",
    "    for start in range(min_train, n - h + 1):\n",
    "        train = ts_m.iloc[:start]\n",
    "        test = ts_m.iloc[start:start + h]\n",
    "        metrics = {}\n",
    "        ex_train = ex_test = None\n",
    "        if exo_monthly is not None:\n",
    "            ex_train = exo_monthly.iloc[:start]\n",
    "            ex_test = exo_monthly.iloc[start:start + h]\n",
    "\n",
    "        mp, fp = fit_prophet_monthly_log(train, ex_train)\n",
    "        if fp is not None:\n",
    "            try:\n",
    "                pv = np.array(fp(h_months=h).values[:h], dtype=float)\n",
    "                pv[~np.isfinite(pv)] = np.nan\n",
    "                metrics['Prophet'] = 200.0 if np.isnan(pv).all() else smape(test.values, np.nan_to_num(pv, nan=0.0))\n",
    "            except Exception:\n",
    "                metrics['Prophet'] = 200.0\n",
    "\n",
    "        try:\n",
    "            ma, fa = fit_arima_monthly_log(train, ex_train)\n",
    "            pv = np.array(fa(h_months=h).values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else smape(test.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            metrics['ARIMA'] = 200.0\n",
    "\n",
    "        try:\n",
    "            mt, ft = fit_tbats_or_ets_monthly_log(train)\n",
    "            pv = np.array(ft(h_months=h).values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else smape(test.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            metrics['TBATS/ETS'] = 200.0\n",
    "\n",
    "        splits.append(metrics)\n",
    "    dfm = pd.DataFrame(splits)\n",
    "    return dfm.mean().to_dict()\n",
    "\n",
    "def select_or_blend_forecasts(fc_dict: Dict[str, pd.Series],\n",
    "                              cv_scores: Dict[str, float],\n",
    "                              blend: bool = True):\n",
    "    \"\"\"\n",
    "    Inverse-error weighted blending; robust to empty or NaN CV scores.\n",
    "    Fallbacks:\n",
    "      - If no CV scores at all -> uniform averaging across all candidates (if blend=True),\n",
    "        else pick the first available model deterministically.\n",
    "      - If all inverted weights sum to 0 -> fallback to winner by min score when available,\n",
    "        otherwise uniform average.\n",
    "    \"\"\"\n",
    "    # Safety: if no forecast candidates, raise early (should not happen upstream)\n",
    "    if not fc_dict:\n",
    "        raise ValueError(\"select_or_blend_forecasts: no forecast candidates provided.\")\n",
    "\n",
    "    # Filter/normalize scores\n",
    "    scores = {k: float(v) for k, v in (cv_scores or {}).items()\n",
    "              if v is not None and np.isfinite(v)}\n",
    "\n",
    "    # If blending is disabled, pick deterministically the first model\n",
    "    if not blend:\n",
    "        winner = next(iter(fc_dict.keys()))\n",
    "        return fc_dict[winner], {'winner': winner, 'weights': {winner: 1.0}}\n",
    "\n",
    "    # Case A: No usable CV scores -> uniform average across all models\n",
    "    if len(scores) == 0:\n",
    "        # Uniform weights across available forecasts\n",
    "        keys = list(fc_dict.keys())\n",
    "        idx = None\n",
    "        for s in fc_dict.values():\n",
    "            idx = s.index if idx is None else idx.union(s.index)\n",
    "        w = {k: 1.0 / len(keys) for k in keys}\n",
    "        blended = sum(w[k] * fc_dict[k].reindex(idx).fillna(0) for k in keys)\n",
    "        # Winner is arbitrary: choose the first key for traceability\n",
    "        winner = keys[0]\n",
    "        return blended, {'winner': winner, 'weights': w}\n",
    "\n",
    "    # Case B: We have scores -> inverse-error weighting\n",
    "    inv = {k: (1.0 / v if v > 0 else 0.0) for k, v in scores.items()}\n",
    "    total = sum(inv.values())\n",
    "\n",
    "    # If all weights collapsed to zero (e.g., all scores are zero/identical in a corner case)\n",
    "    if total == 0:\n",
    "        # Try pick the smallest score as winner if possible\n",
    "        try:\n",
    "            winner = min(scores, key=scores.get)\n",
    "            return fc_dict[winner], {'winner': winner, 'weights': {winner: 1.0}}\n",
    "        except ValueError:\n",
    "            # Fallback again to uniform average across *all* candidates\n",
    "            keys = list(fc_dict.keys())\n",
    "            idx = None\n",
    "            for s in fc_dict.values():\n",
    "                idx = s.index if idx is None else idx.union(s.index)\n",
    "            w = {k: 1.0 / len(keys) for k in keys}\n",
    "            blended = sum(w[k] * fc_dict[k].reindex(idx).fillna(0) for k in keys)\n",
    "            winner = keys[0]\n",
    "            return blended, {'winner': winner, 'weights': w}\n",
    "\n",
    "    # Normal path: compute normalized weights over the models for which we have scores\n",
    "    w = {k: inv[k] / total for k in inv}\n",
    "\n",
    "    # Build union index across all candidate forecasts to avoid NaNs\n",
    "    idx = None\n",
    "    for s in fc_dict.values():\n",
    "        idx = s.index if idx is None else idx.union(s.index)\n",
    "\n",
    "    # Note: if some model is missing in scores (no CV), give it tiny weight (0) implicitly\n",
    "    blended = sum(w.get(k, 0.0) * fc_dict[k].reindex(idx).fillna(0) for k in fc_dict)\n",
    "\n",
    "    # Winner: the one with minimum score among those with scores\n",
    "    winner = min(scores, key=scores.get)\n",
    "    return blended, {'winner': winner, 'weights': w}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5792959",
   "metadata": {},
   "source": [
    "## 7. Week candidate (noisiest series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Weekly candidate (ETS weekly) ====================\n",
    "\n",
    "def forecast_weekly_candidate(g_daily: pd.DataFrame, horizon_months=H_MONTHS) -> Optional[pd.Series]:\n",
    "    if len(g_daily) < 60:\n",
    "        return None\n",
    "\n",
    "    w = (g_daily.set_index('Date').sort_index()['ticket_total']\n",
    "         .resample(WEEKLY_FREQ).sum().dropna())\n",
    "    if len(w) < 30:\n",
    "        return None\n",
    "\n",
    "    y_log = np.log1p(w)\n",
    "    try:\n",
    "        model = ExponentialSmoothing(y_log, trend='add', seasonal=None).fit()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    last_day = g_daily['Date'].max()\n",
    "    end_date = (last_day + pd.offsets.MonthEnd(horizon_months)).to_pydatetime()\n",
    "    steps = int(np.ceil((pd.Timestamp(end_date) - w.index[-1].to_timestamp()).days / 7)) + 4\n",
    "    vals_log = model.forecast(steps=max(steps, 12))\n",
    "    vals = np.expm1(vals_log)\n",
    "    wf = pd.Series(vals, index=pd.period_range(vals.index[0], periods=len(vals), freq='W-WED'))\n",
    "\n",
    "    prof = dow_profile(g_daily)\n",
    "    future_days = pd.date_range(start=last_day + pd.Timedelta(days=1), end=end_date, freq='D')\n",
    "    df_daily = pd.DataFrame({'Date': future_days})\n",
    "    df_daily['week'] = df_daily['Date'].to_period('W-WED')\n",
    "\n",
    "    # Map weekly forecast to daily with DOW profile weights\n",
    "    wk_map = wf.to_timestamp().rename('week_total')\n",
    "    df_daily = df_daily.merge(\n",
    "        wk_map.reset_index().rename(columns={'index': 'Date'}),\n",
    "        left_on='week', right_on=pd.PeriodIndex(wf.index, freq='W-WED').to_timestamp(), how='left'\n",
    "    )\n",
    "    df_daily['dow'] = df_daily['Date'].dt.dayofweek\n",
    "    df_daily['w_sum'] = df_daily.groupby('week')['dow'].transform(lambda x: (prof.reindex(x).fillna(1.0)).sum())\n",
    "    df_daily['w_w'] = np.where(df_daily['w_sum'] > 0,\n",
    "                               (prof.reindex(df_daily['dow']).fillna(1.0)).values / df_daily['w_sum'],\n",
    "                               1.0)\n",
    "    df_daily['forecast_daily'] = df_daily['week_total'] * df_daily['w_w']\n",
    "    df_daily['month'] = df_daily['Date'].dt.to_period('M')\n",
    "    monthly_candidate = df_daily.groupby('month')['forecast_daily'].sum()\n",
    "    monthly_candidate = monthly_candidate.iloc[:H_MONTHS]\n",
    "    monthly_candidate.index = pd.PeriodIndex(monthly_candidate.index, freq='M')\n",
    "    return monthly_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45095f21",
   "metadata": {},
   "source": [
    "## 8. Daily pannel, features and Global LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Global LightGBM per vertical (with daily exogenous) ====================\n",
    "\n",
    "def build_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d['dayofweek'] = d['Date'].dt.dayofweek\n",
    "    d['weekofyear'] = d['Date'].dt.isocalendar().week.astype(int)\n",
    "    d['month'] = d['Date'].dt.month\n",
    "    d['year'] = d['Date'].dt.year\n",
    "    return d\n",
    "\n",
    "def add_lags_and_rolls(d: pd.DataFrame, group_cols: List[str], target_col='y') -> pd.DataFrame:\n",
    "    d = d.sort_values(['Date']).copy()\n",
    "    for lag in LGB_LAGS:\n",
    "        d[f'lag_{lag}'] = d.groupby(group_cols)[target_col].shift(lag)\n",
    "    for w in LGB_ROLLS:\n",
    "        d[f'roll_mean_{w}'] = d.groupby(group_cols)[target_col].shift(1).rolling(w).mean()\n",
    "        d[f'roll_std_{w}']  = d.groupby(group_cols)[target_col].shift(1).rolling(w).std()\n",
    "    return d\n",
    "\n",
    "def prepare_daily_panel(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = incoming.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['ticket_total'] = pd.to_numeric(df['ticket_total'], errors='coerce').fillna(0.0)\n",
    "\n",
    "    df = apply_mapping(df, mapping)\n",
    "    has_lang = ('language' in df.columns) and (df['language'].notna().any())\n",
    "    if LANGUAGE_STRATEGY == 'from_column' and has_lang:\n",
    "        df['language'] = df['language'].fillna('Unknown').astype(str).str.strip()\n",
    "        panel = (df.groupby(['Date', 'department_id', 'language', 'vertical'], as_index=False)['ticket_total']\n",
    "                   .sum().rename(columns={'ticket_total': 'y'}))\n",
    "    else:\n",
    "        base = df.groupby(['Date', 'department_id', 'vertical'], as_index=False)['ticket_total'].sum()\n",
    "        parts = []\n",
    "        for lang, w in LANGUAGE_SHARES.items():\n",
    "            tmp = base.copy(); tmp['language'] = lang; tmp['y'] = tmp['ticket_total'] * float(w)\n",
    "            parts.append(tmp)\n",
    "        panel = pd.concat(parts, ignore_index=True)\n",
    "        panel = panel[['Date', 'department_id', 'language', 'vertical', 'y']]\n",
    "\n",
    "    panel['y'] = panel['y'].clip(lower=0.0)\n",
    "    return panel\n",
    "\n",
    "def merge_exogenous_daily(panel: pd.DataFrame, exo_daily: pd.DataFrame) -> pd.DataFrame:\n",
    "    exo_d = exo_daily.rename(columns={'ds': 'Date'}).copy()\n",
    "    d = panel.merge(exo_d, on='Date', how='left')\n",
    "    d[['is_holiday', 'is_event', 'weight_h', 'weight_e']] = d[['is_holiday', 'is_event', 'weight_h', 'weight_e']].fillna(0.0)\n",
    "    return d\n",
    "\n",
    "def train_global_lgb_per_vertical(incoming_clean: pd.DataFrame, mapping: pd.DataFrame,\n",
    "                                  exo_daily: pd.DataFrame, h_days=DAILY_HORIZON_DAYS\n",
    "                                  ) -> Dict[str, pd.DataFrame]:\n",
    "    if not ENABLE_GLOBAL_LGB or lgb is None:\n",
    "        return {}\n",
    "\n",
    "    res = {}\n",
    "    last_date = incoming_clean['Date'].max()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    future_idx = pd.date_range(start=start, periods=h_days, freq='D')\n",
    "\n",
    "    data = apply_mapping(incoming_clean, mapping)\n",
    "    data = data[['Date', 'department_id', 'language', 'vertical', 'ticket_total']].copy()\n",
    "\n",
    "    # Build daily panel + exogenous\n",
    "    panel = prepare_daily_panel(data.rename(columns={'ticket_total': 'ticket_total'}), mapping)\n",
    "    panel_exo = merge_exogenous_daily(panel, exo_daily)\n",
    "\n",
    "    # Prepare exogenous lookup (vectorized)\n",
    "    exo_lookup = (exo_daily.set_index('ds')[['is_holiday', 'is_event', 'weight_h', 'weight_e']]\n",
    "                  .reindex(future_idx, fill_value=0.0))\n",
    "\n",
    "    for vert, g in panel_exo.groupby('vertical'):\n",
    "        if len(g) < 60:\n",
    "            continue\n",
    "\n",
    "        g = build_time_features(g.copy())\n",
    "        g = add_lags_and_rolls(g, ['department_id', 'language'], target_col='y')\n",
    "\n",
    "        feat_cols = ['dayofweek', 'weekofyear', 'month', 'year', 'department_id', 'language',\n",
    "                     'is_holiday', 'is_event', 'weight_h', 'weight_e']\n",
    "        feat_cols += [c for c in g.columns if c.startswith('lag_') or c.startswith('roll_')]\n",
    "\n",
    "        g_train = g.dropna(subset=feat_cols + ['y']).copy()\n",
    "        if len(g_train) < 200:\n",
    "            continue\n",
    "\n",
    "        # Categorical features\n",
    "        cat_cols = ['department_id', 'language', 'dayofweek', 'month']\n",
    "        X = g_train[feat_cols].copy()\n",
    "        y = g_train['y'].copy()\n",
    "        for c in cat_cols:\n",
    "            if c in X.columns:\n",
    "                X[c] = X[c].astype('category')\n",
    "\n",
    "        # 3-fold TimeSeriesSplit CV\n",
    "        tscv = TimeSeriesSplit(n_splits=2)\n",
    "        cv_smape = []\n",
    "        for tr, va in tscv.split(X):\n",
    "            model = lgb.LGBMRegressor(\n",
    "                n_estimators=LGB_N_ESTIMATORS,\n",
    "                learning_rate=LGB_LEARNING_RATE,\n",
    "                num_leaves=63,\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "            )\n",
    "            model.fit(X.iloc[tr], y.iloc[tr], categorical_feature=[c for c in cat_cols if c in X.columns])\n",
    "            pred = model.predict(X.iloc[va])\n",
    "            cv_smape.append(smape(y.iloc[va].values, np.maximum(pred, 0.0)))\n",
    "        lgb_cv_smape = float(np.mean(cv_smape)) if cv_smape else np.nan\n",
    "\n",
    "        # Train final\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=LGB_N_ESTIMATORS,\n",
    "            learning_rate=LGB_LEARNING_RATE,\n",
    "            num_leaves=63,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs = -1,\n",
    "        )\n",
    "       \n",
    "        model.fit(\n",
    "            X.iloc[tr], y.iloc[tr],\n",
    "            eval_set=[(X.iloc[va], y.iloc[va])],\n",
    "            categorical_feature=[c for c in cat_cols if c in X.columns],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "        )\n",
    "        pred = model.predict(X.iloc[va], num_iteration=model.best_iteration_)\n",
    "\n",
    "\n",
    "        # Autoregressive forecasting per (dept, lang) with efficient updates\n",
    "        hist = (g[['Date', 'department_id', 'language', 'y']]\n",
    "                .pivot_table(index='Date', columns=['department_id', 'language'], values='y')\n",
    "                .asfreq('D').fillna(0.0))\n",
    "\n",
    "        # Pre-allocate a DataFrame to append future rows efficiently (assignment, not concat)\n",
    "        hist_future = pd.DataFrame(index=future_idx, columns=hist.columns, dtype=float)\n",
    "\n",
    "        for d in future_idx:\n",
    "            dayofweek = d.dayofweek\n",
    "            weekofyear = int(d.isocalendar().week)\n",
    "            month = d.month\n",
    "            year = d.year\n",
    "\n",
    "            # Exogenous row for this day\n",
    "            ex_row = exo_lookup.loc[d]\n",
    "            preds = {}\n",
    "            # Iterate each (dept, lang) series\n",
    "            for (dept, lang) in hist.columns:\n",
    "                series = (pd.concat([hist[(dept, lang)], hist_future[(dept, lang)]], axis=0)\n",
    "                          .dropna())  # combine realized + predicted so far\n",
    "\n",
    "                feats = {\n",
    "                    'dayofweek': dayofweek,\n",
    "                    'weekofyear': weekofyear,\n",
    "                    'month': month,\n",
    "                    'year': year,\n",
    "                    'department_id': dept,\n",
    "                    'language': lang,\n",
    "                    'is_holiday': float(ex_row['is_holiday']),\n",
    "                    'is_event': float(ex_row['is_event']),\n",
    "                    'weight_h': float(ex_row['weight_h']),\n",
    "                    'weight_e': float(ex_row['weight_e']),\n",
    "                }\n",
    "                # Lags\n",
    "                for lag in LGB_LAGS:\n",
    "                    if len(series) >= lag:\n",
    "                        feats[f'lag_{lag}'] = float(series.iloc[-lag])\n",
    "                    else:\n",
    "                        feats[f'lag_{lag}'] = float(series.mean()) if len(series) else 0.0\n",
    "                # Rolls\n",
    "                for w in LGB_ROLLS:\n",
    "                    if len(series) > 1:\n",
    "                        tail = series.iloc[:-1].tail(w)\n",
    "                        feats[f'roll_mean_{w}'] = float(tail.mean()) if len(tail) else float(series.mean())\n",
    "                        feats[f'roll_std_{w}'] = float(tail.std()) if len(tail) else 0.0\n",
    "                    else:\n",
    "                        feats[f'roll_mean_{w}'] = float(series.mean()) if len(series) else 0.0\n",
    "                        feats[f'roll_std_{w}'] = 0.0\n",
    "\n",
    "                fx = pd.DataFrame([feats])\n",
    "                for c in ['department_id', 'language']:\n",
    "                    fx[c] = fx[c].astype('category')\n",
    "\n",
    "                yhat = float(model.predict(fx)[0])\n",
    "                preds[(dept, lang)] = max(0.0, yhat)\n",
    "\n",
    "            # Assignment is faster than concat in a loop\n",
    "            hist_future.loc[d] = pd.Series(preds)\n",
    "\n",
    "        future = hist_future.stack().reset_index()\n",
    "        future.columns = ['Date', 'department_id', 'language', 'forecast_daily_lgb']\n",
    "        future['vertical'] = str(vert)\n",
    "        future['lgb_cv_smape'] = lgb_cv_smape\n",
    "        res[str(vert)] = future\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b6402",
   "metadata": {},
   "source": [
    "## 9. Monthly reconciliation  - daily and language split (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Top-down reconciliation (monthly->daily) ====================\n",
    "\n",
    "def disaggregate_month_to_days(group_hist: pd.DataFrame, month_period: pd.Period, target_sum: float) -> pd.DataFrame:\n",
    "    start = month_period.start_time\n",
    "    end = month_period.end_time\n",
    "    days = pd.date_range(start=start, end=end, freq='D')\n",
    "    hist = group_hist.sort_values('Date').tail(90)\n",
    "    profile = dow_profile(hist)\n",
    "    weights = np.array([profile.get(d.dayofweek, 1.0) for d in days], dtype=float)\n",
    "    weights = np.maximum(weights, 1e-6)\n",
    "    weights = weights / weights.sum()\n",
    "    alloc = target_sum * weights\n",
    "    return pd.DataFrame({'Date': days, 'forecast_daily': alloc})\n",
    "\n",
    "def build_daily_from_monthly(incoming: pd.DataFrame, fc_monthly: pd.DataFrame, horizon_days: int) -> pd.DataFrame:\n",
    "    last_date = incoming['Date'].max()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    end = start + pd.Timedelta(days=horizon_days - 1)\n",
    "    future_months = pd.period_range(start=start.to_period('M'), end=end.to_period('M'), freq='M')\n",
    "\n",
    "    rows = []\n",
    "    for (dept, lang), g in incoming.groupby(['department_id', 'language']):\n",
    "        for m in future_months:\n",
    "            fcm = fc_monthly[\n",
    "                (fc_monthly['department_id'] == dept) &\n",
    "                (fc_monthly['language'] == lang) &\n",
    "                (fc_monthly['month'] == m)\n",
    "            ]\n",
    "            if fcm.empty:\n",
    "                continue\n",
    "            target = float(fcm['forecast_monthly'].iloc[0])\n",
    "            if target <= 0:\n",
    "                continue\n",
    "            alloc_df = disaggregate_month_to_days(g[['Date', 'ticket_total']], m, target)\n",
    "            alloc_df = alloc_df[(alloc_df['Date'] >= start) & (alloc_df['Date'] <= end)]\n",
    "            alloc_df.insert(0, 'department_id', dept)\n",
    "            alloc_df.insert(1, 'language', lang)\n",
    "            rows.append(alloc_df)\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "        columns=['department_id', 'language', 'Date', 'forecast_daily']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# ==================== Language split (dynamic shares) ====================\n",
    "\n",
    "def build_language_shares_from_actuals(incoming: pd.DataFrame, window_days: int = 90) -> pd.DataFrame:\n",
    "    if 'language' not in incoming.columns:\n",
    "        return pd.DataFrame(columns=['department_id', 'language', 'share'])\n",
    "    df = incoming.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    cutoff = df['Date'].max() - pd.Timedelta(days=window_days)\n",
    "    df = df[df['Date'] >= cutoff]\n",
    "\n",
    "    g = (df.groupby(['department_id', 'language'], as_index=False)['ticket_total']\n",
    "           .sum().rename(columns={'ticket_total': 'sum_lang'}))\n",
    "    tot = (g.groupby('department_id', as_index=False)['sum_lang'].sum()\n",
    "             .rename(columns={'sum_lang': 'sum_dept'}))\n",
    "    m = g.merge(tot, on='department_id', how='left')\n",
    "    m['share'] = np.where(m['sum_dept'] > 0, m['sum_lang'] / m['sum_dept'], np.nan)\n",
    "    return m[['department_id', 'language', 'share']]\n",
    "\n",
    "def split_daily_by_language(daily_fc: pd.DataFrame) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    for lang, w in LANGUAGE_SHARES.items():\n",
    "        tmp = daily_fc.copy()\n",
    "        tmp['language'] = lang\n",
    "        tmp['forecast_daily_language'] = tmp['forecast_daily'] * float(w)\n",
    "        parts.append(tmp)\n",
    "    out = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    return out\n",
    "\n",
    "def split_daily_forecast_by_language_dynamic(daily_fc: pd.DataFrame, incoming: pd.DataFrame) -> pd.DataFrame:\n",
    "    if LANGUAGE_STRATEGY != 'from_column' or 'language' not in incoming.columns:\n",
    "        return split_daily_by_language(daily_fc)\n",
    "\n",
    "    shares = build_language_shares_from_actuals(incoming)\n",
    "    if shares.empty:\n",
    "        return split_daily_by_language(daily_fc)\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in daily_fc.groupby('department_id'):\n",
    "        s = shares[shares['department_id'] == dept].dropna(subset=['share']).copy()\n",
    "        if s.empty:\n",
    "            rows.append(split_daily_by_language(g))\n",
    "            continue\n",
    "        s['share'] = s['share'] / s['share'].sum()\n",
    "        for _, r in g.iterrows():\n",
    "            for _, row in s.iterrows():\n",
    "                rows.append({\n",
    "                    'Date': r['Date'],\n",
    "                    'department_id': dept,\n",
    "                    'language': row['language'],\n",
    "                    'forecast_daily_language': float(r['forecast_daily']) * float(row['share'])\n",
    "                })\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895eb89",
   "metadata": {},
   "source": [
    "## 10. Daily Blending (estatistic vs ML) and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Daily blending (stat vs ML) ====================\n",
    "\n",
    "def compute_series_noise(panel: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Noise proxy using sMAPE of seasonal-naive(7d) on last h days.\"\"\"\n",
    "    scores = []\n",
    "    for (dept, lang), g in panel.groupby(['department_id', 'language']):\n",
    "        s = g.sort_values('Date')['y']\n",
    "        if len(s) < 21:\n",
    "            sc = np.nan\n",
    "        else:\n",
    "            h = min(7, len(s) // 4) or 1\n",
    "            y_true = s.tail(h).values\n",
    "            y_pred = s.shift(7).tail(h).fillna(method='ffill').fillna(0.0).values\n",
    "            sc = smape(y_true, y_pred)\n",
    "        scores.append({'department_id': dept, 'language': lang, 'noise_smape': sc})\n",
    "    return pd.DataFrame(scores)\n",
    "\n",
    "def blend_daily_predictions(daily_stat: pd.DataFrame, ml_forecasts: pd.DataFrame, panel: pd.DataFrame) -> pd.DataFrame:\n",
    "    noise = compute_series_noise(panel)\n",
    "    ml = ml_forecasts.copy()\n",
    "    if not ml.empty:\n",
    "        ml['Date'] = pd.to_datetime(ml['Date'])\n",
    "        ml['department_id'] = ml['department_id'].astype(str)\n",
    "        ml['language'] = ml['language'].astype(str)\n",
    "\n",
    "    df = daily_stat.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['department_id'] = df['department_id'].astype(str)\n",
    "    df['language'] = df['language'].astype(str)\n",
    "\n",
    "    df = df.merge(ml, on=['Date', 'department_id', 'language'], how='left')\n",
    "    if 'forecast_daily_lgb' not in df.columns:\n",
    "        df['forecast_daily_lgb'] = np.nan\n",
    "    df['forecast_daily_lgb'] = df['forecast_daily_lgb'].fillna(df['forecast_daily_language'])\n",
    "\n",
    "    df = df.merge(noise, on=['department_id', 'language'], how='left')\n",
    "    ns = df['noise_smape'].fillna(50.0).clip(0, 200) / 100.0\n",
    "    w_ml = 0.3 + 0.5 * ns\n",
    "    w_stat = 1.0 - w_ml\n",
    "    df['forecast_daily_language_blend'] = w_ml * df['forecast_daily_lgb'] + w_stat * df['forecast_daily_language']\n",
    "    return df[['Date', 'department_id', 'language',\n",
    "               'forecast_daily_language', 'forecast_daily_lgb', 'forecast_daily_language_blend']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba748d4f",
   "metadata": {},
   "source": [
    "## 11. Monthly Forecast (dept, language) + CV/capacity_error tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651de99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Monthly per (dept, language) with exogenous ====================\n",
    "\n",
    "def forecast_per_dept_lang_monthly(incoming_clean: pd.DataFrame, exo_monthly: pd.DataFrame) -> pd.DataFrame:\n",
    "    out_rows = []\n",
    "\n",
    "    for (dept, lang), g_daily in incoming_clean.groupby(['department_id', 'language']):\n",
    "        g_daily = g_daily.sort_values('Date')\n",
    "        g_daily_clean = clean_outliers_daily(g_daily, method=OUTLIER_METHOD)\n",
    "\n",
    "        g_m = (g_daily_clean.assign(month=g_daily_clean['Date'].dt.to_period('M'))\n",
    "               .groupby('month')['ticket_total'].sum())\n",
    "\n",
    "        if not pd.api.types.is_period_dtype(g_m.index):\n",
    "            g_m.index = pd.PeriodIndex(g_m.index, freq='M')\n",
    "        if len(g_m) == 0:\n",
    "            continue\n",
    "\n",
    "        ts = winsorize_monthly(g_m, IQR_LO, IQR_HI)\n",
    "        ex_m = prepare_monthly_exog(exo_monthly, ts.index)\n",
    "\n",
    "        fc_dict, cv = {}, {}\n",
    "        mp, fp = fit_prophet_monthly_log(ts, ex_m)\n",
    "        if fp is not None:\n",
    "            try:\n",
    "                fc_dict['Prophet'] = fp(H_MONTHS)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            ma, fa = fit_arima_monthly_log(ts, ex_m)\n",
    "            fc_dict['ARIMA'] = fa(H_MONTHS)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            mt, ft = fit_tbats_or_ets_monthly_log(ts)\n",
    "            fc_dict['TBATS/ETS'] = ft(H_MONTHS)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not fc_dict:\n",
    "            idx = pd.period_range(ts.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "            val = max(0.0, float(ts.mean()))\n",
    "            fc_dict['NaiveMean'] = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        try:\n",
    "            cv = rolling_cv_monthly_adaptive(ts, ex_m) or {}\n",
    "        except Exception:\n",
    "            cv = {}\n",
    "\n",
    "        noisy = noise_score_daily(g_daily_clean) >= NOISE_SCORE_THRESH\n",
    "        if ENABLE_WEEKLY_CANDIDATE and noisy:\n",
    "            wk = forecast_weekly_candidate(g_daily_clean, horizon_months=H_MONTHS)\n",
    "            if wk is not None and len(wk) == H_MONTHS:\n",
    "                fc_dict['WEEKLY'] = wk\n",
    "                # If weekly candidate exists, use average of existing CVs as proxy\n",
    "                cv['WEEKLY'] = np.mean([v for v in cv.values() if np.isfinite(v)]) if cv else 80.0\n",
    "\n",
    "        blended, meta = select_or_blend_forecasts(fc_dict, cv_scores=cv, blend=True)\n",
    "        for per, val in blended.items():\n",
    "            out_rows.append({\n",
    "                'department_id': dept, 'language': lang, 'month': per,\n",
    "                'forecast_monthly': max(0.0, float(val)),\n",
    "                'cv_prophet_smape': cv.get('Prophet', np.nan),\n",
    "                'cv_arima_smape': cv.get('ARIMA', np.nan),\n",
    "                'cv_tbats_ets_smape': cv.get('TBATS/ETS', np.nan),\n",
    "                'cv_weekly_smape': cv.get('WEEKLY', np.nan),\n",
    "                'winner_model': meta['winner'],\n",
    "                'w_prophet': meta['weights'].get('Prophet', np.nan) if 'weights' in meta else np.nan,\n",
    "                'w_arima': meta['weights'].get('ARIMA', np.nan) if 'weights' in meta else np.nan,\n",
    "                'w_tbats_ets': meta['weights'].get('TBATS/ETS', np.nan) if 'weights' in meta else np.nan,\n",
    "                'w_weekly': meta['weights'].get('WEEKLY', np.nan) if 'weights' in meta else np.nan,\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows)\n",
    "    if not df_out.empty:\n",
    "        df_out['department_id'] = df_out['department_id'].astype(str)\n",
    "        if not pd.api.types.is_period_dtype(df_out['month']):\n",
    "            df_out['month'] = pd.PeriodIndex(df_out['month'], freq='M')\n",
    "    return df_out\n",
    "\n",
    "# ==================== CV table ====================\n",
    "\n",
    "def build_cv_table(fc_monthly: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    if fc_monthly is None or fc_monthly.empty:\n",
    "        raise ValueError(\"fc_monthly is empty; cannot build CV table.\")\n",
    "    cols_keep = [\n",
    "        'department_id', 'language',\n",
    "        'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape', 'cv_weekly_smape',\n",
    "        'winner_model', 'w_prophet', 'w_arima', 'w_tbats_ets', 'w_weekly'\n",
    "    ]\n",
    "    df = (fc_monthly[cols_keep]\n",
    "          .drop_duplicates(subset=['department_id', 'language']).copy())\n",
    "    df = df.rename(columns={\n",
    "        'cv_prophet_smape': 'sMAPE_Prophet_CV',\n",
    "        'cv_arima_smape': 'sMAPE_ARIMA_CV',\n",
    "        'cv_tbats_ets_smape': 'sMAPE_TBATS_ETS_CV',\n",
    "        'cv_weekly_smape': 'sMAPE_WEEKLY_CV',\n",
    "        'w_prophet': 'Weight_Prophet',\n",
    "        'w_arima': 'Weight_ARIMA',\n",
    "        'w_tbats_ets': 'Weight_TBATS_ETS',\n",
    "        'w_weekly': 'Weight_WEEKLY'\n",
    "    })\n",
    "    df['department_id'] = df['department_id'].astype(str)\n",
    "    df = apply_mapping(df, mapping)\n",
    "    ordered_cols = [\n",
    "        'department_id', 'department_name', 'vertical', 'language',\n",
    "        'sMAPE_Prophet_CV', 'sMAPE_ARIMA_CV', 'sMAPE_TBATS_ETS_CV', 'sMAPE_WEEKLY_CV',\n",
    "        'winner_model', 'Weight_Prophet', 'Weight_ARIMA', 'Weight_TBATS_ETS', 'Weight_WEEKLY'\n",
    "    ]\n",
    "    df = df[ordered_cols]\n",
    "    return df.sort_values(['vertical', 'department_id', 'language'])\n",
    "\n",
    "# ==================== capacity_error-like table ====================\n",
    "\n",
    "def compute_monthly_accuracy_with_history(monthly: pd.DataFrame,\n",
    "                                          fc_monthly: pd.DataFrame,\n",
    "                                          report_start: str) -> pd.DataFrame:\n",
    "    monthly = monthly.copy()\n",
    "    monthly['department_id'] = monthly['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(monthly['month']):\n",
    "        monthly['month'] = pd.PeriodIndex(monthly['month'], freq='M')\n",
    "\n",
    "    fc = fc_monthly.copy()\n",
    "    fc['department_id'] = fc['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(fc['month']):\n",
    "        fc['month'] = pd.PeriodIndex(fc['month'], freq='M')\n",
    "\n",
    "    start_per = pd.Period(report_start, freq='M')\n",
    "    last_actual = monthly['month'].max()\n",
    "\n",
    "    hist = (monthly.loc[monthly['month'] >= start_per, ['department_id', 'language', 'month', 'incoming_monthly']]\n",
    "            .rename(columns={'incoming_monthly': 'Actual_Volume'}))\n",
    "    hist['Forecast'] = np.nan\n",
    "\n",
    "    fut = fc[['department_id', 'language', 'month', 'forecast_monthly',\n",
    "              'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape', 'cv_weekly_smape',\n",
    "              'winner_model', 'w_prophet', 'w_arima', 'w_tbats_ets', 'w_weekly']].copy()\n",
    "    fut = fut.loc[fut['month'] > last_actual]\n",
    "    fut = fut.rename(columns={'forecast_monthly': 'Forecast'})\n",
    "    fut['Actual_Volume'] = np.nan\n",
    "\n",
    "    base = pd.concat([hist, fut], ignore_index=True, sort=False)\n",
    "\n",
    "    base['Forecast_Accuracy'] = np.where(\n",
    "        (base['Actual_Volume'].notna()) & (base['Forecast'].notna()) & (base['Actual_Volume'] > 0),\n",
    "        (1 - (np.abs(base['Forecast'] - base['Actual_Volume']) / base['Actual_Volume'])) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c7f57",
   "metadata": {},
   "source": [
    "## 12. Daily Capacity plan with ML Blending + write to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19948f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Daily capacity plan with ML blending ====================\n",
    "\n",
    "def build_daily_capacity_plan(incoming: pd.DataFrame, mapping: pd.DataFrame, prod_dept: pd.DataFrame,\n",
    "                              fc_monthly: pd.DataFrame, exo_daily: pd.DataFrame, horizon_days: int) -> pd.DataFrame:\n",
    "    # 1) Statistical daily forecast (top-down or MA fallback)\n",
    "    if USE_DAILY_FROM_MONTHLY:\n",
    "        daily_fc = build_daily_from_monthly(incoming, fc_monthly, horizon_days)\n",
    "    else:\n",
    "        # Simple moving-average fallback\n",
    "        d = incoming.copy().sort_values(['department_id', 'language', 'Date'])\n",
    "        last_date = d['Date'].max()\n",
    "        start = last_date + pd.Timedelta(days=1)\n",
    "        idx_future = pd.date_range(start=start, periods=horizon_days, freq='D')\n",
    "        rows = []\n",
    "        for (dept, lang), g in d.groupby(['department_id', 'language']):\n",
    "            g = g.sort_values('Date')\n",
    "            base = float(g['ticket_total'].tail(28).mean()) if len(g) >= 28 else float(g['ticket_total'].mean())\n",
    "            prof = (g.assign(dow=g['Date'].dt.dayofweek).groupby('dow')['ticket_total'].mean())\n",
    "            prof = (prof / prof.mean()) if prof.notna().sum() >= 3 else pd.Series(1.0, index=range(7))\n",
    "            vals = [max(0.0, base * float(prof.get(d.dayofweek, 1.0))) for d in idx_future]\n",
    "            rows.append(pd.DataFrame({'department_id': dept, 'language': lang, 'Date': idx_future, 'forecast_daily': vals}))\n",
    "        daily_fc = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(\n",
    "            columns=['department_id', 'language', 'Date', 'forecast_daily']\n",
    "        )\n",
    "\n",
    "    # 2) Split by language (dynamic if available)\n",
    "    daily_fc_lang = split_daily_forecast_by_language_dynamic(daily_fc, incoming)\n",
    "    daily_fc_lang = apply_mapping(daily_fc_lang, mapping)\n",
    "\n",
    "    # 3) Global ML per vertical (daily)\n",
    "    lgb_dict = train_global_lgb_per_vertical(incoming, mapping, exo_daily, h_days=horizon_days)\n",
    "    ml_fc = pd.concat(list(lgb_dict.values()), ignore_index=True) if lgb_dict else pd.DataFrame(\n",
    "        columns=['Date', 'department_id', 'language', 'vertical', 'forecast_daily_lgb']\n",
    "    )\n",
    "\n",
    "    # 4) Historical panel for noise\n",
    "    panel_hist = prepare_daily_panel(incoming, mapping)\n",
    "\n",
    "    # 5) Blend\n",
    "    blended = blend_daily_predictions(daily_fc_lang, ml_fc, panel_hist)\n",
    "    blended = apply_mapping(blended, mapping)\n",
    "    blended = blended.merge(prod_dept, on='department_id', how='left')\n",
    "    blended['avg_tickets_per_agent_day'] = pd.to_numeric(blended['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    blended['FTE_per_day'] = np.where(\n",
    "        blended['avg_tickets_per_agent_day'] > 0,\n",
    "        blended['forecast_daily_language_blend'] / blended['avg_tickets_per_agent_day'],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    cols = ['Date', 'department_id', 'department_name', 'vertical', 'language',\n",
    "            'forecast_daily_language', 'forecast_daily_lgb', 'forecast_daily_language_blend', 'FTE_per_day']\n",
    "    for c in ['department_name', 'vertical']:\n",
    "        if c not in blended.columns:\n",
    "            blended[c] = None\n",
    "    return blended[cols].sort_values(['Date', 'vertical', 'department_id', 'language'])\n",
    "    \n",
    "\n",
    "# ==================== MAIN ====================\n",
    "\n",
    "def main():\n",
    "    # 1) Load\n",
    "    incoming = load_incoming(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET)\n",
    "    mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "    prod = load_productivity(PRODUCTIVITY_PATH)\n",
    "    incoming = apply_mapping(incoming, mapping)\n",
    "\n",
    "    # 2) Exogenous (daily + monthly)\n",
    "    exo_daily, exo_monthly = build_exogenous_calendar(incoming, DAILY_HORIZON_DAYS)\n",
    "\n",
    "    # 3) Monthly forecasts per (dept, lang)\n",
    "    fc_monthly = forecast_per_dept_lang_monthly(incoming, exo_monthly)\n",
    "\n",
    "    # 4) capacity_error-like table\n",
    "    monthly = build_monthly_series(incoming)\n",
    "    cap_err = compute_monthly_accuracy_with_history(monthly, fc_monthly, REPORT_START_MONTH)\n",
    "    cap_err = apply_mapping(cap_err, mapping)\n",
    "    cap_err = cap_err.merge(prod, on='department_id', how='left')\n",
    "    cap_err['workdays_in_month'] = [business_days_in_month(m.start_time.year, m.start_time.month) for m in cap_err['month']]\n",
    "    cap_err['Capacity_FTE_per_day'] = np.where(\n",
    "        (pd.to_numeric(cap_err['avg_tickets_per_agent_day'], errors='coerce') > 0)\n",
    "        & (cap_err['workdays_in_month'] > 0)\n",
    "        & (cap_err['Forecast'].notna()),\n",
    "        cap_err['Forecast'] / (cap_err['avg_tickets_per_agent_day'] * cap_err['workdays_in_month']),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # 5) Daily plan (stat + ML blend)\n",
    "    daily_capacity_plan = build_daily_capacity_plan(incoming, mapping, prod, fc_monthly, exo_daily, DAILY_HORIZON_DAYS)\n",
    "\n",
    "    # 6) CV table\n",
    "    cv_table = build_cv_table(fc_monthly, mapping)\n",
    "\n",
    "    # 7) Write Excel\n",
    "    for df_out in [cap_err, daily_capacity_plan, cv_table]:\n",
    "        df_out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        (cap_err[['vertical', 'department_id', 'department_name', 'language', 'month',\n",
    "                  'Actual_Volume', 'Forecast', 'Forecast_Accuracy',\n",
    "                  'Capacity_FTE_per_day',\n",
    "                  'winner_model', 'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape', 'cv_weekly_smape',\n",
    "                  'w_prophet', 'w_arima', 'w_tbats_ets', 'w_weekly']]\n",
    "         .sort_values(['vertical', 'department_id', 'language', 'month'])\n",
    "         .to_excel(w, \"capacity_error\", index=False))\n",
    "\n",
    "        daily_capacity_plan.to_excel(w, \"daily_capacity_plan\", index=False)\n",
    "        cv_table.to_excel(w, \"mape_table_cv\", index=False)\n",
    "\n",
    "    print(\"Excel written:\", OUTPUT_XLSX)\n",
    "\n",
    "\n",
    "\n",
    "# Load manually before timing blocks\n",
    "incoming = load_incoming(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET)\n",
    "mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "prod = load_productivity(PRODUCTIVITY_PATH)\n",
    "\n",
    "incoming = apply_mapping(incoming, mapping)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "exo_daily, exo_monthly = build_exogenous_calendar(incoming, DAILY_HORIZON_DAYS)\n",
    "print(f\"[TIMING] exogenous: {time.time() - t0:.1f}s\"); t1 = time.time()\n",
    "\n",
    "fc_monthly = forecast_per_dept_lang_monthly(incoming, exo_monthly)\n",
    "print(f\"[TIMING] monthly models: {time.time() - t1:.1f}s\"); t2 = time.time()\n",
    "\n",
    "daily_capacity_plan = build_daily_capacity_plan(incoming, mapping, prod, fc_monthly, exo_daily, DAILY_HORIZON_DAYS)\n",
    "print(f\"[TIMING] daily (stat+LGBM): {time.time() - t2:.1f}s\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
