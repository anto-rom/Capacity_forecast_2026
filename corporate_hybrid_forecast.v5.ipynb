{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12efc4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:10:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:10:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:11:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:11:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:11:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:11:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:11:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:11:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:12:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:12:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:13:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:14:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:14:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:14:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:15:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:15:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:15:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:15:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:16:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:16:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:17:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:17:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:17:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:17:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:18:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:18:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:19:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:19:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:20:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:20:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:21:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:21:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:22:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:22:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:23:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:23:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:24:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:24:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:24:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:24:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:25:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:25:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:26:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:26:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:27:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:27:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:27:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:27:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:28:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:28:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:28:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:28:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:29:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:29:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:30:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:30:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:31:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:31:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:31:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:31:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:32:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:32:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:33:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:33:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:33:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:33:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:34:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:34:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:34:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:34:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:35:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:35:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:36:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:36:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:36:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:36:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:37:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:37:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:38:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:38:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:38:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:38:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:39:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:39:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:40:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:40:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:40:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:40:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:41:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:41:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:42:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:42:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:42:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:42:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:43:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:43:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:44:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:44:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:45:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:45:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:45:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:45:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:46:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:46:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:47:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:47:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:47:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:48:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:48:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:48:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:49:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:50:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:50:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:50:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:51:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:51:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:51:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:51:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:52:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:52:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:52:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:52:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:53:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:53:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:53:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:53:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:54:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:54:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:55:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:55:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:55:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:55:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:56:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:56:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:56:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:56:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:57:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:57:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:57:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:57:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:58:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:58:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:58:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:58:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:59:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:59:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:59:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:59:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:00:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:00:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:00:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:00:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:01:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:01:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:02:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:02:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:02:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:02:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:03:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:03:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:03:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:03:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:04:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:04:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:04:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:04:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\n",
      "Christmas CSV at: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\christmas_holidays_2024_2027.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Capacity Forecast – Hybrid (Prophet / ARIMA / TBATS-ETS) – v6.1\n",
    "\n",
    "Hotfix:\n",
    "- Hardened 'forecast_per_department_monthly' to always include CV/weights columns.\n",
    "- Hardened 'build_stability_report' to add missing CV/weights columns on the fly (avoids KeyError).\n",
    "\n",
    "Keeps v6 features:\n",
    "- Auto Christmas CSV with Xmas days counts (2024–2027)\n",
    "- Exogenous features from:\n",
    "   * case_reason.xlsx ('Global outage reported' proxy per dept)\n",
    "   * christmas_holidays_*.csv (xmas days per month)\n",
    "- Rate-per-workday modelling, wMAPE blending, robust smoothing (MAD), bias correction\n",
    "- Monthly->Daily reconciliation\n",
    "- Sheets: capacity_error, daily_capacity_plan, mape_table_cv, stability_report\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import Optional, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    Prophet = None\n",
    "try:\n",
    "    from tbats import TBATS\n",
    "except Exception:\n",
    "    TBATS = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "\n",
    "# Inputs (adjust if your file locations change)\n",
    "INCOMING_SOURCE_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\Incoming_new.xlsx\"  # Sheet 'Main'\n",
    "INCOMING_SHEET = \"Main\"\n",
    "DEPT_MAP_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\department.xlsx\"\n",
    "DEPT_MAP_SHEET = \"map\"\n",
    "PRODUCTIVITY_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\productivity_agents.xlsx\"\n",
    "\n",
    "# Outage proxy (case reasons)\n",
    "CASE_REASON_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\case_reason.xlsx\"\n",
    "CASE_REASON_SHEET = \"Main\"             # provided by you\n",
    "CASE_REASON_FILTER = \"Global outage reported\"\n",
    "\n",
    "# Christmas holidays CSV\n",
    "HOLIDAYS_CSV_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\christmas_holidays_2024_2027.csv\"\n",
    "HOLIDAYS_YEARS = [2024, 2025, 2026, 2027]\n",
    "INCLUDE_JAN6 = True                    # include Jan 6 (common in ES/PT/IT)\n",
    "INCLUDE_JAN_POSTXMAS = False           # monthly extra dummy for January (off by default)\n",
    "\n",
    "# Output\n",
    "OUTPUT_XLSX = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\"\n",
    "\n",
    "# Horizons and switches\n",
    "H_MONTHS = 12             # monthly forecast horizon\n",
    "DAILY_HORIZON_DAYS = 90   # daily plan horizon\n",
    "REPORT_START_MONTH = \"2025-01\"  # show historical Actuals from this month in capacity_error\n",
    "\n",
    "# Top-down reconciliation for daily forecasts\n",
    "USE_DAILY_FROM_MONTHLY = True\n",
    "\n",
    "# Optional final growth guard (disabled by default; enable if needed)\n",
    "APPLY_LOCAL_GROWTH_GUARD = False\n",
    "MAX_GROWTH = 1.8   # +80% vs local ref\n",
    "MIN_GROWTH = 0.5   # -50% vs local ref\n",
    "\n",
    "# Language shares\n",
    "LANGUAGE_SHARES = {\n",
    "    'English': 0.6435,\n",
    "    'French': 0.0741,\n",
    "    'German': 0.0860,\n",
    "    'Italian': 0.0667,\n",
    "    'Portuguese': 0.0162,\n",
    "    'Spanish': 0.1135\n",
    "}\n",
    "\n",
    "# ==================== Data loading ====================\n",
    "\n",
    "def load_incoming(path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load daily incoming volumes. Build ticket_total if needed.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Incoming file not found:\\n{path}\\n\")\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xlsm\", \".xls\"]:\n",
    "        if not sheet_name:\n",
    "            raise ValueError(\"Excel file detected but no sheet_name provided (e.g., 'Main').\")\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension for incoming data: {ext}\")\n",
    "\n",
    "    base_required = {'Date', 'department_id'}\n",
    "    missing = base_required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Incoming file must contain {sorted(list(base_required))}. Found: {list(df.columns)}\")\n",
    "\n",
    "    if 'ticket_total' not in df.columns:\n",
    "        if 'total_incoming' in df.columns:\n",
    "            df['ticket_total'] = pd.to_numeric(df['total_incoming'], errors='coerce').fillna(0)\n",
    "        elif {'incoming_from_customers', 'incoming_from_transfers'}.issubset(df.columns):\n",
    "            df['ticket_total'] = (\n",
    "                pd.to_numeric(df['incoming_from_customers'], errors='coerce').fillna(0) +\n",
    "                pd.to_numeric(df['incoming_from_transfers'], errors='coerce').fillna(0)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Missing 'ticket_total' or components to create it.\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if df['Date'].isna().any():\n",
    "        bad = df.loc[df['Date'].isna()]\n",
    "        raise ValueError(f\"Some Date values could not be parsed. Example rows:\\n{bad.head(5)}\")\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['ticket_total'] = pd.to_numeric(df['ticket_total'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    if 'department_name' in df.columns:\n",
    "        df['department_name'] = df['department_name'].astype(str).str.strip()\n",
    "    else:\n",
    "        df['department_name'] = None\n",
    "    if 'vertical' in df.columns:\n",
    "        df['vertical'] = df['vertical'].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dept_map(path: str, sheet: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load dept mapping -> department_name, vertical.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['department_id', 'department_name', 'vertical'])\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xls\"):\n",
    "        if sheet:\n",
    "            mp = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        else:\n",
    "            xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "            mp = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "    else:\n",
    "        mp = pd.read_csv(path)\n",
    "\n",
    "    rename_map = {\n",
    "        'dept_id': 'department_id',\n",
    "        'dept_name': 'department_name',\n",
    "        'name': 'department_name',\n",
    "        'segment': 'vertical',\n",
    "        'vertical_name': 'vertical'\n",
    "    }\n",
    "    mp = mp.rename(columns={k: v for k, v in rename_map.items() if k in mp.columns})\n",
    "    if 'department_id' not in mp.columns:\n",
    "        raise ValueError(f\"Department map must contain 'department_id'. Found: {list(mp.columns)}\")\n",
    "\n",
    "    mp['department_id'] = mp['department_id'].astype(str).str.strip()\n",
    "    mp['department_name'] = (mp['department_name'].astype(str).str.strip()\n",
    "                             if 'department_name' in mp.columns else None)\n",
    "    mp['vertical'] = (mp['vertical'].astype(str).str.strip()\n",
    "                      if 'vertical' in mp.columns else None)\n",
    "\n",
    "    return mp[['department_id', 'department_name', 'vertical']].drop_duplicates('department_id')\n",
    "\n",
    "\n",
    "def load_productivity(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load agent productivity and compute dept-level mean tickets/agent-day.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Productivity file not found: {path}\")\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    req = {'Date', 'agent_id', 'department_id', 'prod_total_model'}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"productivity_agents.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['prod_total_model'] = pd.to_numeric(df['prod_total_model'], errors='coerce')\n",
    "\n",
    "    prod_dept = (df.groupby('department_id', as_index=False)['prod_total_model']\n",
    "                 .mean()\n",
    "                 .rename(columns={'prod_total_model': 'avg_tickets_per_agent_day'}))\n",
    "    return prod_dept\n",
    "\n",
    "# ==================== Helpers & metrics ====================\n",
    "\n",
    "def business_days_in_month(year: int, month: int) -> int:\n",
    "    \"\"\"Approximate Mon-Fri working days in a month.\"\"\"\n",
    "    rng = pd.date_range(start=pd.Timestamp(year=year, month=month, day=1),\n",
    "                        end=pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(0),\n",
    "                        freq='D')\n",
    "    return int(np.sum(rng.weekday < 5))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred) -> float:\n",
    "    \"\"\"sMAPE robust for intermittent series.\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom[denom == 0] = 1.0\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "\n",
    "def wmape(y_true, y_pred) -> float:\n",
    "    \"\"\"Weighted MAPE: sum(|e|)/sum(|y|).\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    if denom <= 0:\n",
    "        return 200.0\n",
    "    return float(100.0 * (np.sum(np.abs(y_true - y_pred)) / denom))\n",
    "\n",
    "\n",
    "def apply_mapping(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge department_name / vertical using department_id.\"\"\"\n",
    "    merged = incoming.merge(mapping, on='department_id', how='left', suffixes=('', '_map'))\n",
    "    if 'department_name' not in merged.columns:\n",
    "        merged['department_name'] = None\n",
    "    if 'department_name_map' not in merged.columns:\n",
    "        merged['department_name_map'] = None\n",
    "    merged['department_name'] = merged['department_name'].fillna(merged['department_name_map']).fillna(\"Unknown\")\n",
    "\n",
    "    if 'vertical' not in merged.columns:\n",
    "        merged['vertical'] = None\n",
    "    if 'vertical_map' not in merged.columns:\n",
    "        merged['vertical_map'] = None\n",
    "    merged['vertical'] = merged['vertical'].fillna(merged['vertical_map']).fillna(\"Unmapped\")\n",
    "\n",
    "    drop_cols = [c for c in merged.columns if c.endswith('_map')]\n",
    "    merged.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    return merged\n",
    "\n",
    "\n",
    "def winsorize_monthly(ts_m: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Winsorize monthly series to reduce the influence of extreme outliers.\"\"\"\n",
    "    if ts_m.empty:\n",
    "        return ts_m\n",
    "    lo = ts_m.quantile(lower_q)\n",
    "    hi = ts_m.quantile(upper_q)\n",
    "    return ts_m.clip(lower=lo, upper=hi)\n",
    "\n",
    "# ---------- Safe inverse & dynamic cap ----------\n",
    "\n",
    "def expm1_safe(log_vals: np.ndarray, cap_original: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe inverse of log1p:\n",
    "    - replace non-finite logs by a very negative number (-> ~0)\n",
    "    - lower-bound logs to avoid underflow\n",
    "    - optional cap on original scale applied in log-domain and after expm1\n",
    "    \"\"\"\n",
    "    x = np.array(log_vals, dtype=float)\n",
    "    x[~np.isfinite(x)] = -50.0\n",
    "    x = np.maximum(x, -50.0)\n",
    "\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        log_cap = np.log1p(cap_original)\n",
    "        x = np.minimum(x, log_cap)\n",
    "\n",
    "    y = np.expm1(x)\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        y = np.minimum(y, cap_original)\n",
    "    return np.clip(y, 0, None)\n",
    "\n",
    "\n",
    "def compute_dynamic_cap(ts_m: pd.Series) -> float:\n",
    "    \"\"\"Generous per-department cap on the original scale to prevent explosions.\"\"\"\n",
    "    if ts_m.empty or (ts_m.max() <= 0):\n",
    "        return np.inf\n",
    "    m12 = float(ts_m.tail(12).mean()) if len(ts_m) >= 3 else float(ts_m.mean())\n",
    "    med = float(ts_m.median())\n",
    "    mx = float(ts_m.max())\n",
    "    base = max(1.0, m12, med, 1.1 * mx)\n",
    "    cap = base * 6.0  # adjust 4.0–8.0 as needed\n",
    "    return cap\n",
    "\n",
    "# ---------- Rate modelling, Xmas CSV and robust smoothing ----------\n",
    "\n",
    "def monthly_rate_series(ts_m: pd.Series) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Return (rate_per_workday, workdays series aligned to ts_m).\"\"\"\n",
    "    w = ts_m.index.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month))\n",
    "    w = w.astype(float).replace(0, np.nan)\n",
    "    rate = ts_m / w\n",
    "    return rate, w\n",
    "\n",
    "\n",
    "def robust_roll_cap(series: pd.Series, window: int = 12, K: float = 6.0) -> pd.Series:\n",
    "    \"\"\"Apply rolling Median ± K*MAD cap to stabilize spikes without flattening the series.\"\"\"\n",
    "    s = series.copy().astype(float)\n",
    "    vals = s.values\n",
    "    for i in range(len(s)):\n",
    "        lo = max(0, i - window)\n",
    "        ref = vals[lo:i] if i > 0 else []\n",
    "        if len(ref) >= 4:\n",
    "            med = np.median(ref)\n",
    "            mad = np.median(np.abs(ref - med)) + 1e-9\n",
    "            upper = med + K * mad\n",
    "            lower = max(0.0, med - K * mad)\n",
    "            vals[i] = min(max(vals[i], lower), upper)\n",
    "        else:\n",
    "            vals[i] = max(vals[i], 0.0)\n",
    "    return pd.Series(vals, index=s.index)\n",
    "\n",
    "# ==================== Christmas Holidays CSV ====================\n",
    "\n",
    "def ensure_christmas_csv(path: str = HOLIDAYS_CSV_PATH,\n",
    "                         years = HOLIDAYS_YEARS,\n",
    "                         include_jan6: bool = INCLUDE_JAN6) -> str:\n",
    "    \"\"\"Create a CSV with core Christmas holidays if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        rows.append((f\"{y}-12-24\", \"Christmas Eve\", 1))\n",
    "        rows.append((f\"{y}-12-25\", \"Christmas Day\", 1))\n",
    "        rows.append((f\"{y}-12-26\", \"Boxing/St. Stephen\", 1))\n",
    "        rows.append((f\"{y}-12-31\", \"New Year Eve\", 1))\n",
    "        ny = y + 1\n",
    "        rows.append((f\"{ny}-01-01\", \"New Year Day\", 1))\n",
    "        if include_jan6:\n",
    "            rows.append((f\"{ny}-01-06\", \"Epiphany\", 1))\n",
    "\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    pd.DataFrame(rows, columns=[\"date\",\"label\",\"is_xmas\"]).to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def load_christmas_csv(path: str = HOLIDAYS_CSV_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load the Christmas holidays CSV (ensure it first).\"\"\"\n",
    "    ensure_christmas_csv(path)\n",
    "    df = pd.read_csv(path)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['is_xmas'] = pd.to_numeric(df['is_xmas'], errors='coerce').fillna(0).astype(int)\n",
    "    return df[['date','label','is_xmas']]\n",
    "\n",
    "# ==================== Outage proxy (case_reason.xlsx) ====================\n",
    "\n",
    "def load_case_reason_proxy(path: str = CASE_REASON_PATH,\n",
    "                           sheet=CASE_REASON_SHEET) -> pd.DataFrame:\n",
    "    \"\"\"Load case_reason.xlsx and keep only rows that can act as outage proxy.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['Date', 'department_id', 'case_reason'])\n",
    "\n",
    "    df = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "\n",
    "    required = {'Date', 'department_id'}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"case_reason.xlsx must contain {sorted(list(required))}. \"\n",
    "                         f\"Found: {list(df.columns)}\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    if 'case_reason' in df.columns:\n",
    "        df['case_reason'] = df['case_reason'].astype(str).str.strip()\n",
    "        if CASE_REASON_FILTER:\n",
    "            df = df[df['case_reason'].fillna('') == CASE_REASON_FILTER]\n",
    "\n",
    "    df = df.dropna(subset=['Date','department_id'])\n",
    "    return df[['Date', 'department_id', 'case_reason']].copy()\n",
    "\n",
    "# ==================== Monthly exogenous features ====================\n",
    "\n",
    "def build_monthly_exog_from_proxy_and_xmas(month_index: pd.PeriodIndex,\n",
    "                                           department_id: str,\n",
    "                                           case_reason_df: pd.DataFrame,\n",
    "                                           xmas_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build monthly exogenous features for a given department:\n",
    "      - outage_cases_z: z-scored count of 'Global outage reported' tickets per month (proxy)\n",
    "      - xmas_days_cnt_z: z-scored count of xmas days per month from the CSV\n",
    "    Returns DataFrame indexed by month.\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(index=month_index)\n",
    "\n",
    "    # ---- Outage proxy aggregation (per department) ----\n",
    "    if case_reason_df is not None and not case_reason_df.empty:\n",
    "        tmp = case_reason_df.copy()\n",
    "        # ✅ fix: cast department_id correctly (no keyword arg in str())\n",
    "        tmp = tmp[tmp['department_id'] == str(department_id)]\n",
    "        tmp['month'] = tmp['Date'].dt.to_period('M')\n",
    "        cnt = (tmp.groupby('month', as_index=False)\n",
    "                  .size()\n",
    "                  .rename(columns={'size': 'outage_cases'})\n",
    "                  .set_index('month')\n",
    "                  .reindex(month_index)\n",
    "                  .fillna(0.0))\n",
    "        mu = float(cnt['outage_cases'].mean())\n",
    "        sd = float(cnt['outage_cases'].std(ddof=0)) + 1e-6\n",
    "        X['outage_cases_z'] = (cnt['outage_cases'] - mu) / sd\n",
    "    else:\n",
    "        X['outage_cases_z'] = 0.0\n",
    "\n",
    "    # ---- Xmas days per month (from CSV) ----\n",
    "    if xmas_df is not None and not xmas_df.empty:\n",
    "        mm = xmas_df.copy()\n",
    "        mm = mm[mm['is_xmas'] == 1]\n",
    "        mm['month'] = mm['date'].dt.to_period('M')\n",
    "        cntx = (mm.groupby('month', as_index=False)\n",
    "                  .size()\n",
    "                  .rename(columns={'size': 'xmas_days_cnt'})\n",
    "                  .set_index('month')\n",
    "                  .reindex(month_index)\n",
    "                  .fillna(0.0))\n",
    "        mux = float(cntx['xmas_days_cnt'].mean())\n",
    "        sdx = float(cntx['xmas_days_cnt'].std(ddof=0)) + 1e-6\n",
    "        X['xmas_days_cnt_z'] = (cntx['xmas_days_cnt'] - mux) / sdx\n",
    "    else:\n",
    "        X['xmas_days_cnt_z'] = 0.0\n",
    "\n",
    "    # ---- Optional January post-Christmas dummy ----\n",
    "    X['jan_postxmas'] = [1.0 if (INCLUDE_JAN_POSTXMAS and p.start_time.month == 1) else 0.0\n",
    "                         for p in month_index]\n",
    "\n",
    "    return X.fillna(0.0).astype(float)\n",
    "\n",
    "\n",
    "# ==================== Monthly modelling (log-scale, rate-aware, with exog) ====================\n",
    "\n",
    "def fit_prophet_monthly_log(ts_m: pd.Series, is_rate: bool = False,\n",
    "                            exog_train: Optional[pd.DataFrame] = None):\n",
    "    \"\"\"Fit Prophet on log1p(ts_m) with optional exogenous regressors.\"\"\"\n",
    "    if Prophet is None:\n",
    "        return None, None\n",
    "\n",
    "    y = np.log1p(ts_m.values)\n",
    "    dfp = pd.DataFrame({'ds': ts_m.index.to_timestamp(), 'y': y})\n",
    "    m = Prophet(weekly_seasonality=False, yearly_seasonality=True, daily_seasonality=False)\n",
    "\n",
    "    exog_cols = []\n",
    "    if exog_train is not None and not exog_train.empty:\n",
    "        ex_al = exog_train.reindex(ts_m.index).fillna(0.0)\n",
    "        for c in ex_al.columns:\n",
    "            m.add_regressor(c, standardize=True)\n",
    "            dfp[c] = ex_al[c].values\n",
    "            exog_cols.append(c)\n",
    "\n",
    "    m.fit(dfp)\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None,\n",
    "              exog_future: Optional[pd.DataFrame] = None):\n",
    "        future = m.make_future_dataframe(periods=h_months, freq='MS')\n",
    "        if exog_future is not None and not exog_future.empty:\n",
    "            exf = exog_future.copy()\n",
    "            exf = exf.reindex(pd.PeriodIndex(future['ds'], freq='M')).fillna(0.0)\n",
    "            for c in exog_cols:\n",
    "                future[c] = exf[c].values\n",
    "\n",
    "        pred = m.predict(future)\n",
    "        pred = pred.set_index(pd.PeriodIndex(pred['ds'], freq='M'))['yhat'].iloc[-h_months:]\n",
    "\n",
    "        vals = expm1_safe(pred.values, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=pred.index)\n",
    "\n",
    "    return m, fcast\n",
    "\n",
    "\n",
    "def fit_arima_monthly_log(ts_m: pd.Series, is_rate: bool = False,\n",
    "                          exog_train: Optional[pd.DataFrame] = None):\n",
    "    \"\"\"SARIMAX on log1p(ts_m) with a conservative grid and exogenous support.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    X = None\n",
    "    if exog_train is not None and not exog_train.empty:\n",
    "        X = exog_train.reindex(ts_m.index).fillna(0.0).values\n",
    "\n",
    "    best_aic, best_model = np.inf, None\n",
    "    pqs = [0, 1]\n",
    "    seasonal = len(ts_m) >= 12\n",
    "    PsQs = [0, 1] if seasonal else [0]\n",
    "\n",
    "    for p in pqs:\n",
    "        for d in ([1] if len(ts_m) < 36 else [0, 1]):\n",
    "            for q in pqs:\n",
    "                for P in PsQs:\n",
    "                    for D in ([0, 1] if seasonal else [0]):\n",
    "                        for Q in PsQs:\n",
    "                            try:\n",
    "                                model = SARIMAX(\n",
    "                                    y, order=(p, d, q),\n",
    "                                    seasonal_order=(P, D, Q, 12 if seasonal else 0),\n",
    "                                    exog=X,\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False\n",
    "                                ).fit(disp=False)\n",
    "                                if model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_model = model\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None,\n",
    "              exog_future: Optional[pd.DataFrame] = None):\n",
    "        Xf = None\n",
    "        if exog_future is not None and not exog_future.empty:\n",
    "            Xf = exog_future.iloc[:h_months].fillna(0.0).values\n",
    "        fc_log = best_model.get_forecast(h_months, exog=Xf).predicted_mean\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        vals = expm1_safe(fc_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=idx)\n",
    "\n",
    "    return best_model, fcast\n",
    "\n",
    "\n",
    "def fit_tbats_or_ets_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"TBATS on log1p(ts_m) if available; else ETS (log1p).\"\"\"\n",
    "    y_log = np.log1p(ts_m)\n",
    "\n",
    "    if TBATS is not None and len(ts_m) >= 12:\n",
    "        y_log_ts = pd.Series(y_log.values, index=ts_m.index.to_timestamp())\n",
    "        estimator = TBATS(use_arma_errors=False, seasonal_periods=[12])\n",
    "        model = estimator.fit(y_log_ts)\n",
    "\n",
    "        def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "            vals_log = model.forecast(steps=h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "            if is_rate and future_workdays is not None:\n",
    "                vals = vals * future_workdays.values\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "    else:\n",
    "        seasonal = 12 if len(ts_m) >= 24 else None\n",
    "        model = ExponentialSmoothing(y_log, trend='add',\n",
    "                                     seasonal=('add' if seasonal else None),\n",
    "                                     seasonal_periods=seasonal).fit()\n",
    "\n",
    "        def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "            vals_log = model.forecast(h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "            if is_rate and future_workdays is not None:\n",
    "                vals = vals * future_workdays.values\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "\n",
    "def fit_ets_damped_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"ETS with damped trend on log1p(ts_m); stable candidate for blending.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    seasonal = 12 if len(ts_m) >= 24 else None\n",
    "    model = ExponentialSmoothing(y, trend='add', damped_trend=True,\n",
    "                                 seasonal=('add' if seasonal else None),\n",
    "                                 seasonal_periods=seasonal).fit()\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "        vals_log = model.forecast(h_months)\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=idx)\n",
    "\n",
    "    return model, fcast\n",
    "\n",
    "# ==================== Adaptive CV (rate-aware) ====================\n",
    "\n",
    "def rolling_cv_monthly_adaptive_rate(ts_vol: pd.Series) -> Tuple[Optional[Dict[str, float]], Optional[Dict[str, float]]]:\n",
    "    \"\"\"Adaptive rolling-origin CV using rate modelling internally (returns sMAPE and wMAPE dicts).\"\"\"\n",
    "    n = len(ts_vol)\n",
    "    if n < 9:\n",
    "        return None, None\n",
    "    h = 3 if n >= 15 else 1\n",
    "    min_train = max(12, n - (h + 2))\n",
    "\n",
    "    s_out, w_out = [], []\n",
    "    for start in range(min_train, n - h + 1):\n",
    "        train_vol = ts_vol.iloc[:start]\n",
    "        test_vol = ts_vol.iloc[start:start + h]\n",
    "\n",
    "        train_rate, _ = monthly_rate_series(train_vol)\n",
    "        future_idx = pd.period_range(train_vol.index[-1] + 1, periods=h, freq='M')\n",
    "        future_w = future_idx.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month)).astype(float)\n",
    "\n",
    "        s_metrics, w_metrics = {}, {}\n",
    "\n",
    "        # Prophet\n",
    "        mp, fp = fit_prophet_monthly_log(train_rate, is_rate=True)\n",
    "        if fp is not None:\n",
    "            try:\n",
    "                pred_vol = fp(h_months=h, future_workdays=future_w)\n",
    "                pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "                pv[~np.isfinite(pv)] = np.nan\n",
    "                s_metrics['Prophet'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "                w_metrics['Prophet'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            except Exception:\n",
    "                s_metrics['Prophet'] = 200.0; w_metrics['Prophet'] = 200.0\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            ma, fa = fit_arima_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = fa(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['ARIMA'] = 200.0; w_metrics['ARIMA'] = 200.0\n",
    "\n",
    "        # TBATS/ETS\n",
    "        try:\n",
    "            mt, ft = fit_tbats_or_ets_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = ft(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['TBATS/ETS'] = 200.0; w_metrics['TBATS/ETS'] = 200.0\n",
    "\n",
    "        # ETS Damped\n",
    "        try:\n",
    "            me, fe = fit_ets_damped_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = fe(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['ETS_Damped'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['ETS_Damped'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['ETS_Damped'] = 200.0; w_metrics['ETS_Damped'] = 200.0\n",
    "\n",
    "        s_out.append(s_metrics); w_out.append(w_metrics)\n",
    "\n",
    "    sm = pd.DataFrame(s_out).mean().to_dict()\n",
    "    wm = pd.DataFrame(w_out).mean().to_dict()\n",
    "    return sm, wm\n",
    "\n",
    "# ==================== Blending ====================\n",
    "\n",
    "def select_or_blend_forecasts(fc_dict: Dict[str, pd.Series],\n",
    "                              cv_scores_wmape: Dict[str, float],\n",
    "                              blend: bool = True):\n",
    "    \"\"\"Blend using 1/wMAPE as weights (lower better).\"\"\"\n",
    "    scores = {k: (v if v is not None and np.isfinite(v) else 1e6) for k, v in cv_scores_wmape.items()}\n",
    "    models = [m for m in fc_dict.keys() if m in scores]\n",
    "    if not models:\n",
    "        k0 = list(fc_dict.keys())[0]\n",
    "        return fc_dict[k0], {'winner': k0, 'weights': {k0: 1.0}}\n",
    "\n",
    "    if not blend:\n",
    "        best = min(models, key=lambda m: scores[m])\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "\n",
    "    inv = {m: (1.0 / scores[m] if scores[m] > 0 else 0.0) for m in models}\n",
    "    total = sum(inv.values())\n",
    "    if total == 0:\n",
    "        best = min(models, key=lambda m: scores[m])\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "    w = {m: inv[m] / total for m in models}\n",
    "\n",
    "    idx = None\n",
    "    for s in fc_dict.values():\n",
    "        idx = s.index if idx is None else idx.union(s.index)\n",
    "    blended = sum(w[m] * fc_dict[m].reindex(idx).fillna(0) for m in models)\n",
    "    return blended, {'winner': min(models, key=lambda m: scores[m]), 'weights': w}\n",
    "\n",
    "# ==================== Monthly pipeline ====================\n",
    "\n",
    "def build_monthly_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate daily incoming to monthly by department.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['month'] = df['Date'].dt.to_period('M')\n",
    "    monthly = (df.groupby(['department_id', 'month'], as_index=False)['ticket_total']\n",
    "               .sum()\n",
    "               .rename(columns={'ticket_total': 'incoming_monthly'}))\n",
    "    return monthly\n",
    "\n",
    "\n",
    "def bias_correction(blended: pd.Series, hist_actuals: pd.Series, window: int = 6) -> pd.Series:\n",
    "    \"\"\"Simple bias correction using rolling ratio (actual/pred).\"\"\"\n",
    "    df = pd.concat([hist_actuals, blended], axis=1)\n",
    "    df.columns = ['y', 'yhat']\n",
    "    df = df.dropna()\n",
    "    if len(df) >= 3:\n",
    "        ratio = (df['y'] / df['yhat']).tail(window).clip(lower=0.5, upper=1.5).mean()\n",
    "        return blended * float(ratio)\n",
    "    return blended\n",
    "\n",
    "\n",
    "def forecast_per_department_monthly(monthly: pd.DataFrame,\n",
    "                                    case_reason_df: pd.DataFrame,\n",
    "                                    xmas_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rate-aware hybrid with exogenous features + adaptive CV + robust sanitation.\n",
    "    Ensures CV/weights columns always exist.\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    dept_ids = monthly['department_id'].unique().tolist()\n",
    "\n",
    "    for dept in dept_ids:\n",
    "        ts_vol = (monthly.loc[monthly['department_id'] == dept, ['month', 'incoming_monthly']]\n",
    "                  .sort_values('month')\n",
    "                  .set_index('month')['incoming_monthly'])\n",
    "        if not pd.api.types.is_period_dtype(ts_vol.index):\n",
    "            ts_vol.index = pd.PeriodIndex(ts_vol.index, freq='M')\n",
    "        if len(ts_vol) == 0:\n",
    "            continue\n",
    "\n",
    "        # Winsorize + rate\n",
    "        ts_vol = winsorize_monthly(ts_vol, 0.01, 0.99)\n",
    "        ts_rate, _ = monthly_rate_series(ts_vol)\n",
    "        ts_rate = ts_rate.fillna(ts_rate.median()).clip(lower=0)\n",
    "\n",
    "        # Future index & workdays\n",
    "        future_idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "        future_w = future_idx.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month)).astype(float)\n",
    "\n",
    "        # EXOG construction (train + future)\n",
    "        X_train = build_monthly_exog_from_proxy_and_xmas(ts_rate.index, str(dept), case_reason_df, xmas_df)\n",
    "        X_future = build_monthly_exog_from_proxy_and_xmas(future_idx,      str(dept), case_reason_df, xmas_df)\n",
    "\n",
    "        # CV (rate-aware)\n",
    "        try:\n",
    "            cv_smape, cv_wmape = rolling_cv_monthly_adaptive_rate(ts_vol)\n",
    "            cv_smape = cv_smape or {}\n",
    "            cv_wmape = cv_wmape or {}\n",
    "        except Exception:\n",
    "            cv_smape, cv_wmape = {}, {}\n",
    "\n",
    "        # Collect forecasts (on volume scale)\n",
    "        fc_dict: Dict[str, pd.Series] = {}\n",
    "\n",
    "        # Prophet (with exog)\n",
    "        if Prophet is not None and len(ts_rate) >= 12:\n",
    "            try:\n",
    "                _, fp = fit_prophet_monthly_log(ts_rate, is_rate=True, exog_train=X_train)\n",
    "                if fp is not None:\n",
    "                    fc_dict['Prophet'] = fp(H_MONTHS, future_workdays=future_w, exog_future=X_future)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # ARIMA (SARIMAX with exog)\n",
    "        try:\n",
    "            _, fa = fit_arima_monthly_log(ts_rate, is_rate=True, exog_train=X_train)\n",
    "            fc_dict['ARIMA'] = fa(H_MONTHS, future_workdays=future_w, exog_future=X_future)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # TBATS/ETS (no exog)\n",
    "        try:\n",
    "            _, ft = fit_tbats_or_ets_monthly_log(ts_rate, is_rate=True)\n",
    "            fc_dict['TBATS/ETS'] = ft(H_MONTHS, future_workdays=future_w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # ETS Damped (no exog)\n",
    "        try:\n",
    "            _, fe = fit_ets_damped_monthly_log(ts_rate, is_rate=True)\n",
    "            fc_dict['ETS_Damped'] = fe(H_MONTHS, future_workdays=future_w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not fc_dict:\n",
    "            idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "            val = max(0.0, float(ts_vol.mean()))\n",
    "            fc_dict['NaiveMean'] = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # Blend/select using wMAPE\n",
    "        blended, meta = select_or_blend_forecasts(fc_dict, cv_scores_wmape=cv_wmape, blend=True)\n",
    "\n",
    "        # Enforce finiteness\n",
    "        if not np.isfinite(blended.values).all():\n",
    "            finite_mask = np.isfinite(blended.values)\n",
    "            if finite_mask.any():\n",
    "                finite_mean = float(np.nanmean(blended.values[finite_mask]))\n",
    "                vals = np.where(finite_mask, blended.values, finite_mean)\n",
    "                blended = pd.Series(vals, index=blended.index)\n",
    "            else:\n",
    "                idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "                val = max(0.0, float(ts_vol.mean()))\n",
    "                blended = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # Robust smoothing (Median ± K·MAD) + bias correction\n",
    "        blended = robust_roll_cap(blended, window=12, K=6.0)\n",
    "        blended = bias_correction(blended, ts_vol, window=6)\n",
    "\n",
    "        # Optional growth guard\n",
    "        if APPLY_LOCAL_GROWTH_GUARD:\n",
    "            ref = max(1.0, float(ts_vol.tail(12).mean())) if len(ts_vol) else 1.0\n",
    "            blended = blended.clip(lower=ref * MIN_GROWTH, upper=ref * MAX_GROWTH)\n",
    "\n",
    "        # Extract safe weights for sheet columns\n",
    "        w_prophet = meta['weights'].get('Prophet', np.nan)\n",
    "        w_arima = meta['weights'].get('ARIMA', np.nan)\n",
    "        w_tbats = meta['weights'].get('TBATS/ETS', np.nan)\n",
    "\n",
    "        for per, val in blended.items():\n",
    "            out_rows.append({\n",
    "                'department_id': dept,\n",
    "                'month': per,\n",
    "                'forecast_monthly': max(0.0, float(val)),\n",
    "                'cv_prophet_smape': cv_smape.get('Prophet', np.nan),\n",
    "                'cv_arima_smape': cv_smape.get('ARIMA', np.nan),\n",
    "                'cv_tbats_ets_smape': cv_smape.get('TBATS/ETS', np.nan),\n",
    "                'winner_model': meta.get('winner', np.nan),\n",
    "                'blend_prophet_w': w_prophet,\n",
    "                'blend_arima_w': w_arima,\n",
    "                'blend_tbats_ets_w': w_tbats,\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows)\n",
    "\n",
    "    # --- Hotfix: ensure expected columns always exist ---\n",
    "    expected_cols = [\n",
    "        'forecast_monthly',\n",
    "        'cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "        'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w'\n",
    "    ]\n",
    "    for c in expected_cols:\n",
    "        if c not in df_out.columns:\n",
    "            df_out[c] = np.nan\n",
    "\n",
    "    if not df_out.empty:\n",
    "        df_out['department_id'] = df_out['department_id'].astype(str)\n",
    "        if not pd.api.types.is_period_dtype(df_out['month']):\n",
    "            df_out['month'] = pd.PeriodIndex(df_out['month'], freq='M')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def compute_monthly_accuracy_with_history(monthly: pd.DataFrame,\n",
    "                                          fc_monthly: pd.DataFrame,\n",
    "                                          report_start: str) -> pd.DataFrame:\n",
    "    \"\"\"Build capacity_error-like table with historical Actuals and future Forecasts.\"\"\"\n",
    "    monthly = monthly.copy()\n",
    "    monthly['department_id'] = monthly['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(monthly['month']):\n",
    "        monthly['month'] = pd.PeriodIndex(monthly['month'], freq='M')\n",
    "\n",
    "    fc = fc_monthly.copy()\n",
    "    fc['department_id'] = fc['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(fc['month']):\n",
    "        fc['month'] = pd.PeriodIndex(fc['month'], freq='M')\n",
    "\n",
    "    start_per = pd.Period(report_start, freq='M')\n",
    "    last_actual = monthly['month'].max()\n",
    "\n",
    "    hist = (monthly.loc[monthly['month'] >= start_per, ['department_id', 'month', 'incoming_monthly']]\n",
    "            .rename(columns={'incoming_monthly': 'Actual_Volume'}))\n",
    "    hist['Forecast'] = np.nan\n",
    "\n",
    "    fut = fc[['department_id', 'month', 'forecast_monthly',\n",
    "              'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "              'winner_model', 'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']].copy()\n",
    "    fut = fut.loc[fut['month'] > last_actual]\n",
    "    fut = fut.rename(columns={'forecast_monthly': 'Forecast'})\n",
    "    fut['Actual_Volume'] = np.nan\n",
    "\n",
    "    base = pd.concat([hist, fut], ignore_index=True, sort=False)\n",
    "\n",
    "    base['Forecast_Accuracy'] = np.where(\n",
    "        (base['Actual_Volume'].notna()) & (base['Forecast'].notna()) & (base['Actual_Volume'] > 0),\n",
    "        (1 - (np.abs(base['Forecast'] - base['Actual_Volume']) / base['Actual_Volume'])) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    return base\n",
    "\n",
    "\n",
    "def compute_capacity_monthly(cap_df: pd.DataFrame, prod_dept: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute FTE/day needed per month.\"\"\"\n",
    "    out = cap_df.merge(prod_dept, on='department_id', how='left')\n",
    "    out['avg_tickets_per_agent_day'] = pd.to_numeric(out['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    out['avg_tickets_per_agent_day'] = out['avg_tickets_per_agent_day'].replace(0, np.nan)\n",
    "    out['workdays_in_month'] = [business_days_in_month(m.start_time.year, m.start_time.month) for m in out['month']]\n",
    "    out['Capacity_FTE_per_day'] = np.where(\n",
    "        (out['avg_tickets_per_agent_day'] > 0) & (out['workdays_in_month'] > 0) & (out['Forecast'].notna()),\n",
    "        out['Forecast'] / (out['avg_tickets_per_agent_day'] * out['workdays_in_month']),\n",
    "        np.nan\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_cv_table(fc_monthly: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build mape_table_cv with sMAPE, best model and weights.\"\"\"\n",
    "    if fc_monthly is None or fc_monthly.empty:\n",
    "        raise ValueError(\"fc_monthly is empty; cannot build CV table.\")\n",
    "    cols_keep = [\n",
    "        'department_id',\n",
    "        'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "        'winner_model',\n",
    "        'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w'\n",
    "    ]\n",
    "    # Hotfix: add any missing columns as NaN to avoid KeyError\n",
    "    for c in cols_keep:\n",
    "        if c not in fc_monthly.columns:\n",
    "            fc_monthly[c] = np.nan\n",
    "\n",
    "    df = (fc_monthly[cols_keep]\n",
    "          .drop_duplicates(subset=['department_id'])\n",
    "          .copy())\n",
    "    df = df.rename(columns={\n",
    "        'cv_prophet_smape': 'sMAPE_Prophet_CV',\n",
    "        'cv_arima_smape': 'sMAPE_ARIMA_CV',\n",
    "        'cv_tbats_ets_smape': 'sMAPE_TBATS_ETS_CV',\n",
    "        'winner_model': 'Best_Model',\n",
    "        'blend_prophet_w': 'Weight_Prophet',\n",
    "        'blend_arima_w': 'Weight_ARIMA',\n",
    "        'blend_tbats_ets_w': 'Weight_TBATS_ETS',\n",
    "    })\n",
    "    df['department_id'] = df['department_id'].astype(str)\n",
    "    df = apply_mapping(df, mapping)\n",
    "    ordered_cols = [\n",
    "        'department_id', 'department_name', 'vertical',\n",
    "        'sMAPE_Prophet_CV', 'sMAPE_ARIMA_CV', 'sMAPE_TBATS_ETS_CV',\n",
    "        'Best_Model',\n",
    "        'Weight_Prophet', 'Weight_ARIMA', 'Weight_TBATS_ETS'\n",
    "    ]\n",
    "    df = df[ordered_cols]\n",
    "    return df.sort_values(['vertical', 'department_id'])\n",
    "\n",
    "# ==================== Daily plan (reconciled) ====================\n",
    "\n",
    "def dow_profile(g: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Build normalized day-of-week profile for a department, fallback to uniform.\"\"\"\n",
    "    prof = (g.assign(dow=g['Date'].dt.dayofweek)\n",
    "              .groupby('dow')['ticket_total']\n",
    "              .mean())\n",
    "    if prof.notna().sum() >= 3:\n",
    "        prof = prof / prof.mean()\n",
    "    else:\n",
    "        prof = pd.Series(1.0, index=range(7))\n",
    "    return prof\n",
    "\n",
    "\n",
    "def disaggregate_month_to_days(dept_df: pd.DataFrame,\n",
    "                               month_period: pd.Period,\n",
    "                               target_sum: float) -> pd.DataFrame:\n",
    "    \"\"\"Allocate monthly forecast to each day in that month using recent DOW profile.\"\"\"\n",
    "    start = month_period.start_time\n",
    "    end = month_period.end_time\n",
    "    days = pd.date_range(start=start, end=end, freq='D')\n",
    "\n",
    "    hist = dept_df.sort_values('Date').tail(90)\n",
    "    profile = dow_profile(hist)\n",
    "\n",
    "    weights = np.array([profile.get(d.dayofweek, 1.0) for d in days], dtype=float)\n",
    "    weights = np.maximum(weights, 1e-6)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    alloc = target_sum * weights\n",
    "    return pd.DataFrame({'Date': days, 'forecast_daily': alloc})\n",
    "\n",
    "\n",
    "def build_daily_from_monthly(incoming: pd.DataFrame,\n",
    "                             fc_monthly: pd.DataFrame,\n",
    "                             horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Top-down daily plan.\"\"\"\n",
    "    last_date = incoming['Date'].max()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    end = start + pd.Timedelta(days=horizon_days - 1)\n",
    "    future_months = pd.period_range(start=start.to_period('M'),\n",
    "                                    end=end.to_period('M'), freq='M')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in incoming.groupby('department_id'):\n",
    "        for m in future_months:\n",
    "            fcm = fc_monthly[(fc_monthly['department_id'] == dept) & (fc_monthly['month'] == m)]\n",
    "            if fcm.empty:\n",
    "                continue\n",
    "            target = float(fcm['forecast_monthly'].iloc[0])\n",
    "            if target <= 0:\n",
    "                continue\n",
    "            alloc_df = disaggregate_month_to_days(g, m, target)\n",
    "            alloc_df = alloc_df[(alloc_df['Date'] >= start) & (alloc_df['Date'] <= end)]\n",
    "            alloc_df.insert(0, 'department_id', dept)\n",
    "            rows.append(alloc_df)\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_daily_by_language(df_daily_fc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Split daily forecast by fixed language shares.\"\"\"\n",
    "    parts = []\n",
    "    for lang, w in LANGUAGE_SHARES.items():\n",
    "        tmp = df_daily_fc.copy()\n",
    "        tmp['language'] = lang\n",
    "        tmp['forecast_daily_language'] = tmp['forecast_daily'] * w\n",
    "        parts.append(tmp)\n",
    "    out = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    return out\n",
    "\n",
    "\n",
    "def forecast_daily_baseline(df_daily: pd.DataFrame, horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Independent daily baseline (optional).\"\"\"\n",
    "    df = df_daily.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df = df.sort_values(['department_id', 'Date'])\n",
    "    last_date = df['Date'].max()\n",
    "    if pd.isna(last_date):\n",
    "        raise ValueError(\"forecast_daily_baseline: No valid dates in incoming.\")\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    idx_future = pd.date_range(start=start, periods=horizon_days, freq='D')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in df.groupby('department_id'):\n",
    "        g = g.sort_values('Date')\n",
    "        if len(g) >= 28:\n",
    "            roll_mean = (g.set_index('Date')['ticket_total']\n",
    "                         .rolling(window=28, min_periods=1)\n",
    "                         .mean()\n",
    "                         .iloc[-1])\n",
    "            base = float(roll_mean) if np.isfinite(roll_mean) else float(g['ticket_total'].mean())\n",
    "        else:\n",
    "            base = float(g['ticket_total'].mean())\n",
    "\n",
    "        prof = dow_profile(g)\n",
    "        vals = []\n",
    "        for d in idx_future:\n",
    "            w = prof[d.dayofweek] if d.dayofweek in prof.index else 1.0\n",
    "            vals.append(max(0.0, base * float(w)))\n",
    "        rows.append(pd.DataFrame({'department_id': dept, 'Date': idx_future, 'forecast_daily': vals}))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "\n",
    "\n",
    "def build_daily_capacity_plan(incoming: pd.DataFrame,\n",
    "                              mapping: pd.DataFrame,\n",
    "                              prod_dept: pd.DataFrame,\n",
    "                              fc_monthly: pd.DataFrame,\n",
    "                              horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"End-to-end daily plan with reconciliation and FTE/day computation.\"\"\"\n",
    "    if USE_DAILY_FROM_MONTHLY:\n",
    "        daily_fc = build_daily_from_monthly(incoming, fc_monthly, horizon_days)\n",
    "    else:\n",
    "        daily_fc = forecast_daily_baseline(incoming, horizon_days)\n",
    "\n",
    "    daily_fc_lang = split_daily_by_language(daily_fc)\n",
    "    daily_fc_lang = apply_mapping(daily_fc_lang, mapping)\n",
    "    daily_fc_lang = daily_fc_lang.merge(prod_dept, on='department_id', how='left')\n",
    "\n",
    "    daily_fc_lang['avg_tickets_per_agent_day'] = pd.to_numeric(daily_fc_lang['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    daily_fc_lang['FTE_per_day'] = np.where(\n",
    "        daily_fc_lang['avg_tickets_per_agent_day'] > 0,\n",
    "        daily_fc_lang['forecast_daily_language'] / daily_fc_lang['avg_tickets_per_agent_day'],\n",
    "        np.nan\n",
    "    )\n",
    "    cols = ['Date', 'department_id', 'department_name', 'vertical', 'language',\n",
    "            'forecast_daily_language', 'FTE_per_day']\n",
    "    daily_plan = daily_fc_lang[cols].sort_values(['Date', 'vertical', 'department_id', 'language'])\n",
    "    return daily_plan\n",
    "\n",
    "# ==================== Stability Report (hardened) ====================\n",
    "\n",
    "def build_stability_report(monthly: pd.DataFrame,\n",
    "                           fc_monthly: pd.DataFrame,\n",
    "                           daily_capacity_plan: pd.DataFrame,\n",
    "                           mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Diagnostic sheet with:\n",
    "      - forecast_monthly vs sum of daily (reconciliation diff)\n",
    "      - ref_mean_12m: mean of last 12 actual months\n",
    "      - forecast_vs_ref_ratio\n",
    "      - CV sMAPE (from fc_monthly), Best_Model, blend weights\n",
    "    Hardened to add any missing columns as NaN (avoids KeyError).\n",
    "    \"\"\"\n",
    "    m = monthly.copy()\n",
    "    m['department_id'] = m['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(m['month']):\n",
    "        m['month'] = pd.PeriodIndex(m['month'], freq='M')\n",
    "    last_actual = m['month'].max()\n",
    "\n",
    "    f = fc_monthly.copy()\n",
    "    f['department_id'] = f['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(f['month']):\n",
    "        f['month'] = pd.PeriodIndex(f['month'], freq='M')\n",
    "    f = f[f['month'] > last_actual].copy()\n",
    "\n",
    "    # Reference mean (last 12 actual months per dept)\n",
    "    ref12 = (m.groupby('department_id')\n",
    "               .apply(lambda g: g.set_index('month')['incoming_monthly'].sort_index().tail(12).mean())\n",
    "               .rename('ref_mean_12m')\n",
    "               .reset_index())\n",
    "\n",
    "    # Daily reconciliation: sum by dept-month across languages\n",
    "    d = daily_capacity_plan.copy()\n",
    "    d['department_id'] = d['department_id'].astype(str)\n",
    "    d['month'] = pd.to_datetime(d['Date']).dt.to_period('M')\n",
    "    daily_sum = (d.groupby(['department_id','month'], as_index=False)['forecast_daily_language']\n",
    "                   .sum()\n",
    "                   .rename(columns={'forecast_daily_language':'daily_sum_monthly'}))\n",
    "\n",
    "    rep = (f.merge(ref12, on='department_id', how='left')\n",
    "             .merge(daily_sum, on=['department_id','month'], how='left'))\n",
    "\n",
    "    rep['reconcile_diff'] = rep['daily_sum_monthly'] - rep['forecast_monthly']\n",
    "    rep['forecast_vs_ref_ratio'] = np.where(rep['ref_mean_12m'] > 0,\n",
    "                                            rep['forecast_monthly'] / rep['ref_mean_12m'],\n",
    "                                            np.nan)\n",
    "\n",
    "    # Attach mapping & CV/weights (deduplicate per dept)\n",
    "    # Hotfix: ensure columns exist in fc_monthly before selecting\n",
    "    for c in ['cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "              'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w']:\n",
    "        if c not in fc_monthly.columns:\n",
    "            fc_monthly[c] = np.nan\n",
    "\n",
    "    head = (fc_monthly[['department_id','cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "                        'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w']]\n",
    "            .drop_duplicates('department_id'))\n",
    "    rep = rep.merge(head, on='department_id', how='left')\n",
    "    rep = apply_mapping(rep, mapping)\n",
    "\n",
    "    # Order columns for readability (add any missing as NaN to avoid KeyError)\n",
    "    cols = ['vertical','department_id','department_name','month',\n",
    "            'forecast_monthly','daily_sum_monthly','reconcile_diff',\n",
    "            'ref_mean_12m','forecast_vs_ref_ratio',\n",
    "            'cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "            'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w']\n",
    "\n",
    "    for c in cols:\n",
    "        if c not in rep.columns:\n",
    "            rep[c] = np.nan\n",
    "\n",
    "    rep = rep[cols].sort_values(['vertical','department_id','month'])\n",
    "    return rep\n",
    "\n",
    "# ==================== Main & Excel writing ====================\n",
    "\n",
    "def main():\n",
    "    # 0) Ensure Christmas CSV exists and load it\n",
    "    ensure_christmas_csv(HOLIDAYS_CSV_PATH, HOLIDAYS_YEARS, INCLUDE_JAN6)\n",
    "    xmas_df = load_christmas_csv(HOLIDAYS_CSV_PATH)\n",
    "\n",
    "    # 1) Load inputs\n",
    "    incoming = load_incoming(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET)\n",
    "    mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "    prod = load_productivity(PRODUCTIVITY_PATH)\n",
    "\n",
    "    # 2) Load exogenous proxy\n",
    "    case_reason_df = load_case_reason_proxy(CASE_REASON_PATH, CASE_REASON_SHEET)\n",
    "\n",
    "    # 3) Monthly forecast (rate-aware + exog)\n",
    "    monthly = build_monthly_series(incoming)\n",
    "    fc_monthly = forecast_per_department_monthly(monthly, case_reason_df, xmas_df)\n",
    "\n",
    "    # 4) capacity_error (historicals + future forecast)\n",
    "    cap_err = compute_monthly_accuracy_with_history(monthly, fc_monthly, REPORT_START_MONTH)\n",
    "    cap_err = compute_capacity_monthly(cap_err, prod)\n",
    "    cap_err = apply_mapping(cap_err, mapping)\n",
    "\n",
    "    # 5) Daily plan (reconciled with monthly by default)\n",
    "    daily_capacity_plan = build_daily_capacity_plan(incoming, mapping, prod, fc_monthly, DAILY_HORIZON_DAYS)\n",
    "\n",
    "    # 6) CV table\n",
    "    cv_table = build_cv_table(fc_monthly, mapping)\n",
    "\n",
    "    # 7) Stability report (hardened)\n",
    "    stability_report = build_stability_report(monthly, fc_monthly, daily_capacity_plan, mapping)\n",
    "\n",
    "    # 8) Ensure no inf/-inf propagate to Excel\n",
    "    for df_out in [cap_err, daily_capacity_plan, cv_table, stability_report]:\n",
    "        df_out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 9) Write Excel with required sheet names\n",
    "    with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        (cap_err[['vertical', 'department_id', 'department_name', 'month',\n",
    "                  'Actual_Volume', 'Forecast', 'Forecast_Accuracy',\n",
    "                  'Capacity_FTE_per_day',\n",
    "                  'winner_model', 'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "                  'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']]\n",
    "         .sort_values(['vertical', 'department_id', 'month'])\n",
    "         .to_excel(w, \"capacity_error\", index=False))\n",
    "\n",
    "        daily_capacity_plan.to_excel(w, \"daily_capacity_plan\", index=False)\n",
    "        cv_table.to_excel(w, \"mape_table_cv\", index=False)\n",
    "        stability_report.to_excel(w, \"stability_report\", index=False)\n",
    "\n",
    "    print(\"Excel written:\", OUTPUT_XLSX)\n",
    "    print(\"Christmas CSV at:\", HOLIDAYS_CSV_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
