{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e709ee08",
   "metadata": {},
   "source": [
    "# Corporate Hybrid Forecast Notebook ((Prophet vs ARIMA vs TBATS/ETS)) – v3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f323b68",
   "metadata": {},
   "source": [
    "## 01 - Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c4f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hybrid 3-Way (Prophet vs ARIMA vs TBATS/ETS) with 12-month horizon,\n",
    "What's included:\n",
    "- Stable monthly modelling in log-scale (log1p) across Prophet/ARIMA/TBATS/ETS\n",
    "- Safe inverse transform with expm1_safe + per-department dynamic cap (prevents inf/overflows)\n",
    "- Light winsorization of monthly history per department (tames accidental outliers)\n",
    "- Adaptive CV with strict sanitation (no-finite folds penalized, prevents sMAPE=200 artifacts)\n",
    "- Temporal reconciliation (monthly -> daily) preserving day-of-week profile\n",
    "- capacity_error shows historical Actuals from REPORT_START_MONTH and future Forecasts\n",
    "- Sheets: capacity_error, daily_capacity_plan, mape_table_cv\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from typing import Optional, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Forecasting libraries\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    Prophet = None\n",
    "try:\n",
    "    from tbats import TBATS\n",
    "except Exception:\n",
    "    TBATS = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "\n",
    "# Inputs (adjust if your file locations change)\n",
    "INCOMING_SOURCE_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\Incoming_new.xlsx\"  # Sheet 'Main'\n",
    "INCOMING_SHEET = \"Main\"\n",
    "DEPT_MAP_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\department.xlsx\"\n",
    "DEPT_MAP_SHEET = \"map\"\n",
    "PRODUCTIVITY_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\productivity_agents.xlsx\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_XLSX = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\"\n",
    "\n",
    "# Horizons and switches\n",
    "H_MONTHS = 12             # monthly forecast horizon\n",
    "DAILY_HORIZON_DAYS = 90   # daily plan horizon\n",
    "REPORT_START_MONTH = \"2025-01\"  # show historical Actuals from this month in capacity_error\n",
    "\n",
    "# Top-down reconciliation for daily forecasts\n",
    "USE_DAILY_FROM_MONTHLY = True\n",
    "\n",
    "# Strong \"hard clipping\" at output is disabled (we now use dynamic caps inside expm1_safe)\n",
    "ENABLE_FORECAST_CLIP = False\n",
    "FORECAST_CLIP_MULTIPLIER = 2.5  # kept for reference if you ever want to re-enable\n",
    "\n",
    "# Organization-specific week rule (reserved)\n",
    "WEEKLY_START_THU = True\n",
    "\n",
    "# Fixed language shares\n",
    "LANGUAGE_SHARES = {\n",
    "    'English': 0.6435,\n",
    "    'French': 0.0741,\n",
    "    'German': 0.0860,\n",
    "    'Italian': 0.0667,\n",
    "    'Portuguese': 0.0162,\n",
    "    'Spanish': 0.1135\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c5b8a",
   "metadata": {},
   "source": [
    "## 02. Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b259fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_incoming(path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load daily incoming volumes from Excel/CSV.\n",
    "    Expected columns: Date, department_id, ticket_total (or construct it).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Incoming file not found:\\n{path}\\n\"\n",
    "            \"Please update INCOMING_SOURCE_PATH to the correct location.\"\n",
    "        )\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xlsm\", \".xls\"]:\n",
    "        if not sheet_name:\n",
    "            raise ValueError(\"Excel file detected but no sheet_name provided (e.g., 'Main').\")\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension for incoming data: {ext}\")\n",
    "\n",
    "    # Basic columns\n",
    "    base_required = {'Date', 'department_id'}\n",
    "    missing_base = base_required - set(df.columns)\n",
    "    if missing_base:\n",
    "        raise ValueError(\n",
    "            f\"Incoming file must contain columns: {sorted(list(base_required))}. \"\n",
    "            f\"Found columns: {list(df.columns)}. Missing: {sorted(list(missing_base))}\"\n",
    "        )\n",
    "\n",
    "    # ticket_total creation\n",
    "    if 'ticket_total' not in df.columns:\n",
    "        if 'total_incoming' in df.columns:\n",
    "            df['ticket_total'] = pd.to_numeric(df['total_incoming'], errors='coerce').fillna(0)\n",
    "        elif {'incoming_from_customers', 'incoming_from_transfers'}.issubset(df.columns):\n",
    "            df['ticket_total'] = (\n",
    "                pd.to_numeric(df['incoming_from_customers'], errors='coerce').fillna(0) +\n",
    "                pd.to_numeric(df['incoming_from_transfers'], errors='coerce').fillna(0)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Incoming file must contain 'ticket_total' or 'total_incoming' or \"\n",
    "                \"both 'incoming_from_customers' and 'incoming_from_transfers'. \"\n",
    "                f\"Found columns: {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "    # Dtypes\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if df['Date'].isna().any():\n",
    "        bad = df.loc[df['Date'].isna()]\n",
    "        raise ValueError(f\"Some Date values could not be parsed. Example rows:\\n{bad.head(5)}\")\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['ticket_total'] = pd.to_numeric(df['ticket_total'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    # Optional columns\n",
    "    if 'department_name' in df.columns:\n",
    "        df['department_name'] = df['department_name'].astype(str).str.strip()\n",
    "    else:\n",
    "        df['department_name'] = None\n",
    "    if 'vertical' in df.columns:\n",
    "        df['vertical'] = df['vertical'].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dept_map(path: str, sheet: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dept mapping -> department_name, vertical.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['department_id', 'department_name', 'vertical'])\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xls\"):\n",
    "        if sheet:\n",
    "            mp = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        else:\n",
    "            xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "            mp = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "    else:\n",
    "        mp = pd.read_csv(path)\n",
    "\n",
    "    rename_map = {\n",
    "        'dept_id': 'department_id',\n",
    "        'dept_name': 'department_name',\n",
    "        'name': 'department_name',\n",
    "        'segment': 'vertical',\n",
    "        'vertical_name': 'vertical'\n",
    "    }\n",
    "    mp = mp.rename(columns={k: v for k, v in rename_map.items() if k in mp.columns})\n",
    "    if 'department_id' not in mp.columns:\n",
    "        raise ValueError(f\"Department map must contain 'department_id'. Found: {list(mp.columns)}\")\n",
    "\n",
    "    mp['department_id'] = mp['department_id'].astype(str).str.strip()\n",
    "    mp['department_name'] = (mp['department_name'].astype(str).str.strip()\n",
    "                             if 'department_name' in mp.columns else None)\n",
    "    mp['vertical'] = (mp['vertical'].astype(str).str.strip()\n",
    "                      if 'vertical' in mp.columns else None)\n",
    "\n",
    "    return mp[['department_id', 'department_name', 'vertical']].drop_duplicates('department_id')\n",
    "\n",
    "\n",
    "def load_productivity(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load agent productivity and compute dept-level mean tickets per agent-day.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Productivity file not found: {path}\")\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    req = {'Date', 'agent_id', 'department_id', 'prod_total_model'}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"productivity_agents.xlsx missing columns: {sorted(list(missing))}. \"\n",
    "                         f\"Found: {list(df.columns)}\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['prod_total_model'] = pd.to_numeric(df['prod_total_model'], errors='coerce')\n",
    "\n",
    "    prod_dept = (df.groupby('department_id', as_index=False)['prod_total_model']\n",
    "                 .mean()\n",
    "                 .rename(columns={'prod_total_model': 'avg_tickets_per_agent_day'}))\n",
    "    return prod_dept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64578b79",
   "metadata": {},
   "source": [
    "## 03 - Utilities & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b60ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def month_floor(dt: pd.Timestamp) -> pd.Timestamp:\n",
    "    return pd.Timestamp(year=dt.year, month=dt.month, day=1)\n",
    "\n",
    "\n",
    "def business_days_in_month(year: int, month: int) -> int:\n",
    "    \"\"\"Approximate Mon-Fri working days in a month.\"\"\"\n",
    "    rng = pd.date_range(start=pd.Timestamp(year=year, month=month, day=1),\n",
    "                        end=pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(0),\n",
    "                        freq='D')\n",
    "    return int(np.sum(rng.weekday < 5))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred) -> float:\n",
    "    \"\"\"sMAPE robust for intermittent series.\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom[denom == 0] = 1.0\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "\n",
    "def apply_mapping(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge department_name / vertical using department_id.\n",
    "    Prefer incoming values over mapping values when available.\n",
    "    \"\"\"\n",
    "    merged = incoming.merge(mapping, on='department_id', how='left', suffixes=('', '_map'))\n",
    "\n",
    "    if 'department_name' not in merged.columns:\n",
    "        merged['department_name'] = None\n",
    "    if 'department_name_map' not in merged.columns:\n",
    "        merged['department_name_map'] = None\n",
    "    merged['department_name'] = merged['department_name'].fillna(merged['department_name_map']).fillna(\"Unknown\")\n",
    "\n",
    "    if 'vertical' not in merged.columns:\n",
    "        merged['vertical'] = None\n",
    "    if 'vertical_map' not in merged.columns:\n",
    "        merged['vertical_map'] = None\n",
    "    merged['vertical'] = merged['vertical'].fillna(merged['vertical_map']).fillna(\"Unmapped\")\n",
    "\n",
    "    drop_cols = [c for c in merged.columns if c.endswith('_map')]\n",
    "    merged.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ==================== v4.1 winsorization ====================\n",
    "\n",
    "def winsorize_monthly(ts_m: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Winsorize monthly series to reduce the influence of extreme outliers.\"\"\"\n",
    "    if ts_m.empty:\n",
    "        return ts_m\n",
    "    lo = ts_m.quantile(lower_q)\n",
    "    hi = ts_m.quantile(upper_q)\n",
    "    return ts_m.clip(lower=lo, upper=hi)\n",
    "\n",
    "# ==================== v4.2 safe inverse & dynamic cap ====================\n",
    "\n",
    "def expm1_safe(log_vals: np.ndarray, cap_original: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe inverse of log1p:\n",
    "    - replace non-finite logs by a very negative number (-> ~0)\n",
    "    - lower-bound logs to avoid underflow\n",
    "    - optional cap on original scale applied in log-domain and after expm1\n",
    "    \"\"\"\n",
    "    x = np.array(log_vals, dtype=float)\n",
    "    x[~np.isfinite(x)] = -50.0              # expm1(-50) ~ 0\n",
    "    x = np.maximum(x, -50.0)                # avoid extreme negatives\n",
    "\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        log_cap = np.log1p(cap_original)\n",
    "        x = np.minimum(x, log_cap)\n",
    "\n",
    "    y = np.expm1(x)\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        y = np.minimum(y, cap_original)\n",
    "    return np.clip(y, 0, None)\n",
    "\n",
    "\n",
    "def compute_dynamic_cap(ts_m: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Build a generous per-department cap on the original scale to prevent explosions.\n",
    "    It allows reasonable growth while remaining finite.\n",
    "    \"\"\"\n",
    "    if ts_m.empty or (ts_m.max() <= 0):\n",
    "        return np.inf  # no cap if no basis\n",
    "    m12 = float(ts_m.tail(12).mean()) if len(ts_m) >= 3 else float(ts_m.mean())\n",
    "    med = float(ts_m.median())\n",
    "    mx = float(ts_m.max())\n",
    "    base = max(1.0, m12, med, 1.1 * mx)\n",
    "    cap = base * 6.0  # adjust 4.0–8.0 as needed\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac18c10",
   "metadata": {},
   "source": [
    "## 04. Modelling (Hybrid 3-Way Prophet / ARIMA / TBATS-ETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ff958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: prepare monthly rate series ---\n",
    "def monthly_rate_series(ts_m: pd.Series) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Return (rate_per_workday, workdays series aligned to ts_m).\"\"\"\n",
    "    w = ts_m.index.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month))\n",
    "    w = w.astype(float).replace(0, np.nan)  # guard\n",
    "    rate = ts_m / w\n",
    "    return rate, w\n",
    "\n",
    "# In forecast_per_department_monthly(...), for each dept:\n",
    "ts = winsorize_monthly(ts, 0.01, 0.99)\n",
    "\n",
    "# Build rate series and use it for modelling\n",
    "ts_rate, w_hist = monthly_rate_series(ts)\n",
    "ts_rate = ts_rate.fillna(ts_rate.median()).clip(lower=0)\n",
    "\n",
    "# Pass ts_rate instead of ts to the fit_*_monthly_log functions:\n",
    "#   _, fp = fit_prophet_monthly_log(ts_rate)   # etc.\n",
    "# And inside each fcast(), after getting predicted \"rate\":\n",
    "#   <pred_rate> -> multiply by workdays of future month to return to volume\n",
    "\n",
    "def fit_arima_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    y = np.log1p(ts_m)  # ts_m here may already be a rate-series\n",
    "    # ... (grid ARIMA igual) ...\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "        fc_log = best_model.get_forecast(h_months).predicted_mean\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        cap = compute_dynamic_cap(ts_m if not is_rate else ts_m * 22.0)  # if rate, build cap on approximate volume\n",
    "        rate = expm1_safe(fc_log, cap_original=None)  # cap on rate not needed\n",
    "        vol = rate if future_workdays is None else rate * future_workdays.values\n",
    "        return pd.Series(vol, index=idx)\n",
    "    return best_model, fcast\n",
    "    \n",
    "# compute future workdays for the next H_MONTHS\n",
    "future_idx = pd.period_range(ts.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "future_w = future_idx.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month)).astype(float)\n",
    "\n",
    "# pass is_rate=True and future_workdays=future_w to all fcasts\n",
    "_, fa = fit_arima_monthly_log(ts_rate, is_rate=True)\n",
    "fc_dict['ARIMA'] = fa(H_MONTHS, future_workdays=future_w)\n",
    "# idem Prophet/TBATS/ETS\n",
    "\n",
    "def fit_prophet_monthly_log(ts_m: pd.Series):\n",
    "    \"\"\"Fit Prophet on log1p(monthly).\"\"\"\n",
    "    if Prophet is None:\n",
    "        return None, None\n",
    "    y = np.log1p(ts_m.values)\n",
    "    dfp = pd.DataFrame({'ds': ts_m.index.to_timestamp(), 'y': y})\n",
    "    m = Prophet(weekly_seasonality=False, yearly_seasonality=True, daily_seasonality=False)\n",
    "    m.fit(dfp)\n",
    "\n",
    "    def fcast(h_months=H_MONTHS):\n",
    "        future = m.make_future_dataframe(periods=h_months, freq='MS')\n",
    "        pred = m.predict(future)\n",
    "        pred = pred.set_index(pd.PeriodIndex(pred['ds'], freq='M'))['yhat']\n",
    "        pred = pred.iloc[-h_months:]\n",
    "        cap = compute_dynamic_cap(ts_m)\n",
    "        vals = expm1_safe(pred.values, cap_original=cap)\n",
    "        return pd.Series(vals, index=pred.index)\n",
    "\n",
    "    return m, fcast\n",
    "\n",
    "\n",
    "def fit_arima_monthly_log(ts_m: pd.Series):\n",
    "    \"\"\"Search SARIMAX on log1p(monthly) with seasonal 12 when length permits.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    best_aic, best_model = np.inf, None\n",
    "    pqs = [0, 1, 2]\n",
    "    seasonal = len(ts_m) >= 12\n",
    "    PsQs = [0, 1] if seasonal else [0]\n",
    "\n",
    "    for p in pqs:\n",
    "        for d in ([1] if len(ts_m) < 24 else [0, 1]):\n",
    "            for q in pqs:\n",
    "                for P in PsQs:\n",
    "                    for D in ([0, 1] if seasonal else [0]):\n",
    "                        for Q in PsQs:\n",
    "                            try:\n",
    "                                model = SARIMAX(\n",
    "                                    y, order=(p, d, q),\n",
    "                                    seasonal_order=(P, D, Q, 12 if seasonal else 0),\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False\n",
    "                                ).fit(disp=False)\n",
    "                                if model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_model = model\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "    def fcast(h_months=H_MONTHS):\n",
    "        fc_log = best_model.get_forecast(h_months).predicted_mean\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        cap = compute_dynamic_cap(ts_m)\n",
    "        fc = expm1_safe(fc_log, cap_original=cap)\n",
    "        return pd.Series(fc, index=idx)\n",
    "\n",
    "    return best_model, fcast\n",
    "\n",
    "\n",
    "def fit_tbats_or_ets_monthly_log(ts_m: pd.Series):\n",
    "    \"\"\"\n",
    "    Fit TBATS (if available) on log1p(series); else ETS on log1p.\n",
    "    Return forecasts on the original scale via expm1_safe.\n",
    "    \"\"\"\n",
    "    y_log = np.log1p(ts_m)\n",
    "\n",
    "    if TBATS is not None and len(ts_m) >= 12:\n",
    "        # TBATS expects a DatetimeIndex\n",
    "        y_log_ts = pd.Series(y_log.values, index=ts_m.index.to_timestamp())\n",
    "        estimator = TBATS(use_arma_errors=False, seasonal_periods=[12])\n",
    "        model = estimator.fit(y_log_ts)\n",
    "\n",
    "        def fcast(h_months=H_MONTHS):\n",
    "            vals_log = model.forecast(steps=h_months)  # log1p scale\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            cap = compute_dynamic_cap(ts_m)\n",
    "            vals = expm1_safe(vals_log, cap_original=cap)\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "    else:\n",
    "        seasonal = 12 if len(ts_m) >= 24 else None\n",
    "        model = ExponentialSmoothing(y_log, trend='add',\n",
    "                                     seasonal=('add' if seasonal else None),\n",
    "                                     seasonal_periods=seasonal).fit()\n",
    "\n",
    "        def fcast(h_months=H_MONTHS):\n",
    "            vals_log = model.forecast(h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            cap = compute_dynamic_cap(ts_m)\n",
    "            vals = expm1_safe(vals_log, cap_original=cap)\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "\n",
    "def rolling_cv_monthly_adaptive(ts_m: pd.Series) -> Optional[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Adaptive rolling-origin CV with sanitation:\n",
    "    - if n >= 15 -> h=3\n",
    "    - if 9 <= n < 15 -> h=1\n",
    "    Returns mean sMAPE per model on original scale.\n",
    "    Non-finite predictions in a fold are penalized with 200 (worst-case).\n",
    "    \"\"\"\n",
    "    n = len(ts_m)\n",
    "    if n < 9:\n",
    "        return None\n",
    "    h = 3 if n >= 15 else 1\n",
    "    min_train = max(12, n - (h + 2))  # ensure at least one split\n",
    "    splits = []\n",
    "\n",
    "    for start in range(min_train, n - h + 1):\n",
    "        train = ts_m.iloc[:start]\n",
    "        test = ts_m.iloc[start:start + h]\n",
    "        metrics = {}\n",
    "\n",
    "        # Prophet\n",
    "        mp, fp = fit_prophet_monthly_log(train)\n",
    "        if fp is not None:\n",
    "            try:\n",
    "                pred = fp(h_months=h)\n",
    "                pv = np.array(pred.values[:h], dtype=float)\n",
    "                pv[~np.isfinite(pv)] = np.nan\n",
    "                metrics['Prophet'] = 200.0 if np.isnan(pv).all() else smape(test.values, np.nan_to_num(pv, nan=0.0))\n",
    "            except Exception:\n",
    "                metrics['Prophet'] = 200.0\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            ma, fa = fit_arima_monthly_log(train)\n",
    "            pred = fa(h_months=h)\n",
    "            pv = np.array(pred.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else smape(test.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            metrics['ARIMA'] = 200.0\n",
    "\n",
    "        # TBATS/ETS\n",
    "        try:\n",
    "            mt, ft = fit_tbats_or_ets_monthly_log(train)\n",
    "            pred = ft(h_months=h)\n",
    "            pv = np.array(pred.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else smape(test.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            metrics['TBATS/ETS'] = 200.0\n",
    "\n",
    "        splits.append(metrics)\n",
    "\n",
    "    dfm = pd.DataFrame(splits)\n",
    "    return dfm.mean().to_dict()\n",
    "\n",
    "\n",
    "def select_or_blend_forecasts(fc_dict: Dict[str, pd.Series], cv_scores: Dict[str, float], blend: bool = True):\n",
    "    \"\"\"\n",
    "    Given forecasts per model and CV scores (lower better):\n",
    "    - select best model or\n",
    "    - blend with weights ~ 1/sMAPE.\n",
    "    \"\"\"\n",
    "    scores = {k: (v if v is not None and np.isfinite(v) else 1e6) for k, v in cv_scores.items()}\n",
    "    if not blend:\n",
    "        best = min(scores, key=scores.get)\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "\n",
    "    inv = {k: (1.0 / v if v > 0 else 0.0) for k, v in scores.items()}\n",
    "    total = sum(inv.values())\n",
    "    if total == 0:\n",
    "        best = min(scores, key=scores.get)\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "    w = {k: inv[k] / total for k in inv}\n",
    "\n",
    "    # Align indices\n",
    "    idx = None\n",
    "    for s in fc_dict.values():\n",
    "        idx = s.index if idx is None else idx.union(s.index)\n",
    "\n",
    "    blended = sum(w[k] * fc_dict[k].reindex(idx).fillna(0) for k in fc_dict)\n",
    "    return blended, {'winner': min(scores, key=scores.get), 'weights': w}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959ca33",
   "metadata": {},
   "source": [
    "## 05. Monthly pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a692d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_monthly_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate daily incoming to monthly by department.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['month'] = df['Date'].dt.to_period('M')\n",
    "    monthly = (df.groupby(['department_id', 'month'], as_index=False)['ticket_total']\n",
    "               .sum()\n",
    "               .rename(columns={'ticket_total': 'incoming_monthly'}))\n",
    "    return monthly\n",
    "\n",
    "\n",
    "def forecast_per_department_monthly(monthly: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hybrid modelling per department (log-scale) + adaptive CV + robust sanitation.\n",
    "    Returns columns:\n",
    "    department_id, month, forecast_monthly,\n",
    "    cv_prophet_smape, cv_arima_smape, cv_tbats_ets_smape,\n",
    "    winner_model, blend_prophet_w, blend_arima_w, blend_tbats_ets_w\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    dept_ids = monthly['department_id'].unique().tolist()\n",
    "\n",
    "    for dept in dept_ids:\n",
    "        ts = (monthly.loc[monthly['department_id'] == dept, ['month', 'incoming_monthly']]\n",
    "              .sort_values('month')\n",
    "              .set_index('month')['incoming_monthly'])\n",
    "        if not pd.api.types.is_period_dtype(ts.index):\n",
    "            ts.index = pd.PeriodIndex(ts.index, freq='M')\n",
    "        if len(ts) == 0:\n",
    "            continue\n",
    "\n",
    "        # Light winsorization to reduce extreme outliers before modelling\n",
    "        ts = winsorize_monthly(ts, 0.01, 0.99)\n",
    "\n",
    "        cv = {}\n",
    "        fc_dict: Dict[str, pd.Series] = {}\n",
    "\n",
    "        # Prophet (log)\n",
    "        if Prophet is not None and len(ts) >= 12:\n",
    "            try:\n",
    "                _, fp = fit_prophet_monthly_log(ts)\n",
    "                if fp is not None:\n",
    "                    fc_dict['Prophet'] = fp(H_MONTHS)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # ARIMA (log)\n",
    "        try:\n",
    "            _, fa = fit_arima_monthly_log(ts)\n",
    "            fc_dict['ARIMA'] = fa(H_MONTHS)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # TBATS/ETS (log)\n",
    "        try:\n",
    "            _, ft = fit_tbats_or_ets_monthly_log(ts)\n",
    "            fc_dict['TBATS/ETS'] = ft(H_MONTHS)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not fc_dict:\n",
    "            # Fallback: naive mean\n",
    "            idx = pd.period_range(ts.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "            val = max(0.0, float(ts.mean()))\n",
    "            fc_dict['NaiveMean'] = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # CV metrics\n",
    "        try:\n",
    "            cv = rolling_cv_monthly_adaptive(ts) or {}\n",
    "        except Exception:\n",
    "            cv = {}\n",
    "\n",
    "        # Blend/select\n",
    "        if not cv:\n",
    "            preferred = ['ARIMA', 'TBATS/ETS', 'Prophet', 'NaiveMean']\n",
    "            winner = next((k for k in preferred if k in fc_dict), list(fc_dict.keys())[0])\n",
    "            blended = fc_dict[winner]\n",
    "            meta = {'winner': winner, 'weights': {winner: 1.0}}\n",
    "        else:\n",
    "            blended, meta = select_or_blend_forecasts(fc_dict, cv_scores=cv, blend=True)\n",
    "\n",
    "        # Enforce finiteness on blended series\n",
    "        if not np.isfinite(blended.values).all():\n",
    "            finite_mask = np.isfinite(blended.values)\n",
    "            if finite_mask.any():\n",
    "                finite_mean = float(np.nanmean(blended.values[finite_mask]))\n",
    "                vals = np.where(finite_mask, blended.values, finite_mean)\n",
    "                blended = pd.Series(vals, index=blended.index)\n",
    "            else:\n",
    "                # complete fallback\n",
    "                idx = pd.period_range(ts.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "                val = max(0.0, float(ts.mean()))\n",
    "                blended = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # Optional \"last line of defense\" cap versus extreme growth\n",
    "        ref = max(1.0, float(ts.tail(12).mean())) if len(ts) else 1.0\n",
    "        blended = blended.clip(lower=0, upper=ref * 8.0)\n",
    "\n",
    "        for per, val in blended.items():\n",
    "            out_rows.append({\n",
    "                'department_id': dept,\n",
    "                'month': per,\n",
    "                'forecast_monthly': max(0.0, float(val)),\n",
    "                'cv_prophet_smape': cv.get('Prophet', np.nan),\n",
    "                'cv_arima_smape': cv.get('ARIMA', np.nan),\n",
    "                'cv_tbats_ets_smape': cv.get('TBATS/ETS', np.nan),\n",
    "                'winner_model': meta['winner'],\n",
    "                'blend_prophet_w': (meta['weights'].get('Prophet', np.nan) if 'weights' in meta else np.nan),\n",
    "                'blend_arima_w': (meta['weights'].get('ARIMA', np.nan) if 'weights' in meta else np.nan),\n",
    "                'blend_tbats_ets_w': (meta['weights'].get('TBATS/ETS', np.nan) if 'weights' in meta else np.nan),\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows)\n",
    "    if not df_out.empty:\n",
    "        df_out['department_id'] = df_out['department_id'].astype(str)\n",
    "        if not pd.api.types.is_period_dtype(df_out['month']):\n",
    "            df_out['month'] = pd.PeriodIndex(df_out['month'], freq='M')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def compute_monthly_accuracy_with_history(monthly: pd.DataFrame,\n",
    "                                          fc_monthly: pd.DataFrame,\n",
    "                                          report_start: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build capacity_error-like table:\n",
    "    - historical months from report_start .. last_actual: Actual_Volume (Forecast = NaN)\n",
    "    - future months: Forecast (Actual_Volume = NaN)\n",
    "    \"\"\"\n",
    "    monthly = monthly.copy()\n",
    "    monthly['department_id'] = monthly['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(monthly['month']):\n",
    "        monthly['month'] = pd.PeriodIndex(monthly['month'], freq='M')\n",
    "\n",
    "    fc = fc_monthly.copy()\n",
    "    fc['department_id'] = fc['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(fc['month']):\n",
    "        fc['month'] = pd.PeriodIndex(fc['month'], freq='M')\n",
    "\n",
    "    # Range\n",
    "    start_per = pd.Period(report_start, freq='M')\n",
    "    last_actual = monthly['month'].max()\n",
    "\n",
    "    # Historical frame (Actuals)\n",
    "    hist = (monthly.loc[monthly['month'] >= start_per, ['department_id', 'month', 'incoming_monthly']]\n",
    "            .rename(columns={'incoming_monthly': 'Actual_Volume'}))\n",
    "    hist['Forecast'] = np.nan\n",
    "\n",
    "    # Future frame (Forecasts)\n",
    "    fut = fc[['department_id', 'month', 'forecast_monthly',\n",
    "              'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "              'winner_model', 'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']].copy()\n",
    "    fut = fut.loc[fut['month'] > last_actual]\n",
    "    fut = fut.rename(columns={'forecast_monthly': 'Forecast'})\n",
    "    fut['Actual_Volume'] = np.nan\n",
    "\n",
    "    # Union\n",
    "    base = pd.concat([hist, fut], ignore_index=True, sort=False)\n",
    "\n",
    "    # Accuracy (only when both are available)\n",
    "    base['Forecast_Accuracy'] = np.where(\n",
    "        (base['Actual_Volume'].notna()) & (base['Forecast'].notna()) & (base['Actual_Volume'] > 0),\n",
    "        (1 - (np.abs(base['Forecast'] - base['Actual_Volume']) / base['Actual_Volume'])) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    return base\n",
    "\n",
    "\n",
    "def compute_capacity_monthly(cap_df: pd.DataFrame, prod_dept: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute FTE/day needed per month = Forecast / (avg_tickets_per_agent_day * workdays_in_month).\"\"\"\n",
    "    out = cap_df.merge(prod_dept, on='department_id', how='left')\n",
    "    out['avg_tickets_per_agent_day'] = pd.to_numeric(out['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    out['avg_tickets_per_agent_day'] = out['avg_tickets_per_agent_day'].replace(0, np.nan)\n",
    "    out['workdays_in_month'] = [business_days_in_month(m.start_time.year, m.start_time.month) for m in out['month']]\n",
    "    out['Capacity_FTE_per_day'] = np.where(\n",
    "        (out['avg_tickets_per_agent_day'] > 0) & (out['workdays_in_month'] > 0) & (out['Forecast'].notna()),\n",
    "        out['Forecast'] / (out['avg_tickets_per_agent_day'] * out['workdays_in_month']),\n",
    "        np.nan\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_cv_table(fc_monthly: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build mape_table_cv with sMAPE, best model and weights per department.\"\"\"\n",
    "    if fc_monthly is None or fc_monthly.empty:\n",
    "        raise ValueError(\"fc_monthly is empty; cannot build CV table.\")\n",
    "    cols_keep = [\n",
    "        'department_id',\n",
    "        'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "        'winner_model',\n",
    "        'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w'\n",
    "    ]\n",
    "    df = (fc_monthly[cols_keep]\n",
    "          .drop_duplicates(subset=['department_id'])\n",
    "          .copy())\n",
    "    df = df.rename(columns={\n",
    "        'cv_prophet_smape': 'sMAPE_Prophet_CV',\n",
    "        'cv_arima_smape': 'sMAPE_ARIMA_CV',\n",
    "        'cv_tbats_ets_smape': 'sMAPE_TBATS_ETS_CV',\n",
    "        'winner_model': 'Best_Model',\n",
    "        'blend_prophet_w': 'Weight_Prophet',\n",
    "        'blend_arima_w': 'Weight_ARIMA',\n",
    "        'blend_tbats_ets_w': 'Weight_TBATS_ETS',\n",
    "    })\n",
    "    df['department_id'] = df['department_id'].astype(str)\n",
    "    df = apply_mapping(df, mapping)\n",
    "    ordered_cols = [\n",
    "        'department_id', 'department_name', 'vertical',\n",
    "        'sMAPE_Prophet_CV', 'sMAPE_ARIMA_CV', 'sMAPE_TBATS_ETS_CV',\n",
    "        'Best_Model',\n",
    "        'Weight_Prophet', 'Weight_ARIMA', 'Weight_TBATS_ETS'\n",
    "    ]\n",
    "    df = df[ordered_cols]\n",
    "    return df.sort_values(['vertical', 'department_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b7257",
   "metadata": {},
   "source": [
    "## 06. Daily plan (reconciled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a357a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dow_profile(g: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Build normalized day-of-week profile for a department, fallback to uniform.\"\"\"\n",
    "    prof = (g.assign(dow=g['Date'].dt.dayofweek)\n",
    "              .groupby('dow')['ticket_total']\n",
    "              .mean())\n",
    "    if prof.notna().sum() >= 3:\n",
    "        prof = prof / prof.mean()\n",
    "    else:\n",
    "        prof = pd.Series(1.0, index=range(7))\n",
    "    return prof\n",
    "\n",
    "\n",
    "def disaggregate_month_to_days(dept_df: pd.DataFrame,\n",
    "                               month_period: pd.Period,\n",
    "                               target_sum: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Allocate monthly forecast to each day in that month using recent DOW profile,\n",
    "    guaranteeing that daily sum equals the monthly target (within rounding).\n",
    "    \"\"\"\n",
    "    start = month_period.start_time\n",
    "    end = month_period.end_time\n",
    "    days = pd.date_range(start=start, end=end, freq='D')\n",
    "\n",
    "    # Build recent DOW profile from last 90 actual days\n",
    "    hist = dept_df.sort_values('Date').tail(90)\n",
    "    profile = dow_profile(hist)\n",
    "\n",
    "    weights = np.array([profile.get(d.dayofweek, 1.0) for d in days], dtype=float)\n",
    "    weights = np.maximum(weights, 1e-6)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    alloc = target_sum * weights\n",
    "    return pd.DataFrame({'Date': days, 'forecast_daily': alloc})\n",
    "\n",
    "\n",
    "def build_daily_from_monthly(incoming: pd.DataFrame,\n",
    "                             fc_monthly: pd.DataFrame,\n",
    "                             horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Top-down daily plan:\n",
    "    - For each department and future month within horizon window,\n",
    "      disaggregate monthly forecast into daily using DOW profile.\n",
    "    \"\"\"\n",
    "    last_date = incoming['Date'].max()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    end = start + pd.Timedelta(days=horizon_days - 1)\n",
    "    future_months = pd.period_range(start=start.to_period('M'),\n",
    "                                    end=end.to_period('M'), freq='M')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in incoming.groupby('department_id'):\n",
    "        for m in future_months:\n",
    "            fcm = fc_monthly[(fc_monthly['department_id'] == dept) & (fc_monthly['month'] == m)]\n",
    "            if fcm.empty:\n",
    "                continue\n",
    "            target = float(fcm['forecast_monthly'].iloc[0])\n",
    "            if target <= 0:\n",
    "                continue\n",
    "            alloc_df = disaggregate_month_to_days(g, m, target)\n",
    "            alloc_df = alloc_df[(alloc_df['Date'] >= start) & (alloc_df['Date'] <= end)]\n",
    "            alloc_df.insert(0, 'department_id', dept)\n",
    "            rows.append(alloc_df)\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_daily_by_language(df_daily_fc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Split daily forecast by fixed language shares.\"\"\"\n",
    "    parts = []\n",
    "    for lang, w in LANGUAGE_SHARES.items():\n",
    "        tmp = df_daily_fc.copy()\n",
    "        tmp['language'] = lang\n",
    "        tmp['forecast_daily_language'] = tmp['forecast_daily'] * w\n",
    "        parts.append(tmp)\n",
    "    out = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    return out\n",
    "\n",
    "\n",
    "def forecast_daily_baseline(df_daily: pd.DataFrame, horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Independent daily baseline (kept as optional):\n",
    "    - If >=28 days: 28-day moving average per department with DOW profile\n",
    "    - Else: historical average\n",
    "    \"\"\"\n",
    "    df = df_daily.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df = df.sort_values(['department_id', 'Date'])\n",
    "    last_date = df['Date'].max()\n",
    "    if pd.isna(last_date):\n",
    "        raise ValueError(\"forecast_daily_baseline: No valid dates in incoming.\")\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    idx_future = pd.date_range(start=start, periods=horizon_days, freq='D')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in df.groupby('department_id'):\n",
    "        g = g.sort_values('Date')\n",
    "        if len(g) >= 28:\n",
    "            roll_mean = (g.set_index('Date')['ticket_total']\n",
    "                         .rolling(window=28, min_periods=1)\n",
    "                         .mean()\n",
    "                         .iloc[-1])\n",
    "            base = float(roll_mean) if np.isfinite(roll_mean) else float(g['ticket_total'].mean())\n",
    "        else:\n",
    "            base = float(g['ticket_total'].mean())\n",
    "\n",
    "        prof = dow_profile(g)\n",
    "        vals = []\n",
    "        for d in idx_future:\n",
    "            w = prof[d.dayofweek] if d.dayofweek in prof.index else 1.0\n",
    "            vals.append(max(0.0, base * float(w)))\n",
    "        rows.append(pd.DataFrame({'department_id': dept, 'Date': idx_future, 'forecast_daily': vals}))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "\n",
    "\n",
    "def build_daily_capacity_plan(incoming: pd.DataFrame,\n",
    "                              mapping: pd.DataFrame,\n",
    "                              prod_dept: pd.DataFrame,\n",
    "                              fc_monthly: pd.DataFrame,\n",
    "                              horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    End-to-end daily plan:\n",
    "    - If USE_DAILY_FROM_MONTHLY: disaggregate monthly forecast (reconciled)\n",
    "    - Else: robust daily baseline (28D moving average)\n",
    "    - Split by languages\n",
    "    - Attach department_name / vertical\n",
    "    - Compute FTE per day per department/language\n",
    "    \"\"\"\n",
    "    if USE_DAILY_FROM_MONTHLY:\n",
    "        daily_fc = build_daily_from_monthly(incoming, fc_monthly, horizon_days)\n",
    "    else:\n",
    "        daily_fc = forecast_daily_baseline(incoming, horizon_days)\n",
    "\n",
    "    daily_fc_lang = split_daily_by_language(daily_fc)\n",
    "\n",
    "    # Attach names/vertical\n",
    "    daily_fc_lang = apply_mapping(daily_fc_lang, mapping)\n",
    "\n",
    "    # Merge productivity\n",
    "    daily_fc_lang = daily_fc_lang.merge(prod_dept, on='department_id', how='left')\n",
    "\n",
    "    # Compute FTE requirement per day\n",
    "    daily_fc_lang['avg_tickets_per_agent_day'] = pd.to_numeric(daily_fc_lang['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    daily_fc_lang['FTE_per_day'] = np.where(\n",
    "        daily_fc_lang['avg_tickets_per_agent_day'] > 0,\n",
    "        daily_fc_lang['forecast_daily_language'] / daily_fc_lang['avg_tickets_per_agent_day'],\n",
    "        np.nan\n",
    "    )\n",
    "    cols = ['Date', 'department_id', 'department_name', 'vertical', 'language',\n",
    "            'forecast_daily_language', 'FTE_per_day']\n",
    "    daily_plan = daily_fc_lang[cols].sort_values(['Date', 'vertical', 'department_id', 'language'])\n",
    "    return daily_plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b39785",
   "metadata": {},
   "source": [
    "## 06. Main entry point and excel writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbaa078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:34:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:34:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:35:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:35:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:36:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:36:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:37:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:37:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:38:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:38:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:39:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:39:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:41:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:41:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:42:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:42:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:43:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:43:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:44:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:44:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:45:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:45:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:46:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:46:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:48:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:48:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:49:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:49:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:50:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:50:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:51:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:51:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:52:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:52:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:53:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:53:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:54:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:54:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:55:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:55:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:56:47 - cmdstanpy - ERROR - Chain [1] error: terminated by signal 3221226228 \n",
      "17:56:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:56:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:57:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:57:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:57:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:57:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:58:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:58:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:58:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:59:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:59:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:59:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:00:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:00:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:02:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:02:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:02:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:03:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:03:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:03:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:04:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:04:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:04:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:05:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:07:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:07:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:09:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:09:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:10:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:10:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:11:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:11:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:11:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:11:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:12:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:12:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:12:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:12:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) Load inputs\n",
    "    incoming = load_incoming(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET)\n",
    "    mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "    prod = load_productivity(PRODUCTIVITY_PATH)\n",
    "\n",
    "    # 2) Monthly forecast\n",
    "    monthly = build_monthly_series(incoming)\n",
    "    fc_monthly = forecast_per_department_monthly(monthly)\n",
    "\n",
    "    # 3) capacity_error (historicals + future forecast)\n",
    "    cap_err = compute_monthly_accuracy_with_history(monthly, fc_monthly, REPORT_START_MONTH)\n",
    "    cap_err = compute_capacity_monthly(cap_err, prod)\n",
    "    cap_err = apply_mapping(cap_err, mapping)\n",
    "\n",
    "    # 4) Daily plan (reconciled with monthly by default)\n",
    "    daily_capacity_plan = build_daily_capacity_plan(incoming, mapping, prod, fc_monthly, DAILY_HORIZON_DAYS)\n",
    "\n",
    "    # 5) CV table\n",
    "    cv_table = build_cv_table(fc_monthly, mapping)\n",
    "\n",
    "    # 6) Ensure no inf/-inf propagate to Excel\n",
    "    for df_out in [cap_err, daily_capacity_plan, cv_table]:\n",
    "        df_out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 7) Write Excel with required sheet names\n",
    "    with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        (cap_err[['vertical', 'department_id', 'department_name', 'month',\n",
    "                  'Actual_Volume', 'Forecast', 'Forecast_Accuracy',\n",
    "                  'Capacity_FTE_per_day',\n",
    "                  'winner_model', 'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "                  'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']]\n",
    "         .sort_values(['vertical', 'department_id', 'month'])\n",
    "         .to_excel(w, \"capacity_error\", index=False))\n",
    "\n",
    "        daily_capacity_plan.to_excel(w, \"daily_capacity_plan\", index=False)\n",
    "        cv_table.to_excel(w, \"mape_table_cv\", index=False)\n",
    "\n",
    "    print(\"Excel written:\", OUTPUT_XLSX)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
