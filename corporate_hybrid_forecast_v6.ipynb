{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f4824e",
   "metadata": {},
   "source": [
    "# Corporate Hybrid Forecast Notebook ((Prophet vs ARIMA vs TBATS/ETS)) – v6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1931be",
   "metadata": {},
   "source": [
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Capacity Forecast - Hybrid (Prophet / ARIMA / TBATS-ETS) - v6.1\n",
    "\n",
    "Hotfix:\n",
    "- Hardened 'forecast_per_department_monthly' to always include CV/weights columns.\n",
    "- Hardened 'build_stability_report' to add missing CV/weights columns on the fly (avoids KeyError).\n",
    "\n",
    "Keeps v6 features:\n",
    "- Auto Christmas CSV with Xmas days counts (2024-2027)\n",
    "- Exogenous features from:\n",
    "   * case_reason.xlsx ('Global outage reported' proxy per dept)\n",
    "   * christmas_holidays_*.csv (xmas days per month)\n",
    "- Rate-per-workday modelling, wMAPE blending, robust smoothing (MAD), bias correction\n",
    "- Monthly->Daily reconciliation\n",
    "- Sheets: capacity_error, daily_capacity_plan, mape_table_cv, stability_report\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from typing import Optional, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    Prophet = None\n",
    "try:\n",
    "    from tbats import TBATS\n",
    "except Exception:\n",
    "    TBATS = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "\n",
    "# Inputs (adjust if your file locations change)\n",
    "INCOMING_SOURCE_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\Incoming_new.xlsx\"  # Sheet 'Main'\n",
    "INCOMING_SHEET = \"Main\"\n",
    "DEPT_MAP_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\department.xlsx\"\n",
    "DEPT_MAP_SHEET = \"map\"\n",
    "PRODUCTIVITY_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\productivity_agents.xlsx\"\n",
    "\n",
    "# Outage proxy (case reasons)\n",
    "CASE_REASON_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\case_reason.xlsx\"\n",
    "CASE_REASON_SHEET = \"Main\"             # provided by you\n",
    "CASE_REASON_FILTER = \"Global outage reported\"\n",
    "\n",
    "# Christmas holidays CSV\n",
    "HOLIDAYS_CSV_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\christmas_holidays_2024_2027.csv\"\n",
    "HOLIDAYS_YEARS = [2024, 2025, 2026, 2027]\n",
    "INCLUDE_JAN6 = True                    # include Jan 6 (common in ES/PT/IT)\n",
    "INCLUDE_JAN_POSTXMAS = False           # monthly extra dummy for January (off by default)\n",
    "\n",
    "# Output\n",
    "OUTPUT_XLSX = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\"\n",
    "\n",
    "# Horizons and switches\n",
    "H_MONTHS = 12             # monthly forecast horizon\n",
    "DAILY_HORIZON_DAYS = 90   # daily plan horizon\n",
    "REPORT_START_MONTH = \"2025-01\"  # show historical Actuals from this month in capacity_error\n",
    "\n",
    "# Top-down reconciliation for daily forecasts\n",
    "USE_DAILY_FROM_MONTHLY = True\n",
    "\n",
    "# Optional final growth guard (disabled by default; enable if needed)\n",
    "\n",
    "APPLY_LOCAL_GROWTH_GUARD = True\n",
    "MAX_GROWTH = 1.6   # 60% of mean of historical month\n",
    "MIN_GROWTH = 0.7   # Does not allow to fall more than 30%\n",
    "\n",
    "\n",
    "# Language shares\n",
    "LANGUAGE_SHARES = {\n",
    "    'English': 0.6435,\n",
    "    'French': 0.0741,\n",
    "    'German': 0.0860,\n",
    "    'Italian': 0.0667,\n",
    "    'Portuguese': 0.0162,\n",
    "    'Spanish': 0.1135\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aacc15",
   "metadata": {},
   "source": [
    "## 2. Load & Clean Data (2023–Current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee2ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_incoming(path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load daily incoming volumes. Build ticket_total if needed.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Incoming file not found:\\n{path}\\n\")\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xlsm\", \".xls\"]:\n",
    "        if not sheet_name:\n",
    "            raise ValueError(\"Excel file detected but no sheet_name provided (e.g., 'Main').\")\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension for incoming data: {ext}\")\n",
    "\n",
    "    base_required = {'Date', 'department_id'}\n",
    "    missing = base_required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Incoming file must contain {sorted(list(base_required))}. Found: {list(df.columns)}\")\n",
    "\n",
    "    if 'ticket_total' not in df.columns:\n",
    "        if 'total_incoming' in df.columns:\n",
    "            df['ticket_total'] = pd.to_numeric(df['total_incoming'], errors='coerce').fillna(0)\n",
    "        elif {'incoming_from_customers', 'incoming_from_transfers'}.issubset(df.columns):\n",
    "            df['ticket_total'] = (\n",
    "                pd.to_numeric(df['incoming_from_customers'], errors='coerce').fillna(0) +\n",
    "                pd.to_numeric(df['incoming_from_transfers'], errors='coerce').fillna(0)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Missing 'ticket_total' or components to create it.\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if df['Date'].isna().any():\n",
    "        bad = df.loc[df['Date'].isna()]\n",
    "        raise ValueError(f\"Some Date values could not be parsed. Example rows:\\n{bad.head(5)}\")\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['ticket_total'] = pd.to_numeric(df['ticket_total'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    if 'department_name' in df.columns:\n",
    "        df['department_name'] = df['department_name'].astype(str).str.strip()\n",
    "    else:\n",
    "        df['department_name'] = None\n",
    "    if 'vertical' in df.columns:\n",
    "        df['vertical'] = df['vertical'].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dept_map(path: str, sheet: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Load dept mapping -> department_name, vertical.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['department_id', 'department_name', 'vertical'])\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xls\"):\n",
    "        if sheet:\n",
    "            mp = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        else:\n",
    "            xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "            mp = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "    else:\n",
    "        mp = pd.read_csv(path)\n",
    "\n",
    "    rename_map = {\n",
    "        'dept_id': 'department_id',\n",
    "        'dept_name': 'department_name',\n",
    "        'name': 'department_name',\n",
    "        'segment': 'vertical',\n",
    "        'vertical_name': 'vertical'\n",
    "    }\n",
    "    mp = mp.rename(columns={k: v for k, v in rename_map.items() if k in mp.columns})\n",
    "    if 'department_id' not in mp.columns:\n",
    "        raise ValueError(f\"Department map must contain 'department_id'. Found: {list(mp.columns)}\")\n",
    "\n",
    "    mp['department_id'] = mp['department_id'].astype(str).str.strip()\n",
    "    mp['department_name'] = (mp['department_name'].astype(str).str.strip()\n",
    "                             if 'department_name' in mp.columns else None)\n",
    "    mp['vertical'] = (mp['vertical'].astype(str).str.strip()\n",
    "                      if 'vertical' in mp.columns else None)\n",
    "\n",
    "    return mp[['department_id', 'department_name', 'vertical']].drop_duplicates('department_id')\n",
    "\n",
    "\n",
    "def load_productivity(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load agent productivity and compute dept-level mean tickets/agent-day.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Productivity file not found: {path}\")\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    req = {'Date', 'agent_id', 'department_id', 'prod_total_model'}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"productivity_agents.xlsx missing columns: {sorted(list(missing))}. Found: {list(df.columns)}\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['prod_total_model'] = pd.to_numeric(df['prod_total_model'], errors='coerce')\n",
    "\n",
    "    prod_dept = (df.groupby('department_id', as_index=False)['prod_total_model']\n",
    "                 .mean()\n",
    "                 .rename(columns={'prod_total_model': 'avg_tickets_per_agent_day'}))\n",
    "    return prod_dept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689be492",
   "metadata": {},
   "source": [
    "## 3. helpers, metrics and exogenous feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_days_in_month(year: int, month: int) -> int:\n",
    "    \"\"\"Approximate Mon-Fri working days in a month.\"\"\"\n",
    "    rng = pd.date_range(start=pd.Timestamp(year=year, month=month, day=1),\n",
    "                        end=pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(0),\n",
    "                        freq='D')\n",
    "    return int(np.sum(rng.weekday < 5))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred) -> float:\n",
    "    \"\"\"sMAPE robust for intermittent series.\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom[denom == 0] = 1.0\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "\n",
    "def wmape(y_true, y_pred) -> float:\n",
    "    \"\"\"Weighted MAPE: sum(|e|)/sum(|y|).\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    if denom <= 0:\n",
    "        return 200.0\n",
    "    return float(100.0 * (np.sum(np.abs(y_true - y_pred)) / denom))\n",
    "\n",
    "\n",
    "def apply_mapping(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge department_name / vertical using department_id.\"\"\"\n",
    "    merged = incoming.merge(mapping, on='department_id', how='left', suffixes=('', '_map'))\n",
    "    if 'department_name' not in merged.columns:\n",
    "        merged['department_name'] = None\n",
    "    if 'department_name_map' not in merged.columns:\n",
    "        merged['department_name_map'] = None\n",
    "    merged['department_name'] = merged['department_name'].fillna(merged['department_name_map']).fillna(\"Unknown\")\n",
    "\n",
    "    if 'vertical' not in merged.columns:\n",
    "        merged['vertical'] = None\n",
    "    if 'vertical_map' not in merged.columns:\n",
    "        merged['vertical_map'] = None\n",
    "    merged['vertical'] = merged['vertical'].fillna(merged['vertical_map']).fillna(\"Unmapped\")\n",
    "\n",
    "    drop_cols = [c for c in merged.columns if c.endswith('_map')]\n",
    "    merged.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    return merged\n",
    "\n",
    "\n",
    "def winsorize_monthly(ts_m: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Winsorize monthly series to reduce the influence of extreme outliers.\"\"\"\n",
    "    if ts_m.empty:\n",
    "        return ts_m\n",
    "    lo = ts_m.quantile(lower_q)\n",
    "    hi = ts_m.quantile(upper_q)\n",
    "    return ts_m.clip(lower=lo, upper=hi)\n",
    "\n",
    "# ---------- Safe inverse & dynamic cap ----------\n",
    "\n",
    "def expm1_safe(log_vals: np.ndarray, cap_original: Optional[float] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safe inverse of log1p:\n",
    "    - replace non-finite logs by a very negative number (-> ~0)\n",
    "    - lower-bound logs to avoid underflow\n",
    "    - optional cap on original scale applied in log-domain and after expm1\n",
    "    \"\"\"\n",
    "    x = np.array(log_vals, dtype=float)\n",
    "    x[~np.isfinite(x)] = -50.0\n",
    "    x = np.maximum(x, -50.0)\n",
    "\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        log_cap = np.log1p(cap_original)\n",
    "        x = np.minimum(x, log_cap)\n",
    "\n",
    "    y = np.expm1(x)\n",
    "    if cap_original is not None and np.isfinite(cap_original) and cap_original > 0:\n",
    "        y = np.minimum(y, cap_original)\n",
    "    return np.clip(y, 0, None)\n",
    "\n",
    "\n",
    "def compute_dynamic_cap(ts_m: pd.Series) -> float:\n",
    "    \"\"\"Generous per-department cap on the original scale to prevent explosions.\"\"\"\n",
    "    if ts_m.empty or (ts_m.max() <= 0):\n",
    "        return np.inf\n",
    "    m12 = float(ts_m.tail(12).mean()) if len(ts_m) >= 3 else float(ts_m.mean())\n",
    "    med = float(ts_m.median())\n",
    "    mx = float(ts_m.max())\n",
    "    base = max(1.0, m12, med, 1.1 * mx)\n",
    "    cap = base * 2.0  # adjust 2.0–6.0 as needed\n",
    "    return cap\n",
    "\n",
    "# ---------- Rate modelling, Xmas CSV and robust smoothing ----------\n",
    "\n",
    "def monthly_rate_series(ts_m: pd.Series) -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"Return (rate_per_workday, workdays series aligned to ts_m).\"\"\"\n",
    "    w = ts_m.index.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month))\n",
    "    w = w.astype(float).replace(0, np.nan)\n",
    "    rate = ts_m / w\n",
    "    return rate, w\n",
    "\n",
    "\n",
    "def robust_roll_cap(series: pd.Series, window: int = 12, K: float = 6.0) -> pd.Series:\n",
    "    \"\"\"Apply rolling Median ± K*MAD cap to stabilize spikes without flattening the series.\"\"\"\n",
    "    s = series.copy().astype(float)\n",
    "    vals = s.values\n",
    "    for i in range(len(s)):\n",
    "        lo = max(0, i - window)\n",
    "        ref = vals[lo:i] if i > 0 else []\n",
    "        if len(ref) >= 4:\n",
    "            med = np.median(ref)\n",
    "            mad = np.median(np.abs(ref - med)) + 1e-9\n",
    "            upper = med + K * mad\n",
    "            lower = max(0.0, med - K * mad)\n",
    "            vals[i] = min(max(vals[i], lower), upper)\n",
    "        else:\n",
    "            vals[i] = max(vals[i], 0.0)\n",
    "    return pd.Series(vals, index=s.index)\n",
    "\n",
    "# ==================== Christmas Holidays CSV ====================\n",
    "\n",
    "def ensure_christmas_csv(path: str = HOLIDAYS_CSV_PATH,\n",
    "                         years = HOLIDAYS_YEARS,\n",
    "                         include_jan6: bool = INCLUDE_JAN6) -> str:\n",
    "    \"\"\"Create a CSV with core Christmas holidays if it doesn't exist.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "\n",
    "    rows = []\n",
    "    for y in years:\n",
    "        rows.append((f\"{y}-12-24\", \"Christmas Eve\", 1))\n",
    "        rows.append((f\"{y}-12-25\", \"Christmas Day\", 1))\n",
    "        rows.append((f\"{y}-12-26\", \"Boxing/St. Stephen\", 1))\n",
    "        rows.append((f\"{y}-12-31\", \"New Year Eve\", 1))\n",
    "        ny = y + 1\n",
    "        rows.append((f\"{ny}-01-01\", \"New Year Day\", 1))\n",
    "        if include_jan6:\n",
    "            rows.append((f\"{ny}-01-06\", \"Epiphany\", 1))\n",
    "\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    pd.DataFrame(rows, columns=[\"date\",\"label\",\"is_xmas\"]).to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def load_christmas_csv(path: str = HOLIDAYS_CSV_PATH) -> pd.DataFrame:\n",
    "    \"\"\"Load the Christmas holidays CSV (ensure it first).\"\"\"\n",
    "    ensure_christmas_csv(path)\n",
    "    df = pd.read_csv(path)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['is_xmas'] = pd.to_numeric(df['is_xmas'], errors='coerce').fillna(0).astype(int)\n",
    "    return df[['date','label','is_xmas']]\n",
    "\n",
    "# ==================== Outage proxy (case_reason.xlsx) ====================\n",
    "\n",
    "def load_case_reason_proxy(path: str = CASE_REASON_PATH,\n",
    "                           sheet=CASE_REASON_SHEET) -> pd.DataFrame:\n",
    "    \"\"\"Load case_reason.xlsx and keep only rows that can act as outage proxy.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['Date', 'department_id', 'case_reason'])\n",
    "\n",
    "    df = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "\n",
    "    required = {'Date', 'department_id'}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"case_reason.xlsx must contain {sorted(list(required))}. \"\n",
    "                         f\"Found: {list(df.columns)}\")\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    if 'case_reason' in df.columns:\n",
    "        df['case_reason'] = df['case_reason'].astype(str).str.strip()\n",
    "        if CASE_REASON_FILTER:\n",
    "            df = df[df['case_reason'].fillna('') == CASE_REASON_FILTER]\n",
    "\n",
    "    df = df.dropna(subset=['Date','department_id'])\n",
    "    return df[['Date', 'department_id', 'case_reason']].copy()\n",
    "\n",
    "    # ==================== Monthly exogenous features ====================\n",
    "\n",
    "def build_monthly_exog_from_proxy_and_xmas(month_index: pd.PeriodIndex,\n",
    "                                           department_id: str,\n",
    "                                           case_reason_df: pd.DataFrame,\n",
    "                                           xmas_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build monthly exogenous features for a given department:\n",
    "      - outage_cases_z: z-scored count of 'Global outage reported' tickets per month (proxy)\n",
    "      - xmas_days_cnt_z: z-scored count of xmas days per month from the CSV\n",
    "    Returns DataFrame indexed by month.\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(index=month_index)\n",
    "\n",
    "    # ---- Outage proxy aggregation (per department) ----\n",
    "    if case_reason_df is not None and not case_reason_df.empty:\n",
    "        tmp = case_reason_df.copy()\n",
    "        # ✅ fix: cast department_id correctly (no keyword arg in str())\n",
    "        tmp = tmp[tmp['department_id'] == str(department_id)]\n",
    "        tmp['month'] = tmp['Date'].dt.to_period('M')\n",
    "        cnt = (tmp.groupby('month', as_index=False)\n",
    "                  .size()\n",
    "                  .rename(columns={'size': 'outage_cases'})\n",
    "                  .set_index('month')\n",
    "                  .reindex(month_index)\n",
    "                  .fillna(0.0))\n",
    "        mu = float(cnt['outage_cases'].mean())\n",
    "        sd = float(cnt['outage_cases'].std(ddof=0)) + 1e-6\n",
    "        X['outage_cases_z'] = (cnt['outage_cases'] - mu) / sd\n",
    "    else:\n",
    "        X['outage_cases_z'] = 0.0\n",
    "\n",
    "    # ---- Xmas days per month (from CSV) ----\n",
    "    if xmas_df is not None and not xmas_df.empty:\n",
    "        mm = xmas_df.copy()\n",
    "        mm = mm[mm['is_xmas'] == 1]\n",
    "        mm['month'] = mm['date'].dt.to_period('M')\n",
    "        cntx = (mm.groupby('month', as_index=False)\n",
    "                  .size()\n",
    "                  .rename(columns={'size': 'xmas_days_cnt'})\n",
    "                  .set_index('month')\n",
    "                  .reindex(month_index)\n",
    "                  .fillna(0.0))\n",
    "        mux = float(cntx['xmas_days_cnt'].mean())\n",
    "        sdx = float(cntx['xmas_days_cnt'].std(ddof=0)) + 1e-6\n",
    "        X['xmas_days_cnt_z'] = (cntx['xmas_days_cnt'] - mux) / sdx\n",
    "    else:\n",
    "        X['xmas_days_cnt_z'] = 0.0\n",
    "\n",
    "    # ---- Optional January post-Christmas dummy ----\n",
    "    X['jan_postxmas'] = [1.0 if (INCLUDE_JAN_POSTXMAS and p.start_time.month == 1) else 0.0\n",
    "                         for p in month_index]\n",
    "\n",
    "    return X.fillna(0.0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97dba5",
   "metadata": {},
   "source": [
    "## 4. Monthly modelling (log-scale, rate-aware, with exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6a8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_prophet_monthly_log(ts_m: pd.Series, is_rate: bool = False,\n",
    "                            exog_train: Optional[pd.DataFrame] = None):\n",
    "    \"\"\"Fit Prophet on log1p(ts_m) with optional exogenous regressors.\"\"\"\n",
    "    if Prophet is None:\n",
    "        return None, None\n",
    "\n",
    "    y = np.log1p(ts_m.values)\n",
    "    dfp = pd.DataFrame({'ds': ts_m.index.to_timestamp(), 'y': y})\n",
    "    m = Prophet(weekly_seasonality=False, yearly_seasonality=True, daily_seasonality=False)\n",
    "\n",
    "    exog_cols = []\n",
    "    if exog_train is not None and not exog_train.empty:\n",
    "        ex_al = exog_train.reindex(ts_m.index).fillna(0.0)\n",
    "        for c in ex_al.columns:\n",
    "            m.add_regressor(c, standardize=True)\n",
    "            dfp[c] = ex_al[c].values\n",
    "            exog_cols.append(c)\n",
    "\n",
    "    m.fit(dfp)\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None,\n",
    "              exog_future: Optional[pd.DataFrame] = None):\n",
    "        future = m.make_future_dataframe(periods=h_months, freq='MS')\n",
    "        if exog_future is not None and not exog_future.empty:\n",
    "            exf = exog_future.copy()\n",
    "            exf = exf.reindex(pd.PeriodIndex(future['ds'], freq='M')).fillna(0.0)\n",
    "            for c in exog_cols:\n",
    "                future[c] = exf[c].values\n",
    "\n",
    "        pred = m.predict(future)\n",
    "        pred = pred.set_index(pd.PeriodIndex(pred['ds'], freq='M'))['yhat'].iloc[-h_months:]\n",
    "\n",
    "        vals = expm1_safe(pred.values, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=pred.index)\n",
    "\n",
    "    return m, fcast\n",
    "\n",
    "\n",
    "def fit_arima_monthly_log(ts_m: pd.Series, is_rate: bool = False,\n",
    "                          exog_train: Optional[pd.DataFrame] = None):\n",
    "    \"\"\"SARIMAX on log1p(ts_m) with a conservative grid and exogenous support.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    X = None\n",
    "    if exog_train is not None and not exog_train.empty:\n",
    "        X = exog_train.reindex(ts_m.index).fillna(0.0).values\n",
    "\n",
    "    best_aic, best_model = np.inf, None\n",
    "    pqs = [0, 1]\n",
    "    seasonal = len(ts_m) >= 12\n",
    "    PsQs = [0, 1] if seasonal else [0]\n",
    "\n",
    "    for p in pqs:\n",
    "        for d in ([1] if len(ts_m) < 36 else [0, 1]):\n",
    "            for q in pqs:\n",
    "                for P in PsQs:\n",
    "                    for D in ([0, 1] if seasonal else [0]):\n",
    "                        for Q in PsQs:\n",
    "                            try:\n",
    "                                model = SARIMAX(\n",
    "                                    y, order=(p, d, q),\n",
    "                                    seasonal_order=(P, D, Q, 12 if seasonal else 0),\n",
    "                                    exog=X,\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False\n",
    "                                ).fit(disp=False)\n",
    "                                if model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_model = model\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None,\n",
    "              exog_future: Optional[pd.DataFrame] = None):\n",
    "        Xf = None\n",
    "        if exog_future is not None and not exog_future.empty:\n",
    "            Xf = exog_future.iloc[:h_months].fillna(0.0).values\n",
    "        fc_log = best_model.get_forecast(h_months, exog=Xf).predicted_mean\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        vals = expm1_safe(fc_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=idx)\n",
    "\n",
    "    return best_model, fcast\n",
    "\n",
    "\n",
    "def fit_tbats_or_ets_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"TBATS on log1p(ts_m) if available; else ETS (log1p).\"\"\"\n",
    "    y_log = np.log1p(ts_m)\n",
    "\n",
    "    if TBATS is not None and len(ts_m) >= 12:\n",
    "        y_log_ts = pd.Series(y_log.values, index=ts_m.index.to_timestamp())\n",
    "        estimator = TBATS(use_arma_errors=False, seasonal_periods=[12])\n",
    "        model = estimator.fit(y_log_ts)\n",
    "\n",
    "        def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "            vals_log = model.forecast(steps=h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "            if is_rate and future_workdays is not None:\n",
    "                vals = vals * future_workdays.values\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "    else:\n",
    "        seasonal = 12 if len(ts_m) >= 24 else None\n",
    "        model = ExponentialSmoothing(y_log, trend='add',\n",
    "                                     seasonal=('add' if seasonal else None),\n",
    "                                     seasonal_periods=seasonal).fit()\n",
    "\n",
    "        def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "            vals_log = model.forecast(h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "            if is_rate and future_workdays is not None:\n",
    "                vals = vals * future_workdays.values\n",
    "            return pd.Series(vals, index=idx)\n",
    "\n",
    "        return model, fcast\n",
    "\n",
    "\n",
    "def fit_ets_damped_monthly_log(ts_m: pd.Series, is_rate: bool = False):\n",
    "    \"\"\"ETS with damped trend on log1p(ts_m); stable candidate for blending.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    seasonal = 12 if len(ts_m) >= 24 else None\n",
    "    model = ExponentialSmoothing(y, trend='add', damped_trend=True,\n",
    "                                 seasonal=('add' if seasonal else None),\n",
    "                                 seasonal_periods=seasonal).fit()\n",
    "\n",
    "    def fcast(h_months=H_MONTHS, future_workdays: Optional[pd.Series] = None):\n",
    "        vals_log = model.forecast(h_months)\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        vals = expm1_safe(vals_log, cap_original=None if is_rate else compute_dynamic_cap(ts_m))\n",
    "        if is_rate and future_workdays is not None:\n",
    "            vals = vals * future_workdays.values\n",
    "        return pd.Series(vals, index=idx)\n",
    "\n",
    "    return model, fcast\n",
    "\n",
    "# ==================== Adaptive CV (rate-aware) ====================\n",
    "\n",
    "def rolling_cv_monthly_adaptive_rate(ts_vol: pd.Series) -> Tuple[Optional[Dict[str, float]], Optional[Dict[str, float]]]:\n",
    "    \"\"\"Adaptive rolling-origin CV using rate modelling internally (returns sMAPE and wMAPE dicts).\"\"\"\n",
    "    n = len(ts_vol)\n",
    "    if n < 9:\n",
    "        return None, None\n",
    "    h = 3 if n >= 15 else 1\n",
    "    min_train = max(12, n - (h + 2))\n",
    "\n",
    "    s_out, w_out = [], []\n",
    "    for start in range(min_train, n - h + 1):\n",
    "        train_vol = ts_vol.iloc[:start]\n",
    "        test_vol = ts_vol.iloc[start:start + h]\n",
    "\n",
    "        train_rate, _ = monthly_rate_series(train_vol)\n",
    "        future_idx = pd.period_range(train_vol.index[-1] + 1, periods=h, freq='M')\n",
    "        future_w = future_idx.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month)).astype(float)\n",
    "\n",
    "        s_metrics, w_metrics = {}, {}\n",
    "\n",
    "        # Prophet\n",
    "        mp, fp = fit_prophet_monthly_log(train_rate, is_rate=True)\n",
    "        if fp is not None:\n",
    "            try:\n",
    "                pred_vol = fp(h_months=h, future_workdays=future_w)\n",
    "                pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "                pv[~np.isfinite(pv)] = np.nan\n",
    "                s_metrics['Prophet'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "                w_metrics['Prophet'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            except Exception:\n",
    "                s_metrics['Prophet'] = 200.0; w_metrics['Prophet'] = 200.0\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            ma, fa = fit_arima_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = fa(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['ARIMA'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['ARIMA'] = 200.0; w_metrics['ARIMA'] = 200.0\n",
    "\n",
    "        # TBATS/ETS\n",
    "        try:\n",
    "            mt, ft = fit_tbats_or_ets_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = ft(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['TBATS/ETS'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['TBATS/ETS'] = 200.0; w_metrics['TBATS/ETS'] = 200.0\n",
    "\n",
    "        # ETS Damped\n",
    "        try:\n",
    "            me, fe = fit_ets_damped_monthly_log(train_rate, is_rate=True)\n",
    "            pred_vol = fe(h_months=h, future_workdays=future_w)\n",
    "            pv = np.array(pred_vol.values[:h], dtype=float)\n",
    "            pv[~np.isfinite(pv)] = np.nan\n",
    "            s_metrics['ETS_Damped'] = 200.0 if np.isnan(pv).all() else smape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "            w_metrics['ETS_Damped'] = 200.0 if np.isnan(pv).all() else wmape(test_vol.values, np.nan_to_num(pv, nan=0.0))\n",
    "        except Exception:\n",
    "            s_metrics['ETS_Damped'] = 200.0; w_metrics['ETS_Damped'] = 200.0\n",
    "\n",
    "        s_out.append(s_metrics); w_out.append(w_metrics)\n",
    "\n",
    "    sm = pd.DataFrame(s_out).mean().to_dict()\n",
    "    wm = pd.DataFrame(w_out).mean().to_dict()\n",
    "    return sm, wm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a9d47",
   "metadata": {},
   "source": [
    "## 5. Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4845d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_or_blend_forecasts(fc_dict: Dict[str, pd.Series],\n",
    "                              cv_scores_wmape: Dict[str, float],\n",
    "                              blend: bool = True):\n",
    "    \"\"\"Blend using 1/wMAPE as weights (lower better).\"\"\"\n",
    "    scores = {k: (v if v is not None and np.isfinite(v) else 1e6) for k, v in cv_scores_wmape.items()}\n",
    "    models = [m for m in fc_dict.keys() if m in scores]\n",
    "    if not models:\n",
    "        k0 = list(fc_dict.keys())[0]\n",
    "        return fc_dict[k0], {'winner': k0, 'weights': {k0: 1.0}}\n",
    "\n",
    "    if not blend:\n",
    "        best = min(models, key=lambda m: scores[m])\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "\n",
    "    inv = {m: (1.0 / scores[m] if scores[m] > 0 else 0.0) for m in models}\n",
    "    total = sum(inv.values())\n",
    "    if total == 0:\n",
    "        best = min(models, key=lambda m: scores[m])\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "    w = {m: inv[m] / total for m in models}\n",
    "\n",
    "    idx = None\n",
    "    for s in fc_dict.values():\n",
    "        idx = s.index if idx is None else idx.union(s.index)\n",
    "    blended = sum(w[m] * fc_dict[m].reindex(idx).fillna(0) for m in models)\n",
    "    return blended, {'winner': min(models, key=lambda m: scores[m]), 'weights': w}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2069483",
   "metadata": {},
   "source": [
    "## 6. Monthly pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e8797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_monthly_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate daily incoming to monthly by department.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['month'] = df['Date'].dt.to_period('M')\n",
    "    monthly = (df.groupby(['department_id', 'month'], as_index=False)['ticket_total']\n",
    "               .sum()\n",
    "               .rename(columns={'ticket_total': 'incoming_monthly'}))\n",
    "    return monthly\n",
    "\n",
    "\n",
    "def bias_correction(blended: pd.Series, hist_actuals: pd.Series, window: int = 6) -> pd.Series:\n",
    "    \"\"\"Simple bias correction using rolling ratio (actual/pred).\"\"\"\n",
    "    df = pd.concat([hist_actuals, blended], axis=1)\n",
    "    df.columns = ['y', 'yhat']\n",
    "    df = df.dropna()\n",
    "    if len(df) >= 3:\n",
    "        ratio = (df['y'] / df['yhat']).tail(window).clip(lower=0.5, upper=1.5).mean()\n",
    "        return blended * float(ratio)\n",
    "    return blended\n",
    "\n",
    "\n",
    "def forecast_per_department_monthly(monthly: pd.DataFrame,\n",
    "                                    case_reason_df: pd.DataFrame,\n",
    "                                    xmas_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rate-aware hybrid with exogenous features + adaptive CV + robust sanitation.\n",
    "    Ensures CV/weights columns always exist.\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    dept_ids = monthly['department_id'].unique().tolist()\n",
    "\n",
    "    for dept in dept_ids:\n",
    "        ts_vol = (monthly.loc[monthly['department_id'] == dept, ['month', 'incoming_monthly']]\n",
    "                  .sort_values('month')\n",
    "                  .set_index('month')['incoming_monthly'])\n",
    "        if not pd.api.types.is_period_dtype(ts_vol.index):\n",
    "            ts_vol.index = pd.PeriodIndex(ts_vol.index, freq='M')\n",
    "        if len(ts_vol) == 0:\n",
    "            continue\n",
    "\n",
    "        # Winsorize + rate\n",
    "        ts_vol = winsorize_monthly(ts_vol, 0.01, 0.99)\n",
    "        ts_rate, _ = monthly_rate_series(ts_vol)\n",
    "        ts_rate = ts_rate.fillna(ts_rate.median()).clip(lower=0)\n",
    "\n",
    "        # Future index & workdays\n",
    "        future_idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "        future_w = future_idx.to_series().apply(lambda p: business_days_in_month(p.start_time.year, p.start_time.month)).astype(float)\n",
    "\n",
    "        # EXOG construction (train + future)\n",
    "        X_train = build_monthly_exog_from_proxy_and_xmas(ts_rate.index, str(dept), case_reason_df, xmas_df)\n",
    "        X_future = build_monthly_exog_from_proxy_and_xmas(future_idx,      str(dept), case_reason_df, xmas_df)\n",
    "\n",
    "        # CV (rate-aware)\n",
    "        try:\n",
    "            cv_smape, cv_wmape = rolling_cv_monthly_adaptive_rate(ts_vol)\n",
    "            cv_smape = cv_smape or {}\n",
    "            cv_wmape = cv_wmape or {}\n",
    "        except Exception:\n",
    "            cv_smape, cv_wmape = {}, {}\n",
    "\n",
    "        # Collect forecasts (on volume scale)\n",
    "        fc_dict: Dict[str, pd.Series] = {}\n",
    "\n",
    "        # Prophet (with exog)\n",
    "        if Prophet is not None and len(ts_rate) >= 12:\n",
    "            try:\n",
    "                _, fp = fit_prophet_monthly_log(ts_rate, is_rate=True, exog_train=X_train)\n",
    "                if fp is not None:\n",
    "                    fc_dict['Prophet'] = fp(H_MONTHS, future_workdays=future_w, exog_future=X_future)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # ARIMA (SARIMAX with exog)\n",
    "        try:\n",
    "            _, fa = fit_arima_monthly_log(ts_rate, is_rate=True, exog_train=X_train)\n",
    "            fc_dict['ARIMA'] = fa(H_MONTHS, future_workdays=future_w, exog_future=X_future)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # TBATS/ETS (no exog)\n",
    "        try:\n",
    "            _, ft = fit_tbats_or_ets_monthly_log(ts_rate, is_rate=True)\n",
    "            fc_dict['TBATS/ETS'] = ft(H_MONTHS, future_workdays=future_w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # ETS Damped (no exog)\n",
    "        try:\n",
    "            _, fe = fit_ets_damped_monthly_log(ts_rate, is_rate=True)\n",
    "            fc_dict['ETS_Damped'] = fe(H_MONTHS, future_workdays=future_w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not fc_dict:\n",
    "            idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "            val = max(0.0, float(ts_vol.mean()))\n",
    "            fc_dict['NaiveMean'] = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # Blend/select using wMAPE\n",
    "        blended, meta = select_or_blend_forecasts(fc_dict, cv_scores_wmape=cv_wmape, blend=True)\n",
    "\n",
    "        # Enforce finiteness\n",
    "        if not np.isfinite(blended.values).all():\n",
    "            finite_mask = np.isfinite(blended.values)\n",
    "            if finite_mask.any():\n",
    "                finite_mean = float(np.nanmean(blended.values[finite_mask]))\n",
    "                vals = np.where(finite_mask, blended.values, finite_mean)\n",
    "                blended = pd.Series(vals, index=blended.index)\n",
    "            else:\n",
    "                idx = pd.period_range(ts_vol.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "                val = max(0.0, float(ts_vol.mean()))\n",
    "                blended = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # Robust smoothing (Median ± K·MAD) + bias correction\n",
    "        blended = robust_roll_cap(blended, window=12, K=6.0)\n",
    "        blended = bias_correction(blended, ts_vol, window=6)\n",
    "\n",
    "        # Optional growth guard\n",
    "        if APPLY_LOCAL_GROWTH_GUARD:\n",
    "            ref = max(1.0, float(ts_vol.tail(12).mean())) if len(ts_vol) else 1.0\n",
    "            blended = blended.clip(lower=ref * MIN_GROWTH, upper=ref * MAX_GROWTH)\n",
    "\n",
    "        # Extract safe weights for sheet columns\n",
    "        w_prophet = meta['weights'].get('Prophet', np.nan)\n",
    "        w_arima = meta['weights'].get('ARIMA', np.nan)\n",
    "        w_tbats = meta['weights'].get('TBATS/ETS', np.nan)\n",
    "\n",
    "        for per, val in blended.items():\n",
    "            out_rows.append({\n",
    "                'department_id': dept,\n",
    "                'month': per,\n",
    "                'forecast_monthly': max(0.0, float(val)),\n",
    "                'cv_prophet_smape': cv_smape.get('Prophet', np.nan),\n",
    "                'cv_arima_smape': cv_smape.get('ARIMA', np.nan),\n",
    "                'cv_tbats_ets_smape': cv_smape.get('TBATS/ETS', np.nan),\n",
    "                'winner_model': meta.get('winner', np.nan),\n",
    "                'blend_prophet_w': w_prophet,\n",
    "                'blend_arima_w': w_arima,\n",
    "                'blend_tbats_ets_w': w_tbats,\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows)\n",
    "\n",
    "    # --- Hotfix: ensure expected columns always exist ---\n",
    "    expected_cols = [\n",
    "        'forecast_monthly',\n",
    "        'cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "        'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w'\n",
    "    ]\n",
    "    for c in expected_cols:\n",
    "        if c not in df_out.columns:\n",
    "            df_out[c] = np.nan\n",
    "\n",
    "    if not df_out.empty:\n",
    "        df_out['department_id'] = df_out['department_id'].astype(str)\n",
    "        if not pd.api.types.is_period_dtype(df_out['month']):\n",
    "            df_out['month'] = pd.PeriodIndex(df_out['month'], freq='M')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def compute_monthly_accuracy_with_history(monthly: pd.DataFrame,\n",
    "                                          fc_monthly: pd.DataFrame,\n",
    "                                          report_start: str) -> pd.DataFrame:\n",
    "    \"\"\"Build capacity_error-like table with historical Actuals and future Forecasts.\"\"\"\n",
    "    monthly = monthly.copy()\n",
    "    monthly['department_id'] = monthly['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(monthly['month']):\n",
    "        monthly['month'] = pd.PeriodIndex(monthly['month'], freq='M')\n",
    "\n",
    "    fc = fc_monthly.copy()\n",
    "    fc['department_id'] = fc['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(fc['month']):\n",
    "        fc['month'] = pd.PeriodIndex(fc['month'], freq='M')\n",
    "\n",
    "    start_per = pd.Period(report_start, freq='M')\n",
    "    last_actual = monthly['month'].max()\n",
    "\n",
    "    hist = (monthly.loc[monthly['month'] >= start_per, ['department_id', 'month', 'incoming_monthly']]\n",
    "            .rename(columns={'incoming_monthly': 'Actual_Volume'}))\n",
    "    hist['Forecast'] = np.nan\n",
    "\n",
    "    fut = fc[['department_id', 'month', 'forecast_monthly',\n",
    "              'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "              'winner_model', 'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']].copy()\n",
    "    fut = fut.loc[fut['month'] > last_actual]\n",
    "    fut = fut.rename(columns={'forecast_monthly': 'Forecast'})\n",
    "    fut['Actual_Volume'] = np.nan\n",
    "\n",
    "    base = pd.concat([hist, fut], ignore_index=True, sort=False)\n",
    "\n",
    "    base['Forecast_Accuracy'] = np.where(\n",
    "        (base['Actual_Volume'].notna()) & (base['Forecast'].notna()) & (base['Actual_Volume'] > 0),\n",
    "        (1 - (np.abs(base['Forecast'] - base['Actual_Volume']) / base['Actual_Volume'])) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    return base\n",
    "\n",
    "\n",
    "def compute_capacity_monthly(cap_df: pd.DataFrame, prod_dept: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute FTE/day needed per month.\"\"\"\n",
    "    out = cap_df.merge(prod_dept, on='department_id', how='left')\n",
    "    out['avg_tickets_per_agent_day'] = pd.to_numeric(out['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    out['avg_tickets_per_agent_day'] = out['avg_tickets_per_agent_day'].replace(0, np.nan)\n",
    "    out['workdays_in_month'] = [business_days_in_month(m.start_time.year, m.start_time.month) for m in out['month']]\n",
    "    out['Capacity_FTE_per_day'] = np.where(\n",
    "        (out['avg_tickets_per_agent_day'] > 0) & (out['workdays_in_month'] > 0) & (out['Forecast'].notna()),\n",
    "        out['Forecast'] / (out['avg_tickets_per_agent_day'] * out['workdays_in_month']),\n",
    "        np.nan\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_cv_table(fc_monthly: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build mape_table_cv with sMAPE, best model and weights.\"\"\"\n",
    "    if fc_monthly is None or fc_monthly.empty:\n",
    "        raise ValueError(\"fc_monthly is empty; cannot build CV table.\")\n",
    "    cols_keep = [\n",
    "        'department_id',\n",
    "        'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "        'winner_model',\n",
    "        'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w'\n",
    "    ]\n",
    "    # Hotfix: add any missing columns as NaN to avoid KeyError\n",
    "    for c in cols_keep:\n",
    "        if c not in fc_monthly.columns:\n",
    "            fc_monthly[c] = np.nan\n",
    "\n",
    "    df = (fc_monthly[cols_keep]\n",
    "          .drop_duplicates(subset=['department_id'])\n",
    "          .copy())\n",
    "    df = df.rename(columns={\n",
    "        'cv_prophet_smape': 'sMAPE_Prophet_CV',\n",
    "        'cv_arima_smape': 'sMAPE_ARIMA_CV',\n",
    "        'cv_tbats_ets_smape': 'sMAPE_TBATS_ETS_CV',\n",
    "        'winner_model': 'Best_Model',\n",
    "        'blend_prophet_w': 'Weight_Prophet',\n",
    "        'blend_arima_w': 'Weight_ARIMA',\n",
    "        'blend_tbats_ets_w': 'Weight_TBATS_ETS',\n",
    "    })\n",
    "    df['department_id'] = df['department_id'].astype(str)\n",
    "    df = apply_mapping(df, mapping)\n",
    "    ordered_cols = [\n",
    "        'department_id', 'department_name', 'vertical',\n",
    "        'sMAPE_Prophet_CV', 'sMAPE_ARIMA_CV', 'sMAPE_TBATS_ETS_CV',\n",
    "        'Best_Model',\n",
    "        'Weight_Prophet', 'Weight_ARIMA', 'Weight_TBATS_ETS'\n",
    "    ]\n",
    "    df = df[ordered_cols]\n",
    "    return df.sort_values(['vertical', 'department_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da44e1f",
   "metadata": {},
   "source": [
    "## 7. Daily plan (reconciled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bff64984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ Language share utilities (NEW) ================\n",
    "EPS_SHARE = 1e-6  # avoid 0/1 in logit\n",
    "KNOWN_LANG_PREFIX = \"lang_\"\n",
    "SERVED_LANGS = {'English','Spanish','Italian','French','Portuguese','German'}\n",
    "\n",
    "\n",
    "def _logit(x):\n",
    "    import numpy as np\n",
    "    x = np.clip(x, EPS_SHARE, 1.0 - EPS_SHARE)\n",
    "    return np.log(x / (1.0 - x))\n",
    "\n",
    "\n",
    "def _inv_logit(z):\n",
    "    import numpy as np\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def _safe_row_renorm(m):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    arr = m.values.astype(float)\n",
    "    row_sum = arr.sum(axis=1, keepdims=True)\n",
    "    row_sum[row_sum <= 0] = 1.0\n",
    "    arr = arr / row_sum\n",
    "    return pd.DataFrame(arr, index=m.index, columns=m.columns)\n",
    "\n",
    "\n",
    "# ================ Monthly by-language (NEW) ================\n",
    "from typing import Tuple\n",
    "\n",
    "def parse_languages_incoming(df_in):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      daily_total  : DataFrame con Date, department_id, ticket_total (coherente con pipeline actual)\n",
    "      daily_bylang : DataFrame largo con Date, department_id, language, ticket_lang\n",
    "    Admite:\n",
    "      - Formato largo: ['Date','department_id','language','ticket_total'] por fila/idioma\n",
    "      - Formato ancho: columnas por idioma (Whitelist o prefijo lang_)\n",
    "    Regla de negocio: cualquier idioma fuera de los 6 servidos se atiende en inglés (se mapea a 'English').\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = df_in.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "\n",
    "    # Caso largo\n",
    "    if 'language' in df.columns:\n",
    "        # Normalizar nombre de idioma\n",
    "        lang = df['language'].astype(str).str.strip()\n",
    "        # mapear no-served -> English\n",
    "        lang_norm = lang.apply(lambda x: x.title() if isinstance(x, str) else x)\n",
    "        lang_norm = lang_norm.apply(lambda x: ('English' if (not x or (x not in SERVED_LANGS)) else x))\n",
    "        df['service_language'] = lang_norm\n",
    "        # volumen por idioma/servicio\n",
    "        df['ticket_total'] = pd.to_numeric(df['ticket_total'], errors='coerce').fillna(0.0)\n",
    "        daily_bylang = (df[['Date','department_id','service_language','ticket_total']]\n",
    "                        .rename(columns={'service_language':'language', 'ticket_total':'ticket_lang'})\n",
    "                        .dropna(subset=['Date','department_id','language']))\n",
    "        # total diario por dept (suma tras mapping)\n",
    "        daily_total = (daily_bylang.groupby(['Date','department_id'], as_index=False)['ticket_lang']\n",
    "                       .sum()\n",
    "                       .rename(columns={'ticket_lang':'ticket_total'}))\n",
    "        return daily_total, daily_bylang\n",
    "\n",
    "    # Caso ancho: detectar columnas de idioma\n",
    "    lang_cols = [c for c in df.columns if (c.title() in SERVED_LANGS) or c.startswith(KNOWN_LANG_PREFIX)]\n",
    "    if lang_cols:\n",
    "        import pandas as pd\n",
    "        melt = (df[['Date','department_id'] + lang_cols]\n",
    "                .melt(id_vars=['Date','department_id'], var_name='language_col', value_name='ticket_lang'))\n",
    "        melt['language'] = melt['language_col'].str.replace(f'^{KNOWN_LANG_PREFIX}', '', regex=True)\n",
    "        melt['language'] = melt['language'].str.strip().str.title()\n",
    "        # mapear no-served -> English\n",
    "        melt['language'] = melt['language'].apply(lambda x: x if x in SERVED_LANGS else 'English')\n",
    "        melt['ticket_lang'] = pd.to_numeric(melt['ticket_lang'], errors='coerce').fillna(0.0)\n",
    "        daily_bylang = melt[['Date','department_id','language','ticket_lang']].dropna(subset=['Date'])\n",
    "        daily_total = (daily_bylang.groupby(['Date','department_id'], as_index=False)['ticket_lang']\n",
    "                       .sum()\n",
    "                       .rename(columns={'ticket_lang':'ticket_total'}))\n",
    "        return daily_total, daily_bylang\n",
    "\n",
    "    # Sin detalle de idioma\n",
    "    return df[['Date','department_id','ticket_total']].copy(), pd.DataFrame(columns=['Date','department_id','language','ticket_lang'])\n",
    "\n",
    "\n",
    "def build_monthly_language_totals(daily_bylang):\n",
    "    \"\"\"\n",
    "    Devuelve DataFrame mensual por idioma:\n",
    "      ['department_id','month','language','incoming_monthly_lang']\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if daily_bylang is None or len(daily_bylang) == 0:\n",
    "        return pd.DataFrame(columns=['department_id','month','language','incoming_monthly_lang'])\n",
    "    tmp = daily_bylang.copy()\n",
    "    tmp['month'] = pd.to_datetime(tmp['Date']).dt.to_period('M')\n",
    "    mon = (tmp.groupby(['department_id','month','language'], as_index=False)['ticket_lang']\n",
    "           .sum()\n",
    "           .rename(columns={'ticket_lang':'incoming_monthly_lang'}))\n",
    "    mon['department_id'] = mon['department_id'].astype(str)\n",
    "    return mon\n",
    "\n",
    "\n",
    "# ================ Language share forecasting (NEW) ================\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "def _fit_ets_logit_share(ts_share):\n",
    "    \"\"\"ETS damped sobre logit(share). Devuelve (model, forecast_fn).\"\"\"\n",
    "    import numpy as np\n",
    "    y = _logit(np.array(ts_share.values, dtype=float))\n",
    "    seasonal = 12 if len(ts_share) >= 24 else None\n",
    "    model = ExponentialSmoothing(y, trend='add', damped_trend=True,\n",
    "                                 seasonal=('add' if seasonal else None),\n",
    "                                 seasonal_periods=seasonal).fit()\n",
    "    def fcast(h_months):\n",
    "        import numpy as np\n",
    "        z = model.forecast(h_months)\n",
    "        return _inv_logit(np.array(z, dtype=float))\n",
    "    return model, fcast\n",
    "\n",
    "\n",
    "def forecast_language_shares_per_dept(monthly_total, monthly_bylang, h_months=12, roll_fallback=6):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    if monthly_bylang is None or len(monthly_bylang) == 0 or monthly_total is None or len(monthly_total) == 0:\n",
    "        return pd.DataFrame(columns=['department_id','month','language','share_fc'])\n",
    "\n",
    "    monthly_total = monthly_total.copy()\n",
    "    if str(monthly_total['month'].dtype) != 'period[M]':\n",
    "        monthly_total['month'] = pd.PeriodIndex(monthly_total['month'], freq='M')\n",
    "\n",
    "    monthly_bylang = monthly_bylang.copy()\n",
    "    if str(monthly_bylang['month'].dtype) != 'period[M]':\n",
    "        monthly_bylang['month'] = pd.PeriodIndex(monthly_bylang['month'], freq='M')\n",
    "\n",
    "    out_rows = []\n",
    "    for dept, g_tot in monthly_total.groupby('department_id'):\n",
    "        g_lang = monthly_bylang[monthly_bylang['department_id'] == dept].copy()\n",
    "        if g_lang.empty:\n",
    "            continue\n",
    "        langs = sorted(g_lang['language'].dropna().unique().tolist())\n",
    "        M = (g_lang.pivot_table(index='month', columns='language', values='incoming_monthly_lang', aggfunc='sum')\n",
    "             .reindex(sorted(g_tot['month'].unique()), fill_value=0.0))\n",
    "        ytot = (g_tot.set_index('month')['incoming_monthly'].reindex(M.index).fillna(0.0))\n",
    "        shares_hist = _safe_row_renorm(M.div(ytot.replace(0, np.nan), axis=0).fillna(0.0))\n",
    "        hist_idx = shares_hist.index\n",
    "        fut_idx = pd.period_range(hist_idx[-1] + 1, periods=h_months, freq='M')\n",
    "        shares_fc = pd.DataFrame(index=fut_idx, columns=langs, dtype=float)\n",
    "        for lang in langs:\n",
    "            s = shares_hist[lang].astype(float)\n",
    "            try:\n",
    "                if s.notna().sum() >= 10:\n",
    "                    _, f = _fit_ets_logit_share(s)\n",
    "                    pred = f(h_months)\n",
    "                    shares_fc[lang] = pred\n",
    "                elif s.notna().sum() >= max(3, roll_fallback):\n",
    "                    val = float(s.tail(roll_fallback).mean())\n",
    "                    shares_fc[lang] = val\n",
    "                else:\n",
    "                    val = float(s.mean() if s.notna().any() else 1.0 / max(1, len(langs)))\n",
    "                    shares_fc[lang] = val\n",
    "            except Exception:\n",
    "                val = float(s.mean() if s.notna().any() else 1.0 / max(1, len(langs)))\n",
    "                shares_fc[lang] = val\n",
    "        shares_fc = _safe_row_renorm(shares_fc)\n",
    "        shares_fc = shares_fc.reset_index().melt(id_vars='index', var_name='language', value_name='share_fc')\n",
    "        shares_fc = shares_fc.rename(columns={'index':'month'})\n",
    "        shares_fc.insert(0, 'department_id', dept)\n",
    "        out_rows.append(shares_fc)\n",
    "    return (pd.concat(out_rows, ignore_index=True) if out_rows else\n",
    "            pd.DataFrame(columns=['department_id','month','language','share_fc']))\n",
    "\n",
    "\n",
    "def rolling_cv_language_shares(monthly_total, monthly_bylang):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    rows = []\n",
    "    if monthly_bylang is None or len(monthly_bylang) == 0:\n",
    "        return pd.DataFrame(columns=['department_id','language','sMAPE_share','obs_count'])\n",
    "    monthly_total = monthly_total.copy()\n",
    "    monthly_bylang = monthly_bylang.copy()\n",
    "    if str(monthly_total['month'].dtype) != 'period[M]':\n",
    "        monthly_total['month'] = pd.PeriodIndex(monthly_total['month'], freq='M')\n",
    "    if str(monthly_bylang['month'].dtype) != 'period[M]':\n",
    "        monthly_bylang['month'] = pd.PeriodIndex(monthly_bylang['month'], freq='M')\n",
    "\n",
    "    from numpy import array\n",
    "    # sMAPE from existing helper in notebook\n",
    "    from math import isnan\n",
    "\n",
    "    def _smape_np(y_true, y_pred):\n",
    "        import numpy as np\n",
    "        y_true = np.array(y_true, dtype=float)\n",
    "        y_pred = np.array(y_pred, dtype=float)\n",
    "        denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "        denom[denom == 0] = 1.0\n",
    "        return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "    for dept, g_tot in monthly_total.groupby('department_id'):\n",
    "        g_lang = monthly_bylang[monthly_bylang['department_id'] == dept]\n",
    "        if g_lang.empty:\n",
    "            continue\n",
    "        langs = sorted(g_lang['language'].dropna().unique().tolist())\n",
    "        M = (g_lang.pivot_table(index='month', columns='language', values='incoming_monthly_lang', aggfunc='sum')\n",
    "             .reindex(sorted(g_tot['month'].unique()), fill_value=0.0))\n",
    "        ytot = (g_tot.set_index('month')['incoming_monthly'].reindex(M.index).fillna(0.0))\n",
    "        shares_hist = _safe_row_renorm(M.div(ytot.replace(0, np.nan), axis=0).fillna(0.0))\n",
    "        n = len(shares_hist.index)\n",
    "        if n < 14:\n",
    "            for lang in langs:\n",
    "                s = shares_hist[lang]\n",
    "                rows.append({'department_id': dept, 'language': lang,\n",
    "                             'sMAPE_share': float(s.std() * 100.0), 'obs_count': n})\n",
    "            continue\n",
    "        h = 1 if n < 24 else 3\n",
    "        start_min = max(12, n - (h + 4))\n",
    "        for lang in langs:\n",
    "            s = shares_hist[lang].astype(float)\n",
    "            errs = []\n",
    "            for start in range(start_min, n - h + 1):\n",
    "                train = s.iloc[:start]\n",
    "                test = s.iloc[start:start+h]\n",
    "                try:\n",
    "                    if train.notna().sum() >= 10:\n",
    "                        _, f = _fit_ets_logit_share(train)\n",
    "                        pred = f(h)\n",
    "                        errs.append(_smape_np(test.values, array(pred[:h], dtype=float)))\n",
    "                    else:\n",
    "                        pred = [float(train.tail(6).mean() if train.notna().any() else 0.0)] * h\n",
    "                        errs.append(_smape_np(test.values, array(pred, dtype=float)))\n",
    "                except Exception:\n",
    "                    pred = [float(train.mean() if train.notna().any() else 0.0)] * h\n",
    "                    errs.append(_smape_np(test.values, array(pred, dtype=float)))\n",
    "            rows.append({'department_id': dept, 'language': lang,\n",
    "                         'sMAPE_share': float(np.mean(errs)) if errs else np.nan, 'obs_count': n})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ================ Apply shares & daily allocation by language (NEW) ================\n",
    "\n",
    "def apply_language_shares_to_monthly(fc_monthly, shares_fc):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    if fc_monthly is None or len(fc_monthly) == 0 or shares_fc is None or len(shares_fc) == 0:\n",
    "        return pd.DataFrame(columns=['department_id','month','language','forecast_monthly_language'])\n",
    "    f = fc_monthly[['department_id','month','forecast_monthly']].copy()\n",
    "    if str(f['month'].dtype) != 'period[M]':\n",
    "        f['month'] = pd.PeriodIndex(f['month'], freq='M')\n",
    "    s = shares_fc.copy()\n",
    "    if str(s['month'].dtype) != 'period[M]':\n",
    "        s['month'] = pd.PeriodIndex(s['month'], freq='M')\n",
    "    m = f.merge(s, on=['department_id','month'], how='left')\n",
    "    m['share_fc'] = m['share_fc'].fillna(0.0)\n",
    "\n",
    "    def _fix_group(g):\n",
    "        if g['share_fc'].sum() <= 0:\n",
    "            k = max(1, g['language'].nunique())\n",
    "            g['share_fc'] = 1.0 / k\n",
    "        else:\n",
    "            g['share_fc'] = g['share_fc'] / g['share_fc'].sum()\n",
    "        return g\n",
    "    m = m.groupby(['department_id','month'], as_index=False).apply(_fix_group)\n",
    "    m['forecast_monthly_language'] = m['forecast_monthly'] * m['share_fc']\n",
    "    return m[['department_id','month','language','forecast_monthly_language']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_daily_from_monthly_by_language(incoming_bylang, fc_monthly_bylang, horizon_days):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    if fc_monthly_bylang is None or len(fc_monthly_bylang) == 0:\n",
    "        return pd.DataFrame(columns=['department_id','Date','language','forecast_daily_language'])\n",
    "    last_date = incoming_bylang['Date'].max() if (incoming_bylang is not None and not incoming_bylang.empty) else pd.Timestamp.today().normalize()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    end = start + pd.Timedelta(days=horizon_days-1)\n",
    "    future_months = pd.period_range(start=start.to_period('M'), end=end.to_period('M'), freq='M')\n",
    "\n",
    "    rows = []\n",
    "    by_key = {}\n",
    "    if incoming_bylang is not None and not incoming_bylang.empty:\n",
    "        incoming_bylang = incoming_bylang.copy()\n",
    "        incoming_bylang['Date'] = pd.to_datetime(incoming_bylang['Date'])\n",
    "        incoming_bylang['department_id'] = incoming_bylang['department_id'].astype(str)\n",
    "        for (dept, lang), g in incoming_bylang.groupby(['department_id','language']):\n",
    "            by_key[(dept, lang)] = g.sort_values('Date')\n",
    "\n",
    "    for (dept, lang), gmon in fc_monthly_bylang.groupby(['department_id','language']):\n",
    "        for m in future_months:\n",
    "            fcm = gmon[gmon['month'] == m]\n",
    "            if fcm.empty:\n",
    "                continue\n",
    "            target = float(fcm['forecast_monthly_language'].iloc[0])\n",
    "            if target <= 0:\n",
    "                continue\n",
    "            hist_lang = by_key.get((dept, lang), None)\n",
    "            if hist_lang is not None and len(hist_lang) >= 30:\n",
    "                prof = (hist_lang.assign(dow=hist_lang['Date'].dt.dayofweek)\n",
    "                        .groupby('dow')['ticket_lang'].mean())\n",
    "                prof = prof / (prof.mean() if prof.mean() != 0 else 1.0)\n",
    "            else:\n",
    "                dept_hist = incoming_bylang[incoming_bylang['department_id'] == dept]\n",
    "                if not dept_hist.empty:\n",
    "                    prof = (dept_hist.assign(dow=dept_hist['Date'].dt.dayofweek)\n",
    "                            .groupby('dow')['ticket_lang'].mean())\n",
    "                    prof = prof / (prof.mean() if prof.mean() != 0 else 1.0)\n",
    "                else:\n",
    "                    prof = pd.Series(1.0, index=range(7))\n",
    "            days = pd.date_range(start=m.start_time, end=m.end_time, freq='D')\n",
    "            weights = np.array([float(prof.get(d.dayofweek, 1.0)) for d in days], dtype=float)\n",
    "            weights = np.maximum(weights, 1e-6)\n",
    "            weights = weights / weights.sum()\n",
    "            alloc = target * weights\n",
    "            alloc_df = pd.DataFrame({'Date': days, 'forecast_daily_language': alloc})\n",
    "            alloc_df = alloc_df[(alloc_df['Date'] >= start) & (alloc_df['Date'] <= end)]\n",
    "            alloc_df.insert(0, 'language', lang)\n",
    "            alloc_df.insert(0, 'department_id', dept)\n",
    "            rows.append(alloc_df)\n",
    "    return (pd.concat(rows, ignore_index=True) if rows else\n",
    "            pd.DataFrame(columns=['department_id','Date','language','forecast_daily_language']))\n",
    "\n",
    "\n",
    "def dow_profile(g: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Build normalized day-of-week profile for a department, fallback to uniform.\"\"\"\n",
    "    prof = (g.assign(dow=g['Date'].dt.dayofweek)\n",
    "              .groupby('dow')['ticket_total']\n",
    "              .mean())\n",
    "    if prof.notna().sum() >= 3:\n",
    "        prof = prof / prof.mean()\n",
    "    else:\n",
    "        prof = pd.Series(1.0, index=range(7))\n",
    "    return prof\n",
    "\n",
    "\n",
    "def disaggregate_month_to_days(dept_df: pd.DataFrame,\n",
    "                               month_period: pd.Period,\n",
    "                               target_sum: float) -> pd.DataFrame:\n",
    "    \"\"\"Allocate monthly forecast to each day in that month using recent DOW profile.\"\"\"\n",
    "    start = month_period.start_time\n",
    "    end = month_period.end_time\n",
    "    days = pd.date_range(start=start, end=end, freq='D')\n",
    "\n",
    "    hist = dept_df.sort_values('Date').tail(90)\n",
    "    profile = dow_profile(hist)\n",
    "\n",
    "    weights = np.array([profile.get(d.dayofweek, 1.0) for d in days], dtype=float)\n",
    "    weights = np.maximum(weights, 1e-6)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    alloc = target_sum * weights\n",
    "    return pd.DataFrame({'Date': days, 'forecast_daily': alloc})\n",
    "\n",
    "\n",
    "def build_daily_from_monthly(incoming: pd.DataFrame,\n",
    "                             fc_monthly: pd.DataFrame,\n",
    "                             horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Top-down daily plan.\"\"\"\n",
    "    last_date = incoming['Date'].max()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    end = start + pd.Timedelta(days=horizon_days - 1)\n",
    "    future_months = pd.period_range(start=start.to_period('M'),\n",
    "                                    end=end.to_period('M'), freq='M')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in incoming.groupby('department_id'):\n",
    "        for m in future_months:\n",
    "            fcm = fc_monthly[(fc_monthly['department_id'] == dept) & (fc_monthly['month'] == m)]\n",
    "            if fcm.empty:\n",
    "                continue\n",
    "            target = float(fcm['forecast_monthly'].iloc[0])\n",
    "            if target <= 0:\n",
    "                continue\n",
    "            alloc_df = disaggregate_month_to_days(g, m, target)\n",
    "            alloc_df = alloc_df[(alloc_df['Date'] >= start) & (alloc_df['Date'] <= end)]\n",
    "            alloc_df.insert(0, 'department_id', dept)\n",
    "            rows.append(alloc_df)\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_daily_by_language(df_daily_fc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Split daily forecast by fixed language shares.\"\"\"\n",
    "    parts = []\n",
    "    for lang, w in LANGUAGE_SHARES.items():\n",
    "        tmp = df_daily_fc.copy()\n",
    "        tmp['language'] = lang\n",
    "        tmp['forecast_daily_language'] = tmp['forecast_daily'] * w\n",
    "        parts.append(tmp)\n",
    "    out = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    return out\n",
    "\n",
    "\n",
    "def forecast_daily_baseline(df_daily: pd.DataFrame, horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"Independent daily baseline (optional).\"\"\"\n",
    "    df = df_daily.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df = df.sort_values(['department_id', 'Date'])\n",
    "    last_date = df['Date'].max()\n",
    "    if pd.isna(last_date):\n",
    "        raise ValueError(\"forecast_daily_baseline: No valid dates in incoming.\")\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    idx_future = pd.date_range(start=start, periods=horizon_days, freq='D')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in df.groupby('department_id'):\n",
    "        g = g.sort_values('Date')\n",
    "        if len(g) >= 28:\n",
    "            roll_mean = (g.set_index('Date')['ticket_total']\n",
    "                         .rolling(window=28, min_periods=1)\n",
    "                         .mean()\n",
    "                         .iloc[-1])\n",
    "            base = float(roll_mean) if np.isfinite(roll_mean) else float(g['ticket_total'].mean())\n",
    "        else:\n",
    "            base = float(g['ticket_total'].mean())\n",
    "\n",
    "        prof = dow_profile(g)\n",
    "        vals = []\n",
    "        for d in idx_future:\n",
    "            w = prof[d.dayofweek] if d.dayofweek in prof.index else 1.0\n",
    "            vals.append(max(0.0, base * float(w)))\n",
    "        rows.append(pd.DataFrame({'department_id': dept, 'Date': idx_future, 'forecast_daily': vals}))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "\n",
    "\n",
    "def build_daily_capacity_plan(incoming: pd.DataFrame,\n",
    "                              mapping: pd.DataFrame,\n",
    "                              prod_dept: pd.DataFrame,\n",
    "                              fc_monthly: pd.DataFrame,\n",
    "                              horizon_days: int,\n",
    "                              daily_bylang: Optional[pd.DataFrame] = None,\n",
    "                              monthly_total: Optional[pd.DataFrame] = None,\n",
    "                              monthly_bylang: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    End-to-end diario por idioma (dinámico). Si no hay detalle por idioma,\n",
    "    cae al reparto legacy (no recomendado).\n",
    "    \"\"\"\n",
    "    # Fallback legacy si no tenemos detalle por idioma\n",
    "    if daily_bylang is None or daily_bylang.empty or monthly_bylang is None or monthly_bylang.empty:\n",
    "        daily_fc = build_daily_from_monthly(incoming, fc_monthly, horizon_days) if USE_DAILY_FROM_MONTHLY else forecast_daily_baseline(incoming, horizon_days)\n",
    "        parts = []\n",
    "        for lang, w in LANGUAGE_SHARES.items():\n",
    "            tmp = daily_fc.copy()\n",
    "            tmp['language'] = lang\n",
    "            tmp['forecast_daily_language'] = tmp['forecast_daily'] * w\n",
    "            parts.append(tmp)\n",
    "        daily_fc_lang = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "    else:\n",
    "        shares_fc = forecast_language_shares_per_dept(monthly_total, monthly_bylang, h_months=H_MONTHS, roll_fallback=6)\n",
    "        fc_mon_by_lang = apply_language_shares_to_monthly(fc_monthly, shares_fc)\n",
    "        daily_fc_lang = build_daily_from_monthly_by_language(daily_bylang, fc_mon_by_lang, horizon_days)\n",
    "\n",
    "    daily_fc_lang = apply_mapping(daily_fc_lang, mapping)\n",
    "    daily_fc_lang = daily_fc_lang.merge(prod_dept, on='department_id', how='left')\n",
    "    daily_fc_lang['avg_tickets_per_agent_day'] = pd.to_numeric(daily_fc_lang['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    daily_fc_lang['FTE_per_day'] = np.where(\n",
    "        daily_fc_lang['avg_tickets_per_agent_day'] > 0,\n",
    "        daily_fc_lang['forecast_daily_language'] / daily_fc_lang['avg_tickets_per_agent_day'],\n",
    "        np.nan\n",
    "    )\n",
    "    cols = ['Date','department_id','department_name','vertical','language','forecast_daily_language','FTE_per_day']\n",
    "    daily_plan = daily_fc_lang[cols].sort_values(['Date','vertical','department_id','language'])\n",
    "    return daily_plan\n",
    "\n",
    "def build_stability_report(monthly: pd.DataFrame,\n",
    "                           fc_monthly: pd.DataFrame,\n",
    "                           daily_capacity_plan: pd.DataFrame,\n",
    "                           mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Diagnostic sheet with:\n",
    "      - forecast_monthly vs sum of daily (reconciliation diff)\n",
    "      - ref_mean_12m: mean of last 12 actual months\n",
    "      - forecast_vs_ref_ratio\n",
    "      - CV sMAPE (from fc_monthly), Best_Model, blend weights\n",
    "    Hardened to add any missing columns as NaN (avoids KeyError).\n",
    "    \"\"\"\n",
    "    m = monthly.copy()\n",
    "    m['department_id'] = m['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(m['month']):\n",
    "        m['month'] = pd.PeriodIndex(m['month'], freq='M')\n",
    "    last_actual = m['month'].max()\n",
    "\n",
    "    f = fc_monthly.copy()\n",
    "    f['department_id'] = f['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(f['month']):\n",
    "        f['month'] = pd.PeriodIndex(f['month'], freq='M')\n",
    "    f = f[f['month'] > last_actual].copy()\n",
    "\n",
    "    # Reference mean (last 12 actual months per dept)\n",
    "    ref12 = (m.groupby('department_id')\n",
    "               .apply(lambda g: g.set_index('month')['incoming_monthly'].sort_index().tail(12).mean())\n",
    "               .rename('ref_mean_12m')\n",
    "               .reset_index())\n",
    "\n",
    "    # Daily reconciliation: sum by dept-month across languages\n",
    "    d = daily_capacity_plan.copy()\n",
    "    d['department_id'] = d['department_id'].astype(str)\n",
    "    d['month'] = pd.to_datetime(d['Date']).dt.to_period('M')\n",
    "    daily_sum = (d.groupby(['department_id','month'], as_index=False)['forecast_daily_language']\n",
    "                   .sum()\n",
    "                   .rename(columns={'forecast_daily_language':'daily_sum_monthly'}))\n",
    "\n",
    "    rep = (f.merge(ref12, on='department_id', how='left')\n",
    "             .merge(daily_sum, on=['department_id','month'], how='left'))\n",
    "\n",
    "    rep['reconcile_diff'] = rep['daily_sum_monthly'] - rep['forecast_monthly']\n",
    "    rep['forecast_vs_ref_ratio'] = np.where(rep['ref_mean_12m'] > 0,\n",
    "                                            rep['forecast_monthly'] / rep['ref_mean_12m'],\n",
    "                                            np.nan)\n",
    "\n",
    "    # Attach mapping & CV/weights (deduplicate per dept)\n",
    "    # Hotfix: ensure columns exist in fc_monthly before selecting\n",
    "    for c in ['cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "              'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w']:\n",
    "        if c not in fc_monthly.columns:\n",
    "            fc_monthly[c] = np.nan\n",
    "\n",
    "    head = (fc_monthly[['department_id','cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "                        'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w']]\n",
    "            .drop_duplicates('department_id'))\n",
    "    rep = rep.merge(head, on='department_id', how='left')\n",
    "    rep = apply_mapping(rep, mapping)\n",
    "\n",
    "    # Order columns for readability (add any missing as NaN to avoid KeyError)\n",
    "    cols = ['vertical','department_id','department_name','month',\n",
    "            'forecast_monthly','daily_sum_monthly','reconcile_diff',\n",
    "            'ref_mean_12m','forecast_vs_ref_ratio',\n",
    "            'cv_prophet_smape','cv_arima_smape','cv_tbats_ets_smape',\n",
    "            'winner_model','blend_prophet_w','blend_arima_w','blend_tbats_ets_w']\n",
    "\n",
    "    for c in cols:\n",
    "        if c not in rep.columns:\n",
    "            rep[c] = np.nan\n",
    "\n",
    "    rep = rep[cols].sort_values(['vertical','department_id','month'])\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4a2ef",
   "metadata": {},
   "source": [
    "## 8. Main & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ed81337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:28:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:29:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:29:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:31:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:31:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:35:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:37:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:37:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:39:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:39:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:42:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:42:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:47:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:47:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:50:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:50:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:52:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:53:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:56:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:56:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:45 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\n",
      "Christmas CSV at: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\christmas_holidays_2024_2027.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # 0) Xmas CSV\n",
    "    ensure_christmas_csv(HOLIDAYS_CSV_PATH, HOLIDAYS_YEARS, INCLUDE_JAN6)\n",
    "    xmas_df = load_christmas_csv(HOLIDAYS_CSV_PATH)\n",
    "\n",
    "    # 1) Load inputs\n",
    "    incoming_raw = load_incoming(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET)\n",
    "    # NEW: separar totales y detalle por idioma (si existe) con regla de negocio (no-served -> English)\n",
    "    incoming_total, incoming_bylang = parse_languages_incoming(incoming_raw)\n",
    "\n",
    "    mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "    prod = load_productivity(PRODUCTIVITY_PATH)\n",
    "\n",
    "    # 2) Exogenous proxy\n",
    "    case_reason_df = load_case_reason_proxy(CASE_REASON_PATH, CASE_REASON_SHEET)\n",
    "\n",
    "    # 3) Monthly forecast (total)\n",
    "    monthly = build_monthly_series(incoming_total)\n",
    "    fc_monthly = forecast_per_department_monthly(monthly, case_reason_df, xmas_df)\n",
    "\n",
    "    # 4) capacity_error (hist + future) + capacidad\n",
    "    cap_err = compute_monthly_accuracy_with_history(monthly, fc_monthly, REPORT_START_MONTH)\n",
    "    cap_err = compute_capacity_monthly(cap_err, prod)\n",
    "    cap_err = apply_mapping(cap_err, mapping)\n",
    "\n",
    "    # 5) NEW: Mensual por idioma (histórico) + CV de shares\n",
    "    monthly_bylang = build_monthly_language_totals(incoming_bylang)\n",
    "    monthly_total = monthly[['department_id','month','incoming_monthly']].copy()\n",
    "    lang_cv = rolling_cv_language_shares(monthly_total, monthly_bylang)\n",
    "\n",
    "    # 6) Daily plan por idioma (dinámico, reconciliado)\n",
    "    daily_capacity_plan = build_daily_capacity_plan(\n",
    "        incoming=incoming_total,\n",
    "        mapping=mapping,\n",
    "        prod_dept=prod,\n",
    "        fc_monthly=fc_monthly,\n",
    "        horizon_days=DAILY_HORIZON_DAYS,\n",
    "        daily_bylang=incoming_bylang,\n",
    "        monthly_total=monthly_total,\n",
    "        monthly_bylang=monthly_bylang\n",
    "    )\n",
    "\n",
    "    # 7) CV table + Stability report\n",
    "    cv_table = build_cv_table(fc_monthly, mapping)\n",
    "    stability_report = build_stability_report(monthly, fc_monthly, daily_capacity_plan, mapping)\n",
    "\n",
    "    # 8) NEW: Hoja de tendencias de idioma (últimos 12m)\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    if not monthly_bylang.empty:\n",
    "        last_m = monthly_total['month'].max()\n",
    "        hist12_idx = pd.period_range(last_m - 11, last_m, freq='M')\n",
    "        mh = monthly_total[monthly_total['month'].isin(hist12_idx)]\n",
    "        M = (monthly_bylang[monthly_bylang['month'].isin(hist12_idx)]\n",
    "             .pivot_table(index=['department_id','month'], columns='language', values='incoming_monthly_lang', aggfunc='sum').fillna(0.0))\n",
    "        ytot = (mh.set_index(['department_id','month'])['incoming_monthly']).reindex(M.index).fillna(0.0)\n",
    "        shares_hist12 = _safe_row_renorm(M.div(ytot.replace(0, np.nan), axis=0).fillna(0.0))\n",
    "        shares_hist12 = shares_hist12.reset_index()\n",
    "        language_trends = shares_hist12\n",
    "    else:\n",
    "        language_trends = pd.DataFrame(columns=['department_id','month'])\n",
    "\n",
    "    # 9) Clean infs\n",
    "    for df_out in [cap_err, daily_capacity_plan, cv_table, stability_report, lang_cv, language_trends]:\n",
    "        if df_out is not None and not df_out.empty:\n",
    "            df_out.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # 10) Write Excel\n",
    "    with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        (cap_err[['vertical', 'department_id', 'department_name', 'month',\n",
    "                  'Actual_Volume', 'Forecast', 'Forecast_Accuracy',\n",
    "                  'Capacity_FTE_per_day',\n",
    "                  'winner_model', 'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "                  'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']]\n",
    "         .sort_values(['vertical', 'department_id', 'month'])\n",
    "         .to_excel(w, \"capacity_error\", index=False))\n",
    "\n",
    "        daily_capacity_plan.to_excel(w, \"daily_capacity_plan\", index=False)\n",
    "        cv_table.to_excel(w, \"mape_table_cv\", index=False)\n",
    "        stability_report.to_excel(w, \"stability_report\", index=False)\n",
    "\n",
    "        if language_trends is not None and not language_trends.empty:\n",
    "            language_trends.to_excel(w, \"language_trends_hist\", index=False)\n",
    "        if lang_cv is not None and not lang_cv.empty:\n",
    "            lang_cv.to_excel(w, \"language_share_cv\", index=False)\n",
    "\n",
    "    print(\"Excel written:\", OUTPUT_XLSX)\n",
    "    print(\"Christmas CSV at:\", HOLIDAYS_CSV_PATH)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
